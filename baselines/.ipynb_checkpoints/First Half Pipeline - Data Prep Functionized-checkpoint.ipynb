{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4cc7d181",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\matts\\.conda\\envs\\measeval\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Punkt tokenizers already installed.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import random\n",
    "import os\n",
    "import json\n",
    "import nltk\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "    print(\"Punkt tokenizers already installed.\")\n",
    "except:\n",
    "    print(\"Punkt tokenizers not found; installing now.\")\n",
    "    nltk.download('punkt')\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import re\n",
    "from datetime import datetime\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "else:\n",
    "    device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d094acdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_dataset(text_path: str, task: str='QUANT', bert_type: str=\"bert-base-cased\", label_path: str=None, \n",
    "                 remove_markers: bool=True, batch_size: int=64, torch_seed: int=434):\n",
    "    \n",
    "    \"\"\"\n",
    "    Prepare data for model creation with BERT variants.\n",
    "    =====================================================\n",
    "    Inputs:\n",
    "    text_path (str, required): filepath to text files\n",
    "    task (str): which MeasEval task to prepare dataset for\n",
    "    bert_type (str): type of BERT variant (huggingface)\n",
    "    label_path (str): filepath to labels for training datasets\n",
    "    remove_markers (bool): toggle to remove figure numbers, publication years, etc.\n",
    "    batch_size (int): batch size for training\n",
    "    torch_seed (int): seed number for (some) reproducibility\n",
    "    =====================================================\n",
    "    Outputs:\n",
    "    dataset (list): flattened/tokenized/batched sentences w/annotations\n",
    "    \"\"\"\n",
    "        \n",
    "    torch.manual_seed(torch_seed)\n",
    "    \n",
    "    if not label_path:\n",
    "        dataset_type = \"test\"\n",
    "    else:\n",
    "        dataset_type = \"training or validation\"\n",
    "        \n",
    "    typemap = {\"Quantity\": \"QUANT\",\n",
    "           \"MeasuredEntity\": \"ME\", \n",
    "           \"MeasuredProperty\": \"MP\", \n",
    "           \"Qualifier\": \"QUAL\"\n",
    "        }\n",
    "        \n",
    "    print(\"=\"*40)\n",
    "    print(\"Preparing **\", dataset_type, \"** dataset, based on arguments provided.\", sep=\"\")\n",
    "    if not label_path:\n",
    "        print(\"If you intended to prepare training/validation data, provide a label_path to the function.\")\n",
    "    print(\"=\"*40)\n",
    "    \n",
    "    textset = {}\n",
    "    for fn in os.listdir(text_path):\n",
    "        with open(text_path+fn, encoding='utf-8', errors='replace') as textfile:\n",
    "            text = textfile.read()\n",
    "            textset[fn[:-4]] = text\n",
    "    \n",
    "    if label_path:\n",
    "        # Load all annotations\n",
    "        files_with_label = [file_name[:-4] for file_name in os.listdir(label_path)]\n",
    "        all_files_with_or_without_label = [file_name[:-4] for file_name in os.listdir(text_path)]\n",
    "        files_without_label = list(set(all_files_with_or_without_label).difference(set(files_with_label)))\n",
    "        print(\"Unlabeled files: \", len(files_without_label), \". Labeled files: \", len(files_with_label),\".\", sep=\"\")\n",
    "        if len(files_without_label) > 0:\n",
    "            print(\"Example of unlabeled file: \", files_without_label[0],\".\", sep=\"\")\n",
    "    else:\n",
    "        files_without_label = [file_name[:-4] for file_name in os.listdir(text_path)]\n",
    "        \n",
    "    \n",
    "    all_data = []\n",
    "    \n",
    "    if label_path:\n",
    "        # Load annotations from files with labels, if applicable\n",
    "        for fn_no_ext in files_with_label:\n",
    "            fn = fn_no_ext + \".tsv\"\n",
    "            entities = {\"QUANT\": [], \"ME\": [], \"MP\": [], \"QUAL\": []}\n",
    "            with open(label_path+fn, encoding='utf-8', errors='replace') as annotfile:\n",
    "                text = textset[fn[:-4]]\n",
    "                next(annotfile)\n",
    "                annots = annotfile.read().splitlines()\n",
    "                for a in annots:\n",
    "                    annot = a.split(\"\\t\")\n",
    "                    atype = typemap[annot[2]]\n",
    "                    start = int(annot[3])\n",
    "                    stop = int(annot[4])\n",
    "                    # This is where we toss out the overlaps:\n",
    "                    overlap = False\n",
    "                    for ent in entities[atype]:\n",
    "                        if ((start >= ent[0] and start <= ent[1]) or (stop >= ent[0] and stop <= ent[1]) or\n",
    "                            (ent[0] >= start and ent[0] <= stop) or (ent[1] >= start and ent[1] <= stop)):\n",
    "                            overlap = True\n",
    "                    if overlap == False:    \n",
    "                        entities[atype].append((start, stop, atype))\n",
    "                all_data.append((text,\n",
    "                                {\"QUANT\": entities[\"QUANT\"],\n",
    "                                 \"ME\": entities[\"ME\"],\n",
    "                                 \"MP\": entities[\"MP\"],\n",
    "                                 \"QUAL\": entities[\"QUAL\"]\n",
    "                                },\n",
    "                                (fn,)\n",
    "                            ))\n",
    "\n",
    "    # Load annotations from files without label\n",
    "    for fn in files_without_label:\n",
    "        text = textset[fn]\n",
    "        all_data.append((text,\n",
    "                        {\"QUANT\": [],\n",
    "                         \"ME\": [],\n",
    "                         \"MP\": [],\n",
    "                         \"QUAL\": []\n",
    "                    },\n",
    "                    (fn,)\n",
    "                ))\n",
    "        \n",
    "    # ===== splits text data into sentences, applying processing if desired =====\n",
    "\n",
    "    cnt_toks = {\"figs.\": 0, \"fig.\": 0, \"et al.\": 0,\n",
    "                \"ref.\": 0, \"eq.\": 0, \"e.g.\": 0,\n",
    "                \"i.e.\": 0, \"nos.\": 0, \"no.\": 0,\n",
    "                \"spp.\": 0\n",
    "                }\n",
    "    regex_end_checker = [\".*[a-zA-Z]figs\\.$\", \n",
    "                        \".*[a-zA-Z]fig\\.$\",\n",
    "                        \".*[a-zA-Z]et al\\.$\",\n",
    "                        \".*[a-zA-Z]ref\\.$\",\n",
    "                        \".*[a-zA-Z]eq\\.$\",\n",
    "                        \".*[a-zA-Z]e\\.g\\.$\",\n",
    "                        \".*[a-zA-Z]i\\.e\\.$\",\n",
    "                        \".*[a-zA-Z]nos\\.$\",\n",
    "                        \".*[a-zA-Z]no\\.$\",\n",
    "                        \".*[a-zA-Z]spp\\.$\",\n",
    "                        # figs., fig., et al., Ref., Eq., e.g., i.e., Nos., No., spp.\n",
    "                    ]\n",
    "\n",
    "    assert len(cnt_toks) == len(regex_end_checker)\n",
    "\n",
    "    # list of sentences\n",
    "    # for every sentence obtained\n",
    "    # check if ends with \"fig or Figs or et al.\"\n",
    "    # keep track of the index where the this sentence starts and ends\n",
    "    # adjust pointers to tokens where changes were made\n",
    "\n",
    "    all_processed_data = []\n",
    "    for doc in all_data:\n",
    "        flag = False\n",
    "        sentences = sent_tokenize(doc[0])\n",
    "\n",
    "        fixed_sentence_tokens = []\n",
    "        curr_len = 0\n",
    "        for s in sentences:\n",
    "            if flag == True:\n",
    "                assert s[0] != ' '\n",
    "                white_length = doc[0][curr_len:].find(s[0])\n",
    "\n",
    "                prev_len = len(fixed_sentence_tokens[-1])\n",
    "                fixed_sentence_tokens[-1] = fixed_sentence_tokens[-1] + (\" \"*white_length) + s\n",
    "\n",
    "                assert fixed_sentence_tokens[-1][prev_len+white_length] == doc[0][curr_len+white_length], (fixed_sentence_tokens[-1], doc[0], curr_len, tmp_this_sent_len)\n",
    "                tmp_this_sent_len = white_length + len(s)\n",
    "                assert fixed_sentence_tokens[-1][-1] == doc[0][curr_len+tmp_this_sent_len-1], (fixed_sentence_tokens[-1], doc[0], curr_len, tmp_this_sent_len)\n",
    "                curr_len += tmp_this_sent_len\n",
    "            else:\n",
    "                if len(fixed_sentence_tokens) != 0:\n",
    "                    assert s[0] != ' '\n",
    "                    white_length = doc[0][curr_len:].find(s[0])\n",
    "                    fixed_sentence_tokens.append( (\" \"*white_length) + s )\n",
    "                else:\n",
    "                    fixed_sentence_tokens.append(s)\n",
    "                assert fixed_sentence_tokens[-1][0] == doc[0][curr_len], (fixed_sentence_tokens, doc[0], curr_len, tmp_this_sent_len)\n",
    "                tmp_this_sent_len = len(fixed_sentence_tokens[-1])\n",
    "                assert fixed_sentence_tokens[-1][-1] == doc[0][curr_len+tmp_this_sent_len-1], (fixed_sentence_tokens[-1], doc[0], curr_len, tmp_this_sent_len)\n",
    "                curr_len += tmp_this_sent_len\n",
    "\n",
    "            lower_cased_s = fixed_sentence_tokens[-1].lower()\n",
    "            flag = False\n",
    "            if remove_markers:\n",
    "                for i, k in enumerate(cnt_toks):\n",
    "                    this_regex_pattern = regex_end_checker[i]\n",
    "                    if lower_cased_s.endswith(k) and re.match(this_regex_pattern, lower_cased_s) == None:\n",
    "                        cnt_toks[k] += 1\n",
    "                        flag = True\n",
    "                        break\n",
    "\n",
    "        all_processed_data.append(fixed_sentence_tokens)      \n",
    "    print(\"Fixed sentence splitting:\", cnt_toks)\n",
    "\n",
    "        \n",
    "    # Load in annotations/associate with text from files\n",
    "    # In each loop,replace all numbers by zero,\n",
    "    # check if any of the annotations fall in between the start and end. \n",
    "    # Deal with overlaps and add offsets\n",
    "\n",
    "    normalize = lambda x: re.sub(r'\\d', '0', x)\n",
    "    all_annotated_split_data = []\n",
    "\n",
    "    for doc, sent_splits in zip(all_data, all_processed_data):\n",
    "\n",
    "        this_offsets = []\n",
    "\n",
    "        prev_end = 0\n",
    "        for s in sent_splits:\n",
    "            this_offsets.append([prev_end, prev_end+len(s)])\n",
    "            prev_end += len(s)\n",
    "\n",
    "        this_annotations = []\n",
    "        for s, offset in zip(sent_splits, this_offsets):\n",
    "            this_sent_ann = {}\n",
    "            for k,v in doc[1].items():\n",
    "                this_key_annotation_sentence = []\n",
    "\n",
    "                for ann in v:\n",
    "                    if offset[0] <= ann[0] and ann[1] < offset[1]:\n",
    "                        this_key_annotation_sentence.append((ann[0]-offset[0], ann[1]-offset[0]))\n",
    "\n",
    "                this_sent_ann[k] = this_key_annotation_sentence\n",
    "\n",
    "            this_annotations.append(this_sent_ann)\n",
    "\n",
    "        all_annotated_split_data.append({'doc_id': doc[-1][0],\n",
    "                    'sentences': [normalize(ss) for ss in sent_splits],\n",
    "                    'offsets': this_offsets,\n",
    "                    'annotations': this_annotations\n",
    "                })\n",
    "        # print(all_annotated_split_data[-1])\n",
    "        assert len(all_annotated_split_data[-1]['offsets']) == len(all_annotated_split_data[-1]['sentences'])\n",
    "        assert len(all_annotated_split_data[-1]['offsets']) == len(all_annotated_split_data[-1]['annotations'])\n",
    "        \n",
    "    bert_tok = AutoTokenizer.from_pretrained(bert_type, use_fast=True)\n",
    "    \n",
    "    # First flatten and shuffle\n",
    "    # Then Batch\n",
    "    \n",
    "    flattened = [[doc['doc_id'], doc['sentences'][i], doc['offsets'][i], doc['annotations'][i]]\n",
    "                 for doc in all_annotated_split_data for i in range(len(doc['sentences']))]\n",
    "\n",
    "    print(f'Flattened {len(all_annotated_split_data)} docs into {len(flattened)} sentences.',\n",
    "            \"\\nSome examples:\", flattened[:2])\n",
    "    \n",
    "    if label_path:\n",
    "        random.shuffle(flattened)\n",
    "\n",
    "    # cls_token_idx = bert_tok.convert_tokens_to_ids(bert_tok.tokenize('[CLS]'))[0]\n",
    "    # sep_token_idx = bert_tok.convert_tokens_to_ids(bert_tok.tokenize('[SEP]'))[0]\n",
    "    # pad_token_idx = bert_tok.convert_tokens_to_ids(bert_tok.tokenize('[PAD]'))[0]\n",
    "\n",
    "    dataset = []\n",
    "    idx = 0\n",
    "    num_data = len(flattened)\n",
    "    while idx < num_data:\n",
    "        batch_doc_ids = []\n",
    "        batch_sent_offsets = []\n",
    "        batch_raw_text = []\n",
    "        batch_raw_labels = []\n",
    "        batch_tokens = []\n",
    "        batch_token_offset = []\n",
    "\n",
    "        for single_docid, single_sentence, single_offset, single_annotations in \\\n",
    "                    flattened[idx:min(idx + batch_size, num_data)]:\n",
    "\n",
    "            batch_doc_ids.append(single_docid)\n",
    "            batch_sent_offsets.append(single_offset)\n",
    "            batch_raw_text.append(single_sentence)\n",
    "            if label_path:\n",
    "                batch_raw_labels.append(single_annotations)\n",
    "\n",
    "\n",
    "        batched_dict = bert_tok.batch_encode_plus(batch_raw_text,\n",
    "                                                    return_offsets_mapping=True,\n",
    "                                                    padding=True)\n",
    "\n",
    "        batch_tokens = torch.LongTensor(batched_dict['input_ids']).to(device)\n",
    "        batch_maxlen = batch_tokens.shape[-1]\n",
    "\n",
    "            \n",
    "        pad_masks = torch.LongTensor(batched_dict['attention_mask']).to(device)\n",
    "        batch_offset_mapping = batched_dict['offset_mapping']\n",
    "\n",
    "        if label_path:\n",
    "            \n",
    "            # Create sequence labels using token offsets\n",
    "            batch_labels = []\n",
    "            for single_token_offs, single_anns in zip(batch_offset_mapping, batch_raw_labels):\n",
    "                anns = single_anns[task]\n",
    "                single_labels = []\n",
    "                i = 0\n",
    "                for off in single_token_offs:\n",
    "                    if off == (0,0):\n",
    "                        single_labels.append(0)\n",
    "                    elif type(anns) == list and i < len(anns):\n",
    "                        if off[1] < anns[i][0]:\n",
    "                            single_labels.append(0)\n",
    "                        elif off[0] > anns[i][1]:\n",
    "                            i += 1\n",
    "                            single_labels.append(0)\n",
    "                        else:\n",
    "                            single_labels.append(1)\n",
    "                    else:\n",
    "                        single_labels.append(0)\n",
    "                batch_labels.append(single_labels)\n",
    "\n",
    "            batch_labels = torch.LongTensor(batch_labels).to(device)\n",
    "\n",
    "        b = batch_size if (idx + batch_size) < num_data else (num_data - idx)\n",
    "        assert batch_tokens.size() == torch.Size([b, batch_maxlen])\n",
    "        assert pad_masks.size() == torch.Size([b, batch_maxlen])\n",
    "        \n",
    "        if label_path:\n",
    "            assert batch_labels.size() == torch.Size([b, batch_maxlen])\n",
    "            dataset.append((batch_tokens, batch_labels, pad_masks, batch_doc_ids, batch_sent_offsets, batch_offset_mapping))\n",
    "        else:\n",
    "            dataset.append((batch_tokens, batch_raw_text, pad_masks, batch_doc_ids, batch_sent_offsets, batch_offset_mapping))\n",
    "        assert pad_masks.size() == torch.Size([b, batch_maxlen])\n",
    "        idx += batch_size\n",
    "\n",
    "    print(\"num_batches=\", len(dataset), \" | num_data=\", num_data)\n",
    "    print(\"=\"*40)\n",
    "    print(\"Dataset created!\")\n",
    "    print(\"=\"*40)\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6acf9fe3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "Preparing **training or validation** dataset, based on arguments provided.\n",
      "========================================\n",
      "Unlabeled files: 15. Labeled files: 233.\n",
      "Example of unlabeled file: S1570870512000637-1206.\n",
      "Fixed sentence splitting: {'figs.': 4, 'fig.': 92, 'et al.': 31, 'ref.': 3, 'eq.': 3, 'e.g.': 8, 'i.e.': 3, 'nos.': 2, 'no.': 3, 'spp.': 2}\n",
      "Flattened 248 docs into 1265 sentences. \n",
      "Some examples: [['S0006322312001096-1136.tsv', 'Data were drawn from the Whitehall II study with baseline examination in 0000; follow-up screenings in 0000, 0000, and 0000; and additional disease ascertainment from hospital data and registry linkage on 0000 participants (mean age 00.0 years, 00% women) without depressive symptoms at baseline.', [0, 296], {'QUANT': [(73, 77), (103, 123), (205, 222), (233, 243), (245, 248)], 'ME': [(49, 69), (79, 99), (25, 43), (205, 222)], 'MP': [(224, 232), (249, 254)], 'QUAL': []}], ['S0006322312001096-1136.tsv', ' Vascular risk was assessed with the Framingham Cardiovascular, Coronary Heart Disease, and Stroke Risk Scores.', [296, 407], {'QUANT': [], 'ME': [], 'MP': [], 'QUAL': []}]]\n",
      "num_batches= 10  | num_data= 1265\n",
      "========================================\n",
      "Dataset created!\n",
      "========================================\n"
     ]
    }
   ],
   "source": [
    "train_text = \"../data/raw/train/text/\"\n",
    "train_labels = \"../data/raw/train/tsv/\"\n",
    "bert_type = \"roberta-base\"\n",
    "\n",
    "task = 'QUANT'\n",
    "\n",
    "train_dataset = prep_dataset(text_path=train_text, task=task, bert_type=bert_type, label_path=train_labels, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ff396320",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "Preparing **training or validation** dataset, based on arguments provided.\n",
      "========================================\n",
      "Unlabeled files: 0. Labeled files: 65.\n",
      "Fixed sentence splitting: {'figs.': 6, 'fig.': 61, 'et al.': 22, 'ref.': 0, 'eq.': 2, 'e.g.': 5, 'i.e.': 0, 'nos.': 0, 'no.': 0, 'spp.': 4}\n",
      "Flattened 65 docs into 420 sentences. \n",
      "Some examples: [['S0012821X12004384-1302.tsv', 'Correspondence analysis (CA) and statistical diversity analysis were carried out on the palynological dataset (total counts per gram) to confirm assemblage designations (Figs. 0 and 0), to identify any disturbance to the core prior to interpretation, and to estimate diversity (Fig. 0).', [0, 286], {'QUANT': [], 'ME': [], 'MP': [], 'QUAL': []}], ['S0012821X12004384-1302.tsv', ' Dinoflagellate cyst assemblages (DA0–DA0) and pollen assemblages (PA0–PA0) were defined by visually comparing changes in the species dominance (Figs. 0 and 0), and confirmed by CA (Fig. 0) using the first three axes (describing the highest percentages of variance).', [286, 552], {'QUANT': [], 'ME': [], 'MP': [], 'QUAL': []}]]\n",
      "num_batches= 4  | num_data= 420\n",
      "========================================\n",
      "Dataset created!\n",
      "========================================\n"
     ]
    }
   ],
   "source": [
    "trial_text = \"../data/raw/trial/txt/\"\n",
    "trial_labels = \"../data/raw/trial/tsv/\"\n",
    "bert_type = \"roberta-base\"\n",
    "\n",
    "valid_dataset = prep_dataset(text_path=trial_text, task='QUANT', bert_type=bert_type, label_path=trial_labels, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d20000df",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OurBERTModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(OurBERTModel, self).__init__()\n",
    "        self.bert = AutoModel.from_pretrained(bert_type)\n",
    "        self.drop = nn.Dropout(self.bert.config.hidden_dropout_prob)\n",
    "        self.classifier = nn.Linear(self.bert.config.hidden_size, 2)\n",
    "\n",
    "    def forward(self, text, att_mask):\n",
    "        b, num_tokens = text.shape\n",
    "        token_type = torch.zeros((b, num_tokens), dtype=torch.long).to(device)\n",
    "        outputs = self.bert(text, attention_mask=att_mask, token_type_ids=token_type)\n",
    "        return self.classifier(self.drop(outputs['last_hidden_state']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "df5fccb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model created. Model has 124647170 parameters.\n"
     ]
    }
   ],
   "source": [
    "model = OurBERTModel()\n",
    "print(\"Model created. Model has\", sum(p.numel() for p in model.parameters()), \"parameters.\")\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2c3b4899",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(bert_model, task: str='QUANT', n_epochs: int=6, lr: float=3e-05, train_dataset: list=train_dataset, \n",
    "                valid_dataset: list=valid_dataset, torch_seed: int=434, dummy_run: bool=False):\n",
    "    \n",
    "    \"\"\"\n",
    "    Build and train BERT model according to specifications provided.\n",
    "    =====================================================\n",
    "    Inputs:\n",
    "    bert_model (OurBERTModel): model to train\n",
    "    task (str): which MeasEval task to train on\n",
    "    n_epochs (int): number of epochs to train the model\n",
    "    lr (float): learning rate\n",
    "    train_dataset (list): training dataset prepared with prep_dataset func\n",
    "    valid_dataset (list): validation dataset prepared with prep_dataset func\n",
    "    torch_seed (int): seed number for (some) reproducibility\n",
    "    dummy_run (bool): flag to not run full evaluation and only test pipeline\n",
    "    =====================================================\n",
    "    Outputs:\n",
    "    dataset (list): flattened/tokenized/batched sentences w/annotations\n",
    "    \"\"\"\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        device = 'cuda'\n",
    "        print(\"Detected\", torch.cuda.device_count(), \"GPUs; will train using CUDA.\")\n",
    "    else:\n",
    "        device = 'cpu'\n",
    "        print(\"No CUDA-enabled GPUs detected; will train using CPU.\")\n",
    "        \n",
    "    torch.manual_seed(torch_seed)\n",
    "\n",
    "    \n",
    "    #### Train one epoch ####\n",
    "    def train(model, batched_dataset, criterion):\n",
    "        model.train() # Put the model to train\n",
    "\n",
    "        # Lets keep track of the losses at each update\n",
    "        train_losses = []\n",
    "        num_batch = 0\n",
    "\n",
    "        for batch in batched_dataset:\n",
    "            # Unpack the batch\n",
    "            (texts, labels, att_masks, doc_ids, offsets, token_offsets) = batch\n",
    "\n",
    "            # Make predictions on the model\n",
    "            preds = model(texts, att_masks)\n",
    "\n",
    "            # Take into account padded while calculating loss\n",
    "            loss_unreduced = criterion(preds.permute(0,2,1), labels)\n",
    "            loss = (loss_unreduced * att_masks).sum() / (att_masks).sum()\n",
    "\n",
    "            # Update model weights\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            if num_batch % 10 == 0:\n",
    "                print(\"Train loss at {}:\".format(num_batch), loss.item())\n",
    "\n",
    "            num_batch += 1\n",
    "            # Append losses\n",
    "            train_losses.append(loss.item())\n",
    "\n",
    "        return np.average(train_losses)\n",
    "    \n",
    "    def evaluate(model, batched_dataset, criterion):\n",
    "        target_names=[\"NOT_\" + task, task]\n",
    "\n",
    "        # Put the model to eval mode\n",
    "        model.eval()\n",
    "\n",
    "        # Keep track on predictions\n",
    "        valid_losses = []\n",
    "        predicts = []\n",
    "        gnd_truths = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch in batched_dataset:\n",
    "                # Unpack the batch\n",
    "                (texts, labels, att_masks, doc_ids, offsets, token_offsets) = batch\n",
    "                # Make predictions on the model\n",
    "                preds = model(texts, att_masks)\n",
    "\n",
    "                # Take into account padded while calculating loss\n",
    "                loss_unreduced = criterion(preds.permute(0,2,1), labels)\n",
    "                loss = (loss_unreduced * att_masks).sum() / (att_masks).sum()\n",
    "\n",
    "                # Get argmax of non-padded tokens\n",
    "                for sent_preds, sent_labels, sent_att_masks in zip(preds, labels, att_masks):\n",
    "                    for token_preds, token_labels, token_masks in zip(sent_preds, sent_labels, sent_att_masks):\n",
    "                        if token_masks.item() != 0:\n",
    "                            predicts.append(token_preds.argmax().item())\n",
    "                            gnd_truths.append((token_labels.item()))\n",
    "                valid_losses.append(loss.item())\n",
    "\n",
    "                assert len(predicts) == len(gnd_truths)\n",
    "\n",
    "        # Create confusion matrix and evaluate on the predictions\n",
    "        confuse_mat = confusion_matrix(gnd_truths, predicts)\n",
    "        if dummy_run:\n",
    "            classify_report = None\n",
    "        else:\n",
    "            classify_report = classification_report(gnd_truths, predicts,\n",
    "                                            target_names=target_names,\n",
    "                                            output_dict=True)\n",
    "\n",
    "        mean_valid_loss = np.average(valid_losses)\n",
    "        print(\"Valid_loss\", mean_valid_loss)\n",
    "        print(confuse_mat)\n",
    "\n",
    "        if not dummy_run:\n",
    "            for labl in target_names:\n",
    "                print(labl,\"F1-score:\", classify_report[labl][\"f1-score\"])\n",
    "            print(\"Accu:\", classify_report[\"accuracy\"])\n",
    "            print(\"F1-Weighted\", classify_report[\"weighted avg\"][\"f1-score\"])\n",
    "            print(\"F1-Avg\", classify_report[\"macro avg\"][\"f1-score\"])\n",
    "\n",
    "        return mean_valid_loss, confuse_mat ,classify_report\n",
    "    \n",
    "    ########## Optimizer & Loss ###########\n",
    "\n",
    "    criterion = torch.nn.CrossEntropyLoss(reduction='none')\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    ########## Training loop ###########\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        print(\"\\n\\n========= Beginning epoch\", epoch+1,\"==========\")\n",
    "\n",
    "        train_loss = train(model, train_dataset, criterion)\n",
    "        print(\"\\n==== EVALUATING On Training set ====\\n\")\n",
    "        _, _, _ = evaluate(model, train_dataset, criterion)\n",
    "\n",
    "        print(\"\\n==== EVALUATING On Validation set ====\\n\")\n",
    "        valid_loss, confuse_mat, classify_report = evaluate(model, valid_dataset, criterion)\n",
    "\n",
    "        epoch_len = len(str(n_epochs))\n",
    "        print_msg = (f'[{epoch+1:>{epoch_len}}/{n_epochs:>{epoch_len}}]     ' +\n",
    "                        f'train_loss: {train_loss:.5f} ' +\n",
    "                        f'valid_loss: {valid_loss:.5f}')\n",
    "        print(print_msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b5fb744",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected 1 GPUs; will train using CUDA.\n",
      "\n",
      "\n",
      "========= Beginning epoch 1 ==========\n",
      "Train loss at 0: 0.7087559103965759\n",
      "\n",
      "==== EVALUATING On Training set ====\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_model(model, task=task, n_epochs=6, lr=3e-05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "279b0897",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict spans on Test, Train and Val set\n",
    "\n",
    "def predict_spans(batched_dataset, save_path):\n",
    "    # Put the model to eval mode and track the predictions\n",
    "        \n",
    "    model.eval()\n",
    "    valid_losses = []\n",
    "    correct_sentences = 0\n",
    "    total_sentences = 0 \n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in valid_dataset:\n",
    "            # Unpack the batch and feed into the model\n",
    "            (texts, labels, att_masks, doc_ids, offsets, token_offsets) = batch\n",
    "            preds = model(texts, att_masks)\n",
    "\n",
    "            # Check for mispredicts\n",
    "            for sent_preds, sent_labels, sent_att_masks, sent_doc_id, sent_offset in zip(preds, labels, att_masks, doc_ids, offsets):\n",
    "                this_correct = (sent_preds.argmax(1) == sent_labels).sum()\n",
    "                this_total = len(sent_labels)\n",
    "\n",
    "                if this_correct != this_total:\n",
    "                    print(\"Mispredict:\", sent_doc_id, \"for sentence at offset\", sent_offset)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        span_dict = {}\n",
    "        for batch in batched_dataset:\n",
    "            (texts, raw_texts, att_masks, doc_ids, offsets, token_offsets) = batch\n",
    "            preds = model(texts, att_masks)\n",
    "\n",
    "            for sent_preds, sent_raw_text, sent_att_masks, sent_doc_id, sent_offset, sent_token_offsets in zip(preds, raw_texts, att_masks, doc_ids, offsets, token_offsets):\n",
    "\n",
    "                this_sentence_positives = []\n",
    "                curr_positive_idx = -1\n",
    "                for i, (token_preds, token_labels, token_masks) in enumerate(zip(sent_preds, sent_labels, sent_att_masks)):\n",
    "                    if token_masks.item() != 0:\n",
    "                        if token_preds.argmax().item() == 1:\n",
    "                            if curr_positive_idx == -1:\n",
    "                                curr_positive_idx = i\n",
    "                        else:\n",
    "                            if curr_positive_idx != -1:\n",
    "                                this_sentence_positives.append([curr_positive_idx, i-1])\n",
    "                                curr_positive_idx = -1\n",
    "                    else:\n",
    "                        if curr_positive_idx != -1:\n",
    "                            this_sentence_positives.append([curr_positive_idx, i-1])\n",
    "                            curr_positive_idx = -1\n",
    "                            break\n",
    "\n",
    "                # Here convert indices to offsets\n",
    "                if sent_doc_id not in span_dict.keys():\n",
    "                    span_dict[sent_doc_id] = {}\n",
    "\n",
    "                this_sent_spans = []\n",
    "                for span_offsets in this_sentence_positives:\n",
    "                    this_sent_spans.append([sent_token_offsets[span_offsets[0]][0],\n",
    "                                            sent_token_offsets[span_offsets[1]][1]\n",
    "                                        ])\n",
    "                \n",
    "                assert sent_offset[0] not in span_dict[sent_doc_id].keys()\n",
    "                span_dict[sent_doc_id][sent_offset[0]] = this_sent_spans\n",
    "    \n",
    "    json.dump(span_dict, open(save_path, 'w+'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5f5434b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model and prediction\n",
    "\n",
    "eval_doc_path = \"../data/raw/eval/text/\"\n",
    "\n",
    "folder_name = \"../span_predictions_\" + bert_type.replace('/', '_') + '_' + datetime.today().strftime('%Y-%m-%d_%H_%M_%S')\n",
    "if os.path.isdir(folder_name):\n",
    "    os.system(\"rm -rf \" + folder_name)\n",
    "os.mkdir(folder_name)\n",
    "\n",
    "test_dataset = prep_dataset(eval_doc_path, task='QUANT')\n",
    "\n",
    "predict_spans(train_dataset, folder_name+\"/train_spans.json\")\n",
    "predict_spans(valid_dataset, folder_name+\"/trial_spans.json\")\n",
    "predict_spans(test_dataset, folder_name+\"/test_spans.json\")\n",
    "\n",
    "# don't write model to repo haha, it's ~500 MB...\n",
    "# torch.save(model.state_dict(), folder_name+\"/model.pt\")\n",
    "\n",
    "print(\"=\"*40)\n",
    "print(\"Predictions complete and written to file!\")\n",
    "print(\"=\"*40)\n",
    "\n",
    "## Add code to write parameters used to file (lr, epochs, what else?)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "measeval-kernel",
   "language": "python",
   "name": "measeval-kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
