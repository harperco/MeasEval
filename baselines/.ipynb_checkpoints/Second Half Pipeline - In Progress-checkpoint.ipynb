{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9b19afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "import re\n",
    "from os import path\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50bb20e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_modifiers(data_filepath: str, output_filepath: str, quant_filepath: str)\n",
    "\n",
    "    # Extracted keywords and units for faster processing   \n",
    "    list_units = ['varves','hour','s−1','g mol−1','mg/l','kgs-1','beach materials','kW','g/m3','scale heights','μs','×','cm−1','mm2','mM'\n",
    "            'Mbps','mbar','hr','passages','participants','μbar','day','m2 g−1','mg','oN','μg/m','kg','GPa','μM','women'\n",
    "            'stems/ha','nbar','vertices','U/ml','m thick','month','ppt','W m−2','centimeters','second','mg/ml','mWm−2','°C','μg/L','wt.%'\n",
    "            'times','months','ppq','M','km','mBar','cm2/Vs','Ma','H','point','monomer units','mA/cm2','nT','cm3 g−1','m'\n",
    "            '°','metre','°N','bp','v/v','Mt','cores','Whitehall II participants','Rs','m/s','mdeg','Torr','Å/s','occasions','clones'\n",
    "            'men','%','kHz','MPa','mH','MW','orders of magnitude','year-old','m s−1','UT','hours','h','μL','weeks','discrete particles'\n",
    "            '% per year','wt%','cm−3','kV','fold','lines of code','days','nJ','years','km/h','week','km s−1','ppm','mmol/L','year'\n",
    "            'Rp','elderly participants','g cm−3','bit','AU','MeV','mm per side','° latitude','mm3','yrs','pairs per mm','K/min',\n",
    "            'Mg ha−1 year−1','wt. %','K''men','%','kHz','MPa','mH','MW','orders of magnitude','year-old','m s−1','UT','hours','h','μL','weeks','discrete particles'\n",
    "            'byr','kR','s','mA g− 1','°S','min','mbsl','m−2','°C/min','W/m2','minutes','percentage points per year','nm','vol%','degrees'\n",
    "            'mg cm− 2','pH','keV','mA','ka','employees','fold per passage','mm','RRh','eV','ms','μg','kg s−1','w/w','item'\n",
    "            'μg/ml','μm','p0','g/L','horizons','V','Hz','SDG vertices','cm','items','percent','mg/L','ppm by mass','‰','g'\n",
    "            'Saturn radii RS'\n",
    "        ]\n",
    "\n",
    "    set_units = set(list_units)\n",
    "    print(set_units)\n",
    "\n",
    "    # IsApproximate IsCount IsRange IsList IsMean IsMedian IsMeanHasSD IsMeanHasTolerance IsRangeHasTolerance HasTolerance                 \n",
    "\n",
    "    revlist = sorted(list(set_units), reverse = True, key = len) # List of units\n",
    "    # print(revlist)\n",
    "\n",
    "    # we set non alphabet as the word boundary in our regex\n",
    "    listcounts = ['[^a-z]half[^a-z]', '[^a-z]quarter[^a-z]', '[^a-z]one[^a-z]', '[^a-z]two[^a-z]', '[^a-z]three[^a-z]', '[^a-z]four[^a-z]', '[^a-z]five[^a-z]', '[^a-z]six[^a-z]', '[^a-z]seven[^a-z]', '[^a-z]eight[^a-z]', '[^a-z]nine[^a-z]', '[^a-z]ten[^a-z]', \n",
    "                  '[^a-z]eleven[^a-z]', '[^a-z]twelve[^a-z]', '[^a-z]thirteen[^a-z]','[^a-z]fourteen[^a-z]','[^a-z]fifteen[^a-z]','[^a-z]sixteen[^a-z]','[^a-z]seventeen[^a-z]','[^a-z]eighteen[^a-z]','[^a-z]nineteen[^a-z]','[^a-z]twenty[^a-z]',\n",
    "                  '[^a-z]thirty[^a-z]','[^a-z]forty[^a-z]','[^a-z]fifty[^a-z]','[^a-z]sixty[^a-z]','[^a-z]seventy[^a-z]','[^a-z]eighty[^a-z]','[^a-z]ninety[^a-z]','[^a-z]hundred[^a-z]',\n",
    "                  '[^a-z]thousand[^a-z]','[^a-z]million[^a-z]','[^a-z]billion[^a-z]','[^a-z]trillion[^a-z]']\n",
    "\n",
    "    listcounts2 = []\n",
    "    listcounts3 = []\n",
    "    listcounts4 = []\n",
    "    for item in listcounts:\n",
    "        temp = \"^\"+item[6:]\n",
    "        listcounts2.append(temp)\n",
    "\n",
    "    # print(listcounts2)\n",
    "\n",
    "    for item in listcounts:\n",
    "        temp = item[0:len(item)-6] + \"$\"\n",
    "        listcounts3.append(temp)\n",
    "\n",
    "    # print(listcounts3)\n",
    "\n",
    "    for item in listcounts:\n",
    "        temp = \"^\"+item[6:len(item)-6]+\"$\"\n",
    "        listcounts4.append(temp)\n",
    "\n",
    "    # print(listcounts4)\n",
    "\n",
    "    listcounts.extend(listcounts2)\n",
    "    listcounts.extend(listcounts3)\n",
    "    listcounts.extend(listcounts4)\n",
    "    # print(listcounts)\n",
    "\n",
    "    listapproximate = ['∼','~', 'about', 'around', 'close to', 'the order of','approximately', 'nominally', 'near', 'roughly', 'almost', 'approximation',\n",
    "    '≈', 'circa']\n",
    "\n",
    "    listmean = ['average', 'mean']\n",
    "\n",
    "    listmedian = ['median']\n",
    "\n",
    "    listrange = ['to', 'from', 'below', 'beyond', 'above', 'between', 'up to', '<', '>', 'upper',\n",
    "    'greater', 'lesser', 'bigger', 'smaller', 'more than', 'less than', '≥', '≤', 'within',\n",
    "    'throughout', 'at least', 'or more', 'or less', 'past', 'higher', 'almost', 'high', 'before', 'after',\n",
    "    'over', 'under', 'range', 'ranging', 'top', 'at most', 'down to', '⩽', '⩾', '≳', 'as much as','±']\n",
    "\n",
    "    listhyphen = ['−', '-']\n",
    "\n",
    "    listlist = ['or', 'and']\n",
    "\n",
    "\n",
    "\n",
    "    '''\n",
    "    We set any non alphabet Character as the word boundary.\n",
    "    Since scientific document can have non-unicode\n",
    "    ''' \n",
    "\n",
    "\n",
    "    def findmodifier(sent, start_off, end_off):\n",
    "        Numpresent = False\n",
    "        IsUnit = False\n",
    "        IsApproximate = False\n",
    "        IsCount = True\n",
    "        IsRange = False\n",
    "        IsList = False \n",
    "        Mean = False\n",
    "        IsMean = False\n",
    "        IsMedian = False\n",
    "        Tolerance = False\n",
    "        Range = False\n",
    "        IsMeanHasSD = False\n",
    "        IsMeanHasTolerance = False\n",
    "        IsRangeHasTolerance = False\n",
    "        HasTolerance = False\n",
    "        Unit = \"\"\n",
    "\n",
    "        # We find if plus-minus exists\n",
    "\n",
    "\n",
    "        t_sent = sent[start_off:end_off]\n",
    "        print(\"Span is: \", t_sent)\n",
    "\n",
    "        if re.search(\"[^a-z][0-9]|^[0-9]\", t_sent.lower()) is not None:\n",
    "            Numpresent = True\n",
    "        else:\n",
    "            for item in listcounts:\n",
    "                if re.search(item, t_sent.lower()) is not None:\n",
    "                    Numpresent = True\n",
    "\n",
    "        if Numpresent is False:\n",
    "            IsCount = False\n",
    "            return [\"null\", IsApproximate, IsCount, IsRange, IsList, IsMean, IsMedian, IsMeanHasSD, IsMeanHasTolerance, IsRangeHasTolerance, HasTolerance]\n",
    "\n",
    "        for unit in revlist:\n",
    "            if re.search(\"[^a-zA-Z]\"+unit+\"[^a-zA-Z]\"+'|'+\"[^a-zA-Z]\"+unit+\"$\", sent[start_off:end_off]) is not None:\n",
    "                Unit+=unit\n",
    "                IsUnit = True\n",
    "                IsCount = False\n",
    "                break\n",
    "\n",
    "        if '±' in t_sent:\n",
    "            IsCount = False\n",
    "            Tolerance = True\n",
    "\n",
    "        for item in listmean:\n",
    "            tmp = re.search(\"[^a-z]\"+item+\"[^a-z]\"+'|'+\"[^a-z]\"+item+\"$\" +'|'+\"^\"+item+\"[^a-z]\", sent[start_off:end_off].lower())\n",
    "            if tmp is not None:\n",
    "                Mean = True\n",
    "\n",
    "\n",
    "        if re.search(\"[^a-z]\"+\"sd\"+\"[^a-z]\"+\"|\"+\"[^a-z]\"+\"sd\"+\"$\", sent[start_off-20:end_off].lower()) is True:\n",
    "            IsMeanHasSD = True\n",
    "\n",
    "        for item in listrange:\n",
    "            tmp = re.search(\"[^a-z]\"+item+\"[^a-z]\"+'|'+\"[^a-z]\"+item+\"$\" +'|'+\"^\"+item+\"[^a-z]\", t_sent.lower())# sent[max(0,start_off-10):end_off].lower())\n",
    "            if tmp is not None:\n",
    "                Range = True\n",
    "\n",
    "        tmp = re.search(\"[−|-]\", t_sent)\n",
    "        if tmp is not None:\n",
    "            tmp1 = re.search(\"[0-9]+\", sent[max(0,tmp.start()+start_off-10):tmp.start()+start_off])\n",
    "            if tmp1 is not None:\n",
    "                Range = True\n",
    "\n",
    "        if re.search('[^a-z]'+'median'+'[^a-z]'+'|'+'^'+'median'+'[^a-z]'+'|'+'[^a-z]'+'median'+'$', t_sent.lower()):\n",
    "            IsMedian = True\n",
    "\n",
    "        tmp = re.search(\"[^a-z]and[^a-z]|[^a-z]or[^a-z]\", t_sent.lower())\n",
    "        if tmp is not None:\n",
    "            tmp1 = re.search(\"[0-9]+\", sent[start_off+tmp.end():end_off])\n",
    "            tmp2 = re.search(\"[0-9]+\", sent[max(0,start_off-1):(tmp.start()+start_off)])\n",
    "            if tmp1 is not None and tmp2 is not None:\n",
    "                if Range is not True:\n",
    "                    IsList = True\n",
    "                IsCount = False\n",
    "\n",
    "\n",
    "        for item in listapproximate:\n",
    "            tmp = re.search(\"[^a-z]\"+item+\"[^a-z]\"+'|'+\"[^a-z]\"+item+\"$\" +'|'+\"^\"+item+\"[^a-z]\", sent[max(0,start_off-30):end_off].lower())\n",
    "            if tmp is not None:\n",
    "                IsApproximate = True\n",
    "\n",
    "        if Range and not Tolerance:\n",
    "            IsRange = True\n",
    "        if IsMean and Tolerance and not IsMeanHasSD:\n",
    "            IsMeanHasTolerance = True\n",
    "        if Tolerance and Range:\n",
    "            IsRangeHasTolerance = True\n",
    "        if Tolerance and not IsMeanHasSD and not IsMeanHasTolerance and not IsRangeHasTolerance:\n",
    "            HasTolerance = True\n",
    "\n",
    "        if IsUnit is True:\n",
    "            return [Unit, IsApproximate, IsCount, IsRange, IsList, IsMean, IsMedian, IsMeanHasSD, IsMeanHasTolerance, IsRangeHasTolerance, HasTolerance]\n",
    "        else:\n",
    "            return [\"null\", IsApproximate, IsCount, IsRange, IsList, IsMean, IsMedian, IsMeanHasSD, IsMeanHasTolerance, IsRangeHasTolerance, HasTolerance]\n",
    "\n",
    "\n",
    "    def convert_output_to_dict(preds):\n",
    "        label_dict = {}\n",
    "        if preds[0] != \"null\":\n",
    "            label_dict['unit'] = preds[0]\n",
    "\n",
    "        mods = []\n",
    "\n",
    "        mods_order_ = [\"IsApproximate\", \"IsCount\", \"IsRange\", \"IsList\", \"IsMean\", \"IsMedian\",\n",
    "                        \"IsMeanHasSD\", \"IsMeanHasTolerance\", \"IsRangeHasTolerance\",\"HasTolerance\"\n",
    "                    ]\n",
    "\n",
    "        for i, bool_value in enumerate(preds):\n",
    "            if i == 0:\n",
    "                pass\n",
    "            else:\n",
    "                if bool_value:\n",
    "                    mods.append(mods_order_[i-1])\n",
    "\n",
    "        if len(mods) > 0:\n",
    "            label_dict[\"mods\"] = mods\n",
    "\n",
    "        if len(label_dict) == 0:\n",
    "            return \"\"\n",
    "\n",
    "        return str(label_dict)\n",
    "\n",
    "    import json\n",
    "\n",
    "    def label_and_dump(text_path, quant_path, save_path, trial_or_train=False):\n",
    "        all_text = {f[:-4]: open(text_path + f, encoding=\"utf8\").read() for f in os.listdir(text_path)}\n",
    "\n",
    "\n",
    "        if trial_or_train:\n",
    "            trim_tsv = lambda x: x[:-4] if x[-4:] == \".tsv\" else x\n",
    "            quant_labels = {trim_tsv(k):v for k, v in json.load(open(quant_path)).items()}\n",
    "        else:\n",
    "            quant_labels = json.load(open(quant_path))\n",
    "\n",
    "        '''\n",
    "        Quant_labels:\n",
    "        Dict{\n",
    "                Doc1_id: {sent1_offset: [list of quant offsets for sent1],\n",
    "                        sent2_offset: [list of quant offsets for sent2],\n",
    "                            ....\n",
    "                        }\n",
    "                Doc2_id ....\n",
    "                ....\n",
    "        }\n",
    "        Output:\n",
    "        Dict{\n",
    "                Doc1_id: [[docId, annotSet, annotType, startOffset, endOffset, annotId, text, other, sentence_start, sentence_end]\n",
    "                            repeat same for second quant identified\n",
    "                            ....\n",
    "                        ]\n",
    "                Doc2_id ....\n",
    "                ....\n",
    "        }\n",
    "        '''\n",
    "        assert set(sorted(all_text.keys())) == set(sorted(quant_labels.keys())), (quant_labels.keys(), \"   |||||    \", all_text.keys())\n",
    "\n",
    "        task2_output = {}\n",
    "        for docid in quant_labels.keys():\n",
    "            doc_txt = all_text[docid]\n",
    "            ann_id = 1\n",
    "            this_doc_labels = []\n",
    "            this_sents_offs = sorted([int(x) for x in list(quant_labels[docid].keys())])\n",
    "            for i, sent_offset in enumerate(this_sents_offs):\n",
    "                this_sentence_start = sent_offset\n",
    "                this_sentence_end = this_sents_offs[i+1] if (i +1) < len(this_sents_offs) else len(doc_txt)\n",
    "                sent = doc_txt[this_sentence_start:this_sentence_end]\n",
    "\n",
    "                sent_offset_str = str(sent_offset)\n",
    "                single_sent_quant_offs = quant_labels[docid][sent_offset_str]\n",
    "\n",
    "                for single_quant_offs in single_sent_quant_offs:\n",
    "                    others_val = convert_output_to_dict(findmodifier(sent, single_quant_offs[0], single_quant_offs[1]))\n",
    "\n",
    "                    this_offset = (single_quant_offs[0] + sent_offset, single_quant_offs[1] + sent_offset)\n",
    "                    this_doc_labels.append([docid, ann_id, \"Quantity\",\n",
    "                                            this_offset[0], this_offset[1], ann_id,\n",
    "                                            doc_txt[this_offset[0]:this_offset[1]],\n",
    "                                            others_val, this_sentence_start, this_sentence_end\n",
    "                                            ])\n",
    "                    ann_id += 1\n",
    "\n",
    "            task2_output[docid] = this_doc_labels\n",
    "\n",
    "        json.dump(task2_output, open(save_path, 'w+'), indent=2)\n",
    "\n",
    "    train_doc_path = data_filepath + \"train/text/\"\n",
    "    trial_doc_path = data_filepath + \"trial/txt/\"\n",
    "    test_doc_path = data_filepath + \"eval/text/\"\n",
    "    bert_type = (quant_filepath[:-1]).split('/')[-1]\n",
    "    print(\"Bert type:\", bert_type)\n",
    "    folder_name = output_filepath + bert_type.replace('/', '_') + '_' + datetime.today().strftime('%Y-%m-%d_%H_%M_%S')\n",
    "    if os.path.isdir(folder_name):\n",
    "        os.system(\"rm -rf \" + folder_name)\n",
    "    os.mkdir(folder_name)\n",
    "\n",
    "    print(\"\\n\\n\\n====================================================\")\n",
    "    print(\"====================== Train =======================\")\n",
    "    print(\"====================================================\\n\\n\\n\")\n",
    "\n",
    "\n",
    "    label_and_dump(train_doc_path, quant_filepath + \"train_spans.json\",\n",
    "                    os.path.join(folder_name, \"train_labels.json\"), True\n",
    "                )\n",
    "\n",
    "    print(\"\\n\\n\\n====================================================\")\n",
    "    print(\"======================= Trial ======================\")\n",
    "    print(\"====================================================\\n\\n\\n\")\n",
    "\n",
    "    label_and_dump(trial_doc_path, quant_filepath + \"trial_spans.json\",\n",
    "                    os.path.join(folder_name, \"trial_labels.json\"), True\n",
    "                )\n",
    "\n",
    "    print(\"\\n\\n\\n====================================================\")\n",
    "    print(\"======================= Test =======================\")\n",
    "    print(\"====================================================\\n\\n\\n\")\n",
    "\n",
    "    label_and_dump(test_doc_path, quant_filepath + \"test_spans.json\",\n",
    "                    os.path.join(folder_name + \"/test_labels.json\"), False\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f55246c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_modifiers(data_filepath=\"../data/\", output_filepath: \"../outputs/modifiers_\", quant_filepath=\"../outputs/span_predictions_roberta-base_2022-07-13_01_48_39\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "measeval-kernel",
   "language": "python",
   "name": "measeval-kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
