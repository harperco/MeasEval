{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import os\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoConfig, AutoProcessor\n",
    "from transformers import DataCollatorWithPadding\n",
    "from transformers import AutoModel, TrainingArguments, Trainer\n",
    "# from transformers import RobertaTokenizer, RobertaModel\n",
    "\n",
    "random.seed(42)\n",
    "percent_to_train = .7\n",
    "\n",
    "model_name = 'allenai/biomed_roberta_base'\n",
    "dropout = .03\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "currentdir = os.getcwd() # ~/MeasEval/baselines\n",
    "\n",
    "trainpaths = [os.path.join(currentdir, \"../data/trial/tsv/\"),\n",
    "             os.path.join(currentdir, \"../data/train/tsv/\")]\n",
    "\n",
    "evalpath = os.path.join(currentdir, \"../data/eval/text/\")\n",
    "\n",
    "textpaths = [os.path.join(currentdir, \"../data/trial/txt/\"),\n",
    "            os.path.join(currentdir, \"../data/train/text/\")]\n",
    "\n",
    "# Set shorthands for annotation spans\n",
    "typemap = {\"Quantity\": \"QUANT\",\n",
    "           \"MeasuredEntity\": \"ME\", \n",
    "           \"MeasuredProperty\": \"MP\", \n",
    "           \"Qualifier\": \"QUAL\"}\n",
    "\n",
    "# Collect all the ids and all the text files in both the train and trial directories\n",
    "# Set our train test split for doing initial model development.\n",
    "docIds = []\n",
    "textset = {}\n",
    "for fileset in textpaths:\n",
    "    for fn in os.listdir(fileset):\n",
    "        path = fileset+fn\n",
    "        #print(fn)\n",
    "        with open(path) as textfile:\n",
    "            text = textfile.read()\n",
    "            #[:-4] strips off the .txt to get the id\n",
    "            textset[fn[:-4]] = text\n",
    "            docIds.append(fn[:-4])\n",
    "\n",
    "random.shuffle(docIds)\n",
    "\n",
    "tt_split = int(np.round(len(docIds) * percent_to_train))\n",
    "\n",
    "trainIds = docIds[:tt_split]\n",
    "testIds = docIds[tt_split:]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first 5 train IDs: \n",
      "['S2213671113000738-647', 'S2213671113001306-1398', 'S0925443913001385-1646', 'S1750583613004192-714', 'S0925443913001385-1429']\n",
      "first 5 test IDs: \n",
      "['S0019103512004009-4350', 'S016412121300188X-5038', 'S0022459611006116-1448', 'S030881461301604X-1002', 'S016412121300188X-4937']\n",
      "folders containing data and annotations: \n",
      "['/home/sam/MeasEval/baselines/../data/trial/txt/', '/home/sam/MeasEval/baselines/../data/train/text/']\n"
     ]
    }
   ],
   "source": [
    "print(f\"first 5 train IDs: \\n{trainIds[:5]}\")\n",
    "\n",
    "print(f\"first 5 test IDs: \\n{testIds[:5]}\")\n",
    "\n",
    "print(f\"folders containing data and annotations: \\n{textpaths}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build training data from TSVs in expected format for spacy NER models...\n",
    "# We have to train each model separately, because spacy doesn't let us have \n",
    "# Multiple entities that overlap, and we have this a lot (Especially in our Qualifiers)\n",
    "# Unfortunately, we even have a fair bit of overlap within annotation types, \n",
    "# and end up needing to throw away a bunch of training data.\n",
    "\n",
    "# Note that we have data split for train / test, and we also have full training data.\n",
    "\n",
    "trainents = {\"QUANT\": [], \"ME\": [], \"MP\": [], \"QUAL\": []}\n",
    "traindata = {\"QUANT\": [], \"ME\": [], \"MP\": [], \"QUAL\": []}\n",
    "testents = {\"QUANT\": [], \"ME\": [], \"MP\": [], \"QUAL\": []}\n",
    "testdata = {\"QUANT\": [], \"ME\": [], \"MP\": [], \"QUAL\": []}\n",
    "\n",
    "alltrainents = {\"QUANT\": [], \"ME\": [], \"MP\": [], \"QUAL\": []}\n",
    "alltraindata = {\"QUANT\": [], \"ME\": [], \"MP\": [], \"QUAL\": []}\n",
    "\n",
    "for fileset in trainpaths:\n",
    "    for fn in os.listdir(fileset):\n",
    "        entities = {\"QUANT\": [], \"ME\": [], \"MP\": [], \"QUAL\": []}\n",
    "        with open(fileset+fn) as annotfile:\n",
    "            text = textset[fn[:-4]]\n",
    "\n",
    "            # Overlap handler (not required for RoBERTa??)\n",
    "            next(annotfile)\n",
    "            annots = annotfile.read().splitlines()\n",
    "            for a in annots:\n",
    "                annot = a.split(\"\\t\")\n",
    "                atype = typemap[annot[2]]\n",
    "                start = int(annot[3])\n",
    "                stop = int(annot[4])\n",
    "                overlap = False\n",
    "                # This is where we toss out the overlaps:\n",
    "                # for ent in entities[atype]:\n",
    "                #     if ((start >= ent[0] and start <= ent[1]) or (stop >= ent[0] and stop <= ent[1]) or\n",
    "                #         (ent[0] >= start and ent[0] <= stop) or (ent[1] >= start and ent[1] <= stop)):\n",
    "                #         #print(str(start)+\"-\"+str(stop)+\" overlaps \" + str(ent))\n",
    "                #         overlap = True\n",
    "                if overlap == False:    \n",
    "                    entities[atype].append((start, stop, atype))\n",
    "\n",
    "            if fn[:-4] in trainIds:\n",
    "                traindata[\"QUANT\"].append((text, {\"entities\": entities[\"QUANT\"]}))\n",
    "                traindata[\"ME\"].append((text, {\"entities\": entities[\"ME\"]}))\n",
    "                traindata[\"MP\"].append((text, {\"entities\": entities[\"MP\"]}))\n",
    "                traindata[\"QUAL\"].append((text, {\"entities\": entities[\"QUAL\"]}))\n",
    "                trainents[\"QUANT\"].extend(entities[\"QUANT\"])\n",
    "                trainents[\"ME\"].extend(entities[\"ME\"])\n",
    "                trainents[\"MP\"].extend(entities[\"MP\"])\n",
    "                trainents[\"QUAL\"].extend(entities[\"QUAL\"])\n",
    "            else:\n",
    "                testdata[\"QUANT\"].append((text, {\"entities\": entities[\"QUANT\"]}))\n",
    "                testdata[\"ME\"].append((text, {\"entities\": entities[\"ME\"]}))\n",
    "                testdata[\"MP\"].append((text, {\"entities\": entities[\"MP\"]}))\n",
    "                testdata[\"QUAL\"].append((text, {\"entities\": entities[\"QUAL\"]}))\n",
    "                testents[\"QUANT\"].extend(entities[\"QUANT\"])\n",
    "                testents[\"ME\"].extend(entities[\"ME\"])\n",
    "                testents[\"MP\"].extend(entities[\"MP\"])\n",
    "                testents[\"QUAL\"].extend(entities[\"QUAL\"])\n",
    "            alltraindata[\"QUANT\"].append((text, {\"entities\": entities[\"QUANT\"]}))\n",
    "            alltraindata[\"ME\"].append((text, {\"entities\": entities[\"ME\"]}))\n",
    "            alltraindata[\"MP\"].append((text, {\"entities\": entities[\"MP\"]}))\n",
    "            alltraindata[\"QUAL\"].append((text, {\"entities\": entities[\"QUAL\"]}))\n",
    "            alltrainents[\"QUANT\"].extend(entities[\"QUANT\"])\n",
    "            alltrainents[\"ME\"].extend(entities[\"ME\"])\n",
    "            alltrainents[\"MP\"].extend(entities[\"MP\"])\n",
    "            alltrainents[\"QUAL\"].extend(entities[\"QUAL\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "example QUANT train data: \n",
      "Cleaned sponge and diatom opal was dissolved via wet alkaline digestion (Cardinal et al., 2007; Ragueneau et al., 2005) in 0.2 M NaOH at 100 °C for 40 min (diatoms) or up to 1 week (sponge spicules). The samples were acidified to pH∼2 with 0.2 M thermally distilled HCl and separated from major ions using cation exchange resin (BioRad AG50W-X12, Georg et al., 2006). Silicon isotope analysis was carried out using a Nu Instruments Nu-Plasma HR multi-collector inductively coupled plasma mass spectrometer run in medium resolution mode (m/Δm∼3500 at 5% and 95%). Samples were introduced via a self-aspirating PFA microconcentric nebuliser (ESI) in a Cetac Aridus II desolvating unit. Measurements included six to eight standard-sample brackets (brackets where the rate of machine drift outstripped bracketing rate were disregarded), each composed of twenty eight-second integrations. Samples were measured relative to the NIST RM 8546 standard. The external diatomite standard (1.26±0.2‰, Reynolds et al., 2007) yielded a mean and 2SD of 1.23±0.25‰ (n=104). Error bars in the figures and text are this 2SD external reproducibility unless the internal reproducibility of the standard sample brackets was larger, in which case this is quoted instead.\n",
      "\n",
      "example QUANT entity from train data: \n",
      "[(123, 128, 'QUANT'), (137, 143, 'QUANT'), (148, 154, 'QUANT'), (168, 180, 'QUANT'), (230, 234, 'QUANT'), (240, 245, 'QUANT'), (550, 560, 'QUANT'), (706, 718, 'QUANT'), (850, 856, 'QUANT'), (857, 869, 'QUANT'), (978, 987, 'QUANT'), (1022, 1048, 'QUANT'), (1052, 1055, 'QUANT')]\n",
      "\n",
      "123, 128, QUANT\n",
      "0.2 M\n",
      "137, 143, QUANT\n",
      "100 °C\n",
      "148, 154, QUANT\n",
      "40 min\n",
      "168, 180, QUANT\n",
      "up to 1 week\n",
      "230, 234, QUANT\n",
      "pH∼2\n",
      "240, 245, QUANT\n",
      "0.2 M\n",
      "550, 560, QUANT\n",
      "5% and 95%\n",
      "706, 718, QUANT\n",
      "six to eight\n",
      "850, 856, QUANT\n",
      "twenty\n",
      "857, 869, QUANT\n",
      "eight-second\n",
      "978, 987, QUANT\n",
      "1.26±0.2‰\n",
      "1022, 1048, QUANT\n",
      "mean and 2SD of 1.23±0.25‰\n",
      "1052, 1055, QUANT\n",
      "104\n",
      "\n",
      "Those same entity numbers, but from the tsv: \n",
      "[(123, 128, 'QUANT'), (137, 143, 'QUANT'), (148, 154, 'QUANT'), (168, 180, 'QUANT'), (230, 234, 'QUANT'), (240, 245, 'QUANT'), (550, 560, 'QUANT'), (706, 718, 'QUANT'), (850, 856, 'QUANT'), (857, 869, 'QUANT'), (978, 987, 'QUANT'), (1022, 1048, 'QUANT'), (1052, 1055, 'QUANT')]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ex = 5\n",
    "ex_type = 'QUANT'\n",
    "\n",
    "exampletxt = traindata[ex_type][ex][0]\n",
    "\n",
    "exampleents = traindata[ex_type][ex][1]['entities']\n",
    "\n",
    "print(f\"example QUANT train data: \\n{exampletxt}\\n\")\n",
    "\n",
    "print(f\"example QUANT entity from train data: \\n{exampleents}\\n\")\n",
    "\n",
    "for start, stop, type in exampleents:\n",
    "    print(f'{start}, {stop}, {type}')\n",
    "    print(f'{exampletxt[start:stop]}')\n",
    "\n",
    "# Trainents is an ordered list of all entities of a type the the tsv\n",
    "# this prints those which match the train data\n",
    "tsvent = 0\n",
    "for i in range(0,ex):\n",
    "    tsvent += len(traindata[ex_type][i][1]['entities'])\n",
    "    #print(tsvent)\n",
    "\n",
    "print(f\"\\nThose same entity indexes, but from the tsv: \\n{trainents['QUANT'][tsvent:tsvent+len(exampleents)]}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:\n",
      "QUANT: 830\n",
      "ME: 820\n",
      "MP: 537\n",
      "QUAL: 238\n",
      "Total: 2425\n",
      "\n",
      "Test:\n",
      "QUANT: 334\n",
      "ME: 328\n",
      "MP: 205\n",
      "QUAL: 71\n",
      "Total: 938\n",
      "\n",
      "All training:\n",
      "QUANT: 1164\n",
      "ME: 1148\n",
      "MP: 742\n",
      "QUAL: 309\n",
      "Total: 3363\n",
      "\n",
      "Test to train rations:\n",
      "All examples : 0.3868041237113402\n",
      "QUANT : 0.40240963855421685\n",
      "ME : 0.4\n",
      "MP : 0.3817504655493482\n",
      "QUAL : 0.29831932773109243\n"
     ]
    }
   ],
   "source": [
    "# We don't throw out _that_ many, see counts below.\n",
    "print(\"Training:\")\n",
    "trainentscount = 0\n",
    "trainenttypecount = {\"QUANT\":0, \"ME\":0, \"MP\":0, \"QUAL\":0}\n",
    "for t in [\"QUANT\", \"ME\", \"MP\", \"QUAL\"]:\n",
    "    trainenttypecount[t]=len(trainents[t])\n",
    "    print(t + \": \" + str(trainenttypecount[t]))\n",
    "    trainentscount+=trainenttypecount[t]\n",
    "print(\"Total: \" + str(trainentscount))\n",
    "\n",
    "testentscount = 0\n",
    "testenttypecount = {\"QUANT\":0, \"ME\":0, \"MP\":0, \"QUAL\":0}\n",
    "print(\"\\nTest:\")\n",
    "for t in [\"QUANT\", \"ME\", \"MP\", \"QUAL\"]:\n",
    "    print(t + \": \" + str(len(testents[t])))\n",
    "    testenttypecount[t]=len(testents[t])\n",
    "    testentscount+=len(testents[t])\n",
    "print(\"Total: \" + str(testentscount))\n",
    "\n",
    "allentscount = 0\n",
    "print(\"\\nAll training:\")\n",
    "for t in [\"QUANT\", \"ME\", \"MP\", \"QUAL\"]:\n",
    "    print(t + \": \" + str(len(alltrainents[t])))\n",
    "    allentscount+=len(alltrainents[t])\n",
    "print(\"Total: \" + str(allentscount))\n",
    "\n",
    "print(\"\\nTest to train rations:\")\n",
    "print(f\"All examples : {(testentscount / trainentscount)}\")\n",
    "for t in [\"QUANT\", \"ME\", \"MP\", \"QUAL\"]:\n",
    "    print(f\"{t} : {(testenttypecount[t] / trainenttypecount[t])}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# todo: build models\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "# processor = AutoProcessor.from_pretrained(model_name)\n",
    "\n",
    "config = AutoConfig.from_pretrained(model_name)\n",
    "\n",
    "model = AutoModel.from_config(config)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 ('test')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b5060d9997a95c2acb3a42af5d14caeb5dba3e5b7e20123b9f235f707614ce30"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
