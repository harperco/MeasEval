{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoConfig, AutoProcessor\n",
    "from transformers import DataCollatorWithPadding\n",
    "from transformers import AutoModel, TrainingArguments, Trainer\n",
    "# from transformers import RobertaTokenizer, RobertaModel\n",
    "\n",
    "random.seed(42)\n",
    "percent_to_train = .8\n",
    "\n",
    "model_name = 'allenai/biomed_roberta_base'\n",
    "dropout = .03\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "currentdir = os.getcwd() # ~/MeasEval/baselines\n",
    "\n",
    "trainpaths = [os.path.join(currentdir, \"../data/raw/trial/tsv/\"),\n",
    "             os.path.join(currentdir, \"../data/raw/train/tsv/\")]\n",
    "\n",
    "evalpath = os.path.join(currentdir, \"../data/raw/eval/text/\")\n",
    "\n",
    "textpaths = [os.path.join(currentdir, \"../data/raw/trial/txt/\"),\n",
    "            os.path.join(currentdir, \"../data/raw/train/text/\")]\n",
    "\n",
    "# place to save interim data\n",
    "interimpath = os.path.join(currentdir, \"../data/interim/\")\n",
    "\n",
    "# Set shorthands for annotation spans\n",
    "typemap = {\"Quantity\": \"QUANT\",\n",
    "           \"MeasuredEntity\": \"ME\", \n",
    "           \"MeasuredProperty\": \"MP\", \n",
    "           \"Qualifier\": \"QUAL\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Collect all the ids and all the text files in both the train and trial directories\n",
    "# Set our train test split for doing initial model development.\n",
    "docIds_path = os.path.join(interimpath, \"docIds\")\n",
    "textset_path = os.path.join(interimpath, \"textset\")\n",
    "trainIds_path = os.path.join(interimpath, \"trainIds\")\n",
    "testIds_path = os.path.join(interimpath, \"testIds\")\n",
    "\n",
    "# Toggle to re-split the data\n",
    "do_split_raw_data = True\n",
    "\n",
    "if do_split_raw_data:\n",
    "    docIds = []\n",
    "    textset = {}\n",
    "    for fileset in textpaths:\n",
    "        for fn in os.listdir(fileset):\n",
    "            path = fileset+fn\n",
    "            #print(fn)\n",
    "            with open(path) as textfile:\n",
    "                text = textfile.read()\n",
    "                #[:-4] strips off the .txt to get the id\n",
    "                textset[fn[:-4]] = text\n",
    "                docIds.append(fn[:-4])\n",
    "    \n",
    "    print(type(textset))\n",
    "\n",
    "    random.shuffle(docIds)\n",
    "\n",
    "    tt_split = int(np.round(len(docIds) * percent_to_train))\n",
    "\n",
    "    trainIds = docIds[:tt_split]\n",
    "    testIds = docIds[tt_split:]\n",
    "\n",
    "    textset_file = open(textset_path,'w')\n",
    "    json.dump(textset, textset_file)\n",
    "    textset_file.close()\n",
    "    np.savetxt(docIds_path, docIds, fmt='%s')\n",
    "    np.savetxt(trainIds_path, trainIds, fmt='%s')\n",
    "    np.savetxt(testIds_path, testIds, fmt='%s')\n",
    "else:\n",
    "    trainIds = np.loadtxt(trainIds_path, dtype='str')\n",
    "    testIds = np.loadtxt(testIds_path, dtype='str')\n",
    "    docIds = np.loadtxt(docIds_path, dtype='str')\n",
    "\n",
    "    textset_file = open(textset_path,'r')\n",
    "    textset = textset_file.read()\n",
    "    textset_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first 5 train IDs: \n",
      "['S2213671113000738-647' 'S2213671113001306-1398' 'S0925443913001385-1646'\n",
      " 'S1750583613004192-714' 'S0925443913001385-1429']\n",
      "first 5 test IDs: \n",
      "['S0960148113002048-3775' 'S0167577X14001256-389' 'S0006322312001096-1230'\n",
      " 'S0022399913003358-943' 'S0167739X12001525-5094']\n",
      "folders containing data and annotations: \n",
      "['/home/sam/MeasEval/baselines/../data/raw/trial/txt/', '/home/sam/MeasEval/baselines/../data/raw/train/text/']\n"
     ]
    }
   ],
   "source": [
    "print(f\"first 5 train IDs: \\n{trainIds[:5]}\")\n",
    "\n",
    "print(f\"first 5 test IDs: \\n{testIds[:5]}\")\n",
    "\n",
    "print(f\"folders containing data and annotations: \\n{textpaths}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "string indices must be integers",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/sam/MeasEval/baselines/sam_baseline.ipynb Cell 5'\u001b[0m in \u001b[0;36m<cell line: 17>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/sam/MeasEval/baselines/sam_baseline.ipynb#ch0000008?line=18'>19</a>\u001b[0m entities \u001b[39m=\u001b[39m {\u001b[39m\"\u001b[39m\u001b[39mQUANT\u001b[39m\u001b[39m\"\u001b[39m: [], \u001b[39m\"\u001b[39m\u001b[39mME\u001b[39m\u001b[39m\"\u001b[39m: [], \u001b[39m\"\u001b[39m\u001b[39mMP\u001b[39m\u001b[39m\"\u001b[39m: [], \u001b[39m\"\u001b[39m\u001b[39mQUAL\u001b[39m\u001b[39m\"\u001b[39m: []}\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/sam/MeasEval/baselines/sam_baseline.ipynb#ch0000008?line=19'>20</a>\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(fileset\u001b[39m+\u001b[39mfn) \u001b[39mas\u001b[39;00m annotfile:\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/sam/MeasEval/baselines/sam_baseline.ipynb#ch0000008?line=20'>21</a>\u001b[0m     text \u001b[39m=\u001b[39m textset[fn[:\u001b[39m-\u001b[39;49m\u001b[39m4\u001b[39;49m]]\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/sam/MeasEval/baselines/sam_baseline.ipynb#ch0000008?line=22'>23</a>\u001b[0m     \u001b[39m# Overlap handler (not required for RoBERTa??)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/sam/MeasEval/baselines/sam_baseline.ipynb#ch0000008?line=23'>24</a>\u001b[0m     \u001b[39mnext\u001b[39m(annotfile)\n",
      "\u001b[0;31mTypeError\u001b[0m: string indices must be integers"
     ]
    }
   ],
   "source": [
    "# Build training data from TSVs in expected format for spacy NER models...\n",
    "# We have to train each model separately, because spacy doesn't let us have \n",
    "# Multiple entities that overlap, and we have this a lot (Especially in our Qualifiers)\n",
    "# Unfortunately, we even have a fair bit of overlap within annotation types, \n",
    "# and end up needing to throw away a bunch of training data.\n",
    "\n",
    "# Note that we have data split for train / test, and we also have full training data.\n",
    "\n",
    "trainents = {\"QUANT\": [], \"ME\": [], \"MP\": [], \"QUAL\": []}\n",
    "traindata = {\"QUANT\": [], \"ME\": [], \"MP\": [], \"QUAL\": []}\n",
    "testents = {\"QUANT\": [], \"ME\": [], \"MP\": [], \"QUAL\": []}\n",
    "testdata = {\"QUANT\": [], \"ME\": [], \"MP\": [], \"QUAL\": []}\n",
    "\n",
    "alltrainents = {\"QUANT\": [], \"ME\": [], \"MP\": [], \"QUAL\": []}\n",
    "alltraindata = {\"QUANT\": [], \"ME\": [], \"MP\": [], \"QUAL\": []}\n",
    "\n",
    "for fileset in trainpaths:\n",
    "    for fn in os.listdir(fileset):\n",
    "        entities = {\"QUANT\": [], \"ME\": [], \"MP\": [], \"QUAL\": []}\n",
    "        with open(fileset+fn) as annotfile:\n",
    "            text = textset[fn[:-4]]\n",
    "\n",
    "            # Overlap handler (not required for RoBERTa??)\n",
    "            next(annotfile)\n",
    "            annots = annotfile.read().splitlines()\n",
    "            for a in annots:\n",
    "                annot = a.split(\"\\t\")\n",
    "                atype = typemap[annot[2]]\n",
    "                start = int(annot[3])\n",
    "                stop = int(annot[4])\n",
    "                overlap = False\n",
    "                # This is where we toss out the overlaps:\n",
    "                # for ent in entities[atype]:\n",
    "                #     if ((start >= ent[0] and start <= ent[1]) or (stop >= ent[0] and stop <= ent[1]) or\n",
    "                #         (ent[0] >= start and ent[0] <= stop) or (ent[1] >= start and ent[1] <= stop)):\n",
    "                #         #print(str(start)+\"-\"+str(stop)+\" overlaps \" + str(ent))\n",
    "                #         overlap = True\n",
    "                if overlap == False:    \n",
    "                    entities[atype].append((start, stop, atype))\n",
    "\n",
    "            if fn[:-4] in trainIds:\n",
    "                traindata[\"QUANT\"].append((text, {\"entities\": entities[\"QUANT\"]}))\n",
    "                traindata[\"ME\"].append((text, {\"entities\": entities[\"ME\"]}))\n",
    "                traindata[\"MP\"].append((text, {\"entities\": entities[\"MP\"]}))\n",
    "                traindata[\"QUAL\"].append((text, {\"entities\": entities[\"QUAL\"]}))\n",
    "                trainents[\"QUANT\"].extend(entities[\"QUANT\"])\n",
    "                trainents[\"ME\"].extend(entities[\"ME\"])\n",
    "                trainents[\"MP\"].extend(entities[\"MP\"])\n",
    "                trainents[\"QUAL\"].extend(entities[\"QUAL\"])\n",
    "            else:\n",
    "                testdata[\"QUANT\"].append((text, {\"entities\": entities[\"QUANT\"]}))\n",
    "                testdata[\"ME\"].append((text, {\"entities\": entities[\"ME\"]}))\n",
    "                testdata[\"MP\"].append((text, {\"entities\": entities[\"MP\"]}))\n",
    "                testdata[\"QUAL\"].append((text, {\"entities\": entities[\"QUAL\"]}))\n",
    "                testents[\"QUANT\"].extend(entities[\"QUANT\"])\n",
    "                testents[\"ME\"].extend(entities[\"ME\"])\n",
    "                testents[\"MP\"].extend(entities[\"MP\"])\n",
    "                testents[\"QUAL\"].extend(entities[\"QUAL\"])\n",
    "            alltraindata[\"QUANT\"].append((text, {\"entities\": entities[\"QUANT\"]}))\n",
    "            alltraindata[\"ME\"].append((text, {\"entities\": entities[\"ME\"]}))\n",
    "            alltraindata[\"MP\"].append((text, {\"entities\": entities[\"MP\"]}))\n",
    "            alltraindata[\"QUAL\"].append((text, {\"entities\": entities[\"QUAL\"]}))\n",
    "            alltrainents[\"QUANT\"].extend(entities[\"QUANT\"])\n",
    "            alltrainents[\"ME\"].extend(entities[\"ME\"])\n",
    "            alltrainents[\"MP\"].extend(entities[\"MP\"])\n",
    "            alltrainents[\"QUAL\"].extend(entities[\"QUAL\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "example QUANT train data: \n",
      "Sediments were prepared following existing techniques (Hendry et al., 2010), which were refined slightly to better suit the microseparation technique utilised here as a convenient alternative to sieving. Initial cleaning with H2O2 and HCl was carried out to concentrate biogenic opal. From this pre-cleaned sample the >100 μm fraction was separated using microseparation (Minoletti et al., 2009) and between 50 and 100 spicules were hand-picked. A range of spicule morphotypes were included, as it has been shown that neither spicule morphology nor species composition creates any consistent offset in δ30Si (Hendry et al., 2010, 2011). Spicules were sonicated in reagent grade methanol and dried down in 200 μL of concentrated HNO3. Sponge δ30Si data presented here are generally in agreement with the data from this site produced by De la Rocha (2003), despite different methodologies and specific sampling intervals (Fig. S2). XRD analysis showed sponge spicules from the Eocene/Oligocene interval of this site to be amorphous opal (De la Rocha, 2003).\n",
      "\n",
      "example QUANT entity from train data: \n",
      "[(318, 325, 'QUANT'), (400, 418, 'QUANT'), (705, 711, 'QUANT')]\n",
      "\n",
      "318, 325, QUANT\n",
      ">100 μm\n",
      "400, 418, QUANT\n",
      "between 50 and 100\n",
      "705, 711, QUANT\n",
      "200 μL\n",
      "\n",
      "Those same entity indexes, but from the tsv: \n",
      "[(318, 325, 'QUANT'), (400, 418, 'QUANT'), (705, 711, 'QUANT')]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ex = 5\n",
    "ex_type = 'QUANT'\n",
    "\n",
    "exampletxt = traindata[ex_type][ex][0]\n",
    "\n",
    "exampleents = traindata[ex_type][ex][1]['entities']\n",
    "\n",
    "print(f\"example QUANT train data: \\n{exampletxt}\\n\")\n",
    "\n",
    "print(f\"example QUANT entity from train data: \\n{exampleents}\\n\")\n",
    "\n",
    "for start, stop, type in exampleents:\n",
    "    print(f'{start}, {stop}, {type}')\n",
    "    print(f'{exampletxt[start:stop]}')\n",
    "\n",
    "# Trainents is an ordered list of all entities of a type the the tsv\n",
    "# this prints those which match the train data\n",
    "tsvent = 0\n",
    "for i in range(0,ex):\n",
    "    tsvent += len(traindata[ex_type][i][1]['entities'])\n",
    "    #print(tsvent)\n",
    "\n",
    "print(f\"\\nThose same entity indexes, but from the tsv: \\n{trainents['QUANT'][tsvent:tsvent+len(exampleents)]}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:\n",
      "QUANT: 900\n",
      "ME: 884\n",
      "MP: 562\n",
      "QUAL: 244\n",
      "Total: 2590\n",
      "\n",
      "Test:\n",
      "QUANT: 264\n",
      "ME: 264\n",
      "MP: 180\n",
      "QUAL: 65\n",
      "Total: 773\n",
      "\n",
      "All training:\n",
      "QUANT: 1164\n",
      "ME: 1148\n",
      "MP: 742\n",
      "QUAL: 309\n",
      "Total: 3363\n",
      "\n",
      "Test to train rations:\n",
      "All examples : 0.29845559845559844\n",
      "QUANT : 0.29333333333333333\n",
      "ME : 0.2986425339366516\n",
      "MP : 0.3202846975088968\n",
      "QUAL : 0.26639344262295084\n"
     ]
    }
   ],
   "source": [
    "# We don't throw out _that_ many, see counts below.\n",
    "print(\"Training:\")\n",
    "trainentscount = 0\n",
    "trainenttypecount = {\"QUANT\":0, \"ME\":0, \"MP\":0, \"QUAL\":0}\n",
    "for t in [\"QUANT\", \"ME\", \"MP\", \"QUAL\"]:\n",
    "    trainenttypecount[t]=len(trainents[t])\n",
    "    print(t + \": \" + str(trainenttypecount[t]))\n",
    "    trainentscount+=trainenttypecount[t]\n",
    "print(\"Total: \" + str(trainentscount))\n",
    "\n",
    "testentscount = 0\n",
    "testenttypecount = {\"QUANT\":0, \"ME\":0, \"MP\":0, \"QUAL\":0}\n",
    "print(\"\\nTest:\")\n",
    "for t in [\"QUANT\", \"ME\", \"MP\", \"QUAL\"]:\n",
    "    print(t + \": \" + str(len(testents[t])))\n",
    "    testenttypecount[t]=len(testents[t])\n",
    "    testentscount+=len(testents[t])\n",
    "print(\"Total: \" + str(testentscount))\n",
    "\n",
    "allentscount = 0\n",
    "print(\"\\nAll training:\")\n",
    "for t in [\"QUANT\", \"ME\", \"MP\", \"QUAL\"]:\n",
    "    print(t + \": \" + str(len(alltrainents[t])))\n",
    "    allentscount+=len(alltrainents[t])\n",
    "print(\"Total: \" + str(allentscount))\n",
    "\n",
    "print(\"\\nTest to train rations:\")\n",
    "print(f\"All examples : {(testentscount / trainentscount)}\")\n",
    "for t in [\"QUANT\", \"ME\", \"MP\", \"QUAL\"]:\n",
    "    print(f\"{t} : {(testenttypecount[t] / trainenttypecount[t])}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# todo: build models\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "# processor = AutoProcessor.from_pretrained(model_name)\n",
    "\n",
    "config = AutoConfig.from_pretrained(model_name)\n",
    "\n",
    "model = AutoModel.from_config(config)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 ('test')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b5060d9997a95c2acb3a42af5d14caeb5dba3e5b7e20123b9f235f707614ce30"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
