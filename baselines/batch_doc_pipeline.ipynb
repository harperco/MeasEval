{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from torch import nn\n",
    "from torch.optim import AdamW, Adam\n",
    "from transformers import AutoModel, AutoTokenizer, AutoModelForTokenClassification, AutoModelForSequenceClassification\n",
    "from transformers import RobertaForSequenceClassification\n",
    "from transformers import RobertaModel\n",
    "from transformers import get_scheduler\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn import metrics as metrics\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "import re\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "random.seed(42)\n",
    "reprocess_raw =  False\n",
    "\n",
    "batch_size = 10 # documents\n",
    "learning_rate = 5e-5\n",
    "n_epochs = 10\n",
    "\n",
    "# task_map = {'Quantity':1}\n",
    "task_map = {'Quantity':1,'MeasuredProperty':2,'MeasuredEntity':3,'Qualifier':4} # uncomment for multi-class\n",
    "num_classes = len(task_map)\n",
    "\n",
    "model_name = 'allenai/biomed_roberta_base'\n",
    "# model_name = 'bert-base-cased'\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "# device = 'cpu' # uncomment this to make debugging easier\n",
    "\n",
    "data_size_reduce = 1 # multiplier for making small datasets\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "currentdir = os.getcwd() # ~/MeasEval/baselines\n",
    "\n",
    "combopath_txt = os.path.join(currentdir, \"../data/raw/combo/text/\")\n",
    "combopath_annot = os.path.join(currentdir, \"../data/raw/combo/tsv/\")\n",
    "\n",
    "interimpath = os.path.join(currentdir, \"../data/interim/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_raw_txt(docs):\n",
    "    processesd_txt = {}\n",
    "    remove_markers = True\n",
    "\n",
    "    cnt_toks = {\"figs.\": 0, \"fig.\": 0, \"et al.\": 0,\n",
    "            \"ref.\": 0, \"eq.\": 0, \"e.g.\": 0,\n",
    "            \"i.e.\": 0, \"nos.\": 0, \"no.\": 0,\n",
    "            \"spp.\": 0\n",
    "            }\n",
    "    regex_end_checker = [\".*[a-zA-Z]figs\\.$\", \n",
    "                        \".*[a-zA-Z]fig\\.$\",\n",
    "                        \".*[a-zA-Z]et al\\.$\",\n",
    "                        \".*[a-zA-Z]ref\\.$\",\n",
    "                        \".*[a-zA-Z]eq\\.$\",\n",
    "                        \".*[a-zA-Z]e\\.g\\.$\",\n",
    "                        \".*[a-zA-Z]i\\.e\\.$\",\n",
    "                        \".*[a-zA-Z]nos\\.$\",\n",
    "                        \".*[a-zA-Z]no\\.$\",\n",
    "                        \".*[a-zA-Z]spp\\.$\",\n",
    "                        # figs., fig., et al., Ref., Eq., e.g., i.e., Nos., No., spp.\n",
    "                    ]\n",
    "\n",
    "    assert len(cnt_toks) == len(regex_end_checker)\n",
    "\n",
    "    for docId, doc in docs.items():\n",
    "        flag = False\n",
    "        sentences = sent_tokenize(doc)\n",
    "\n",
    "        fixed_sentence_tokens = []\n",
    "        curr_len = 0\n",
    "        for s in sentences:\n",
    "            if flag == True:\n",
    "                assert s[0] != ' '\n",
    "                white_length = doc[curr_len:].find(s[0])\n",
    "\n",
    "                prev_len = len(fixed_sentence_tokens[-1])\n",
    "                fixed_sentence_tokens[-1] = fixed_sentence_tokens[-1] + (\" \"*white_length) + s\n",
    "\n",
    "                assert fixed_sentence_tokens[-1][prev_len+white_length] == doc[curr_len+white_length], (fixed_sentence_tokens[-1], doc, curr_len, tmp_this_sent_len)\n",
    "                tmp_this_sent_len = white_length + len(s)\n",
    "                assert fixed_sentence_tokens[-1][-1] == doc[curr_len+tmp_this_sent_len-1], (fixed_sentence_tokens[-1], doc, curr_len, tmp_this_sent_len)\n",
    "                curr_len += tmp_this_sent_len\n",
    "            else:\n",
    "                if len(fixed_sentence_tokens) != 0:\n",
    "                    assert s[0] != ' '\n",
    "                    white_length = doc[curr_len:].find(s[0])\n",
    "                    fixed_sentence_tokens.append( (\" \"*white_length) + s )\n",
    "                else:\n",
    "                    fixed_sentence_tokens.append(s)\n",
    "                assert fixed_sentence_tokens[-1][0] == doc[curr_len], (fixed_sentence_tokens, doc, curr_len, tmp_this_sent_len)\n",
    "                tmp_this_sent_len = len(fixed_sentence_tokens[-1])\n",
    "                assert fixed_sentence_tokens[-1][-1] == doc[curr_len+tmp_this_sent_len-1], (fixed_sentence_tokens[-1], doc, curr_len, tmp_this_sent_len)\n",
    "                curr_len += tmp_this_sent_len\n",
    "\n",
    "            lower_cased_s = fixed_sentence_tokens[-1].lower()\n",
    "            flag = False\n",
    "            if remove_markers:\n",
    "                for i, k in enumerate(cnt_toks):\n",
    "                    this_regex_pattern = regex_end_checker[i]\n",
    "                    if lower_cased_s.endswith(k) and re.match(this_regex_pattern, lower_cased_s) == None:\n",
    "                        cnt_toks[k] += 1\n",
    "                        flag = True\n",
    "                        break\n",
    "\n",
    "        processesd_txt[docId] = ''.join(fixed_sentence_tokens)\n",
    "    return processesd_txt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(reprocess_raw = False):\n",
    "\n",
    "    if reprocess_raw == True:\n",
    "        docIds = []\n",
    "        combo_txt = {}\n",
    "        for fn in os.listdir(combopath_txt):\n",
    "            docIds.append(fn[:-4])\n",
    "            path = combopath_txt+fn\n",
    "            with open(path) as textfile:\n",
    "                    text = textfile.read()\n",
    "                    #[:-4] strips off the .txt to get the id\n",
    "                    combo_txt[fn[:-4]] = text\n",
    "\n",
    "        combo_annot = pd.DataFrame()\n",
    "        for fn in os.listdir(combopath_annot):\n",
    "            path = combopath_annot+fn\n",
    "            file = pd.read_csv(path,delimiter='\\t',encoding='utf-8')\n",
    "            combo_annot = pd.concat([combo_annot, file],ignore_index=True)\n",
    "\n",
    "        combo_txt = process_raw_txt(combo_txt)\n",
    "        assert docIds == list(combo_txt.keys()), (len(docIds), len(list(combo_txt.keys())))\n",
    "\n",
    "        with open(interimpath+'combo_txt.json','w') as f:\n",
    "            json.dump(combo_txt, f)\n",
    "\n",
    "        combo_annot.to_csv(interimpath+'combo_annot.csv')\n",
    "\n",
    "        return docIds, combo_txt, combo_annot\n",
    "    else:\n",
    "        combo_annot = pd.read_csv(interimpath+'combo_annot.csv')\n",
    "\n",
    "        with open(interimpath+'combo_txt.json','r') as f:\n",
    "            combo_txt = json.load(f)\n",
    "\n",
    "        docIds = list(combo_txt.keys())\n",
    "    \n",
    "        return docIds, combo_txt, combo_annot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "combo_docs, combo_txt, combo_annot = read_data(reprocess_raw = reprocess_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### train/dev/test split options\n",
    "\n",
    "random.seed(42)\n",
    "\n",
    "percent_to_test = .1\n",
    "percent_to_dev = .2\n",
    "percent_to_train =  1 - percent_to_dev - percent_to_test\n",
    "\n",
    "n_doc = len(combo_docs)\n",
    "split_train = int(np.round(n_doc * percent_to_train))\n",
    "split_dev = split_train + int(np.round(n_doc * percent_to_dev))\n",
    "\n",
    "train_docs = combo_docs[:split_train]\n",
    "dev_docs = combo_docs[split_train:split_dev]\n",
    "test_docs = combo_docs[split_dev:]\n",
    "\n",
    "train_docs = random.sample(train_docs, int(len(train_docs)*data_size_reduce))\n",
    "dev_docs = random.sample(dev_docs, int(len(dev_docs)*data_size_reduce))\n",
    "test_docs = random.sample(test_docs, int(len(test_docs)*data_size_reduce))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## Tokenizer ###########\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>docId</th>\n",
       "      <th>annotId</th>\n",
       "      <th>annotType</th>\n",
       "      <th>annotSpan</th>\n",
       "      <th>subSpanType</th>\n",
       "      <th>linkId</th>\n",
       "      <th>linkSpan</th>\n",
       "      <th>subSpan</th>\n",
       "      <th>unit</th>\n",
       "      <th>unitEncoded</th>\n",
       "      <th>misc</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>comboId</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>S0019103512003995-1910_T1-2</th>\n",
       "      <td>S0019103512003995-1910</td>\n",
       "      <td>T1-2</td>\n",
       "      <td>MeasuredEntity</td>\n",
       "      <td>[80, 102]</td>\n",
       "      <td>HasQuantity</td>\n",
       "      <td>T102-2</td>\n",
       "      <td>[141, 162]</td>\n",
       "      <td>[80, 162]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>S0378383912000130-3891_T3-2</th>\n",
       "      <td>S0378383912000130-3891</td>\n",
       "      <td>T3-2</td>\n",
       "      <td>MeasuredProperty</td>\n",
       "      <td>[150, 158]</td>\n",
       "      <td>HasQuantity</td>\n",
       "      <td>T2-2</td>\n",
       "      <td>[132, 149]</td>\n",
       "      <td>[132, 158]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>S0012821X12004384-1178_T4-1</th>\n",
       "      <td>S0012821X12004384-1178</td>\n",
       "      <td>T4-1</td>\n",
       "      <td>Qualifier</td>\n",
       "      <td>[108, 111]</td>\n",
       "      <td>Qualifies</td>\n",
       "      <td>T1-1</td>\n",
       "      <td>[100, 107]</td>\n",
       "      <td>[100, 111]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>S0019103512003995-1237_T5-2</th>\n",
       "      <td>S0019103512003995-1237</td>\n",
       "      <td>T5-2</td>\n",
       "      <td>Qualifier</td>\n",
       "      <td>[292, 308]</td>\n",
       "      <td>Qualifies</td>\n",
       "      <td>T3-2</td>\n",
       "      <td>[258, 275]</td>\n",
       "      <td>[258, 308]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>S0016236113008041-3269_T2310-10</th>\n",
       "      <td>S0016236113008041-3269</td>\n",
       "      <td>T2310-10</td>\n",
       "      <td>Qualifier</td>\n",
       "      <td>[1017, 1040]</td>\n",
       "      <td>Qualifies</td>\n",
       "      <td>T1510-10</td>\n",
       "      <td>[989, 992]</td>\n",
       "      <td>[989, 1040]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>S0012821X12004384-1594_T62-2</th>\n",
       "      <td>S0012821X12004384-1594</td>\n",
       "      <td>T62-2</td>\n",
       "      <td>Quantity</td>\n",
       "      <td>[310, 314]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>°N</td>\n",
       "      <td>[12938, 487]</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>S0019103513005058-3094_T154-4</th>\n",
       "      <td>S0019103513005058-3094</td>\n",
       "      <td>T154-4</td>\n",
       "      <td>MeasuredEntity</td>\n",
       "      <td>[1000, 1004]</td>\n",
       "      <td>HasProperty</td>\n",
       "      <td>T144-4</td>\n",
       "      <td>[1041, 1046]</td>\n",
       "      <td>[1000, 1046]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  docId   annotId  \\\n",
       "comboId                                                             \n",
       "S0019103512003995-1910_T1-2      S0019103512003995-1910      T1-2   \n",
       "S0378383912000130-3891_T3-2      S0378383912000130-3891      T3-2   \n",
       "S0012821X12004384-1178_T4-1      S0012821X12004384-1178      T4-1   \n",
       "S0019103512003995-1237_T5-2      S0019103512003995-1237      T5-2   \n",
       "S0016236113008041-3269_T2310-10  S0016236113008041-3269  T2310-10   \n",
       "S0012821X12004384-1594_T62-2     S0012821X12004384-1594     T62-2   \n",
       "S0019103513005058-3094_T154-4    S0019103513005058-3094    T154-4   \n",
       "\n",
       "                                        annotType     annotSpan  subSpanType  \\\n",
       "comboId                                                                        \n",
       "S0019103512003995-1910_T1-2        MeasuredEntity     [80, 102]  HasQuantity   \n",
       "S0378383912000130-3891_T3-2      MeasuredProperty    [150, 158]  HasQuantity   \n",
       "S0012821X12004384-1178_T4-1             Qualifier    [108, 111]    Qualifies   \n",
       "S0019103512003995-1237_T5-2             Qualifier    [292, 308]    Qualifies   \n",
       "S0016236113008041-3269_T2310-10         Qualifier  [1017, 1040]    Qualifies   \n",
       "S0012821X12004384-1594_T62-2             Quantity    [310, 314]          NaN   \n",
       "S0019103513005058-3094_T154-4      MeasuredEntity  [1000, 1004]  HasProperty   \n",
       "\n",
       "                                   linkId      linkSpan       subSpan unit  \\\n",
       "comboId                                                                      \n",
       "S0019103512003995-1910_T1-2        T102-2    [141, 162]     [80, 162]  NaN   \n",
       "S0378383912000130-3891_T3-2          T2-2    [132, 149]    [132, 158]  NaN   \n",
       "S0012821X12004384-1178_T4-1          T1-1    [100, 107]    [100, 111]  NaN   \n",
       "S0019103512003995-1237_T5-2          T3-2    [258, 275]    [258, 308]  NaN   \n",
       "S0016236113008041-3269_T2310-10  T1510-10    [989, 992]   [989, 1040]  NaN   \n",
       "S0012821X12004384-1594_T62-2          NaN           NaN           NaN   °N   \n",
       "S0019103513005058-3094_T154-4      T144-4  [1041, 1046]  [1000, 1046]  NaN   \n",
       "\n",
       "                                  unitEncoded misc  \n",
       "comboId                                             \n",
       "S0019103512003995-1910_T1-2               NaN  NaN  \n",
       "S0378383912000130-3891_T3-2               NaN  NaN  \n",
       "S0012821X12004384-1178_T4-1               NaN  NaN  \n",
       "S0019103512003995-1237_T5-2               NaN  NaN  \n",
       "S0016236113008041-3269_T2310-10           NaN  NaN  \n",
       "S0012821X12004384-1594_T62-2     [12938, 487]  NaN  \n",
       "S0019103513005058-3094_T154-4             NaN  NaN  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def process_annotation_set(annot_set):\n",
    "\n",
    "    annot_set_processed = []\n",
    "\n",
    "    annot_set['comboIds'] = annot_set[['docId','annotId']].agg('_'.join, axis=1)\n",
    "    annot_set.set_index('comboIds',inplace=True)\n",
    "\n",
    "    for comboId in list(annot_set.index):\n",
    "        \n",
    "        docId = annot_set.loc[comboId]['docId']\n",
    "        annotId = annot_set.loc[comboId]['annotId']\n",
    "\n",
    "        annotType = annot_set.loc[comboId]['annotType']\n",
    "        annotSpan = [annot_set.loc[comboId]['startOffset'],annot_set.loc[comboId]['endOffset']]\n",
    "\n",
    "        ent_annot_processed = {\n",
    "            'comboId':comboId,\n",
    "            'docId':docId,\n",
    "            'annotId':annotId,\n",
    "            'annotType':annotType,\n",
    "            'annotSpan':annotSpan,\n",
    "            'subSpanType':np.nan,\n",
    "            'linkId':np.nan,\n",
    "            'linkSpan':np.nan,\n",
    "            'subSpan':np.nan,\n",
    "            'unit':np.nan,\n",
    "            'unitEncoded':np.nan,\n",
    "            'misc':np.nan\n",
    "        }\n",
    "        \n",
    "        other = annot_set.loc[comboId]['other']\n",
    "        if isinstance(other,str):\n",
    "            otherDict = json.loads(str(other))\n",
    "\n",
    "            if annot_set.loc[comboId]['annotType'] != 'Quantity':\n",
    "\n",
    "                ent_annot_processed['subSpanType'] = list(otherDict.keys())[0]\n",
    "                link = list(otherDict.values())[0]\n",
    "\n",
    "                ent_annot_processed['linkId'] = link\n",
    "                linkIdx = docId+'_'+link\n",
    "                linkSpan = [int(annot_set.loc[linkIdx]['startOffset']),int(annot_set.loc[linkIdx]['endOffset'])]\n",
    "                ent_annot_processed['linkSpan'] = linkSpan\n",
    "\n",
    "                spanEnds = annotSpan + linkSpan\n",
    "                ent_annot_processed['subSpan'] = [min(spanEnds),max(spanEnds)]\n",
    "\n",
    "            elif 'unit' in list(otherDict.keys()):\n",
    "                unit = otherDict['unit']\n",
    "                ent_annot_processed['unit'] = unit\n",
    "                ent_annot_processed['unitEncoded'] = tokenizer.encode(unit)[1:-1]\n",
    "            else:\n",
    "                ent_annot_processed['misc'] = otherDict\n",
    "\n",
    "\n",
    "        annot_set_processed.append(ent_annot_processed)\n",
    "   \n",
    "    return pd.DataFrame.from_dict(annot_set_processed).set_index('comboId')\n",
    "\n",
    "combo_annot_processed = process_annotation_set(combo_annot)\n",
    "combo_annot_processed.to_csv(interimpath+'combo_annot_processed.csv')\n",
    "combo_annot_processed.sample(7)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "### insert special tokens for subspans (Sam)\n",
    "# will make docs longer\n",
    "\n",
    "# def char_map(doc_annot, task_map)\n",
    "\n",
    "\n",
    "def tokenize_and_align_labels(\n",
    "                                doc_list=combo_docs,\n",
    "                                txt=combo_txt,\n",
    "                                processed_annotation=combo_annot_processed,\n",
    "                                tokenizer=tokenizer,\n",
    "                                taskLabelMap=task_map\n",
    "                            ):\n",
    "\n",
    "    toks_with_labels = []\n",
    "    special_ids = tokenizer.all_special_ids\n",
    "\n",
    "    for doc in doc_list:\n",
    "        # print(doc)\n",
    "        # print(processed_annotation.loc[processed_annotation['docId'] == doc])\n",
    "        doc_annot = processed_annotation.loc[processed_annotation['docId'] == doc]\n",
    "        doc_annot.set_index('annotId',inplace=True)\n",
    "        # print(doc_annot)\n",
    "\n",
    "        encoded_txt = tokenizer(txt[doc], padding='max_length', max_length=512, truncation=True)\n",
    "        encoded_tokens = encoded_txt['input_ids']\n",
    "        # print(encoded_tokens)\n",
    "\n",
    "        ############### Label Primary Spans ###############\n",
    "\n",
    "        labelIds = np.full(len(encoded_tokens),-1)\n",
    "        taskCharMap = {} # \n",
    "        taskCharList = []\n",
    "        taskAnnotIdCharMap = {} # to check for token collision\n",
    "        \n",
    "        for task in list(taskLabelMap.keys()):\n",
    "            #print(task)\n",
    "            annotId = doc_annot.loc[doc_annot['annotType']==task].index\n",
    "            # print(annotId)\n",
    "            spans = list(doc_annot.loc[doc_annot['annotType']==task]['annotSpan'])\n",
    "            # print(spans)\n",
    "            for span in spans:\n",
    "                # print(span)\n",
    "                span = list(range(span[0],span[-1]))\n",
    "                # print(span)\n",
    "                for spanCharIdx in span:\n",
    "                    # print(spanCharIdx)\n",
    "                    taskCharMap[spanCharIdx] = taskLabelMap[task]\n",
    "                # print(taskCharMap)\n",
    "                    # taskAnnotIdCharMap[spanCharIdx] = annotId\n",
    "\n",
    "        decoded = [''] * len(encoded_tokens)\n",
    "        for tokenIdx, token in enumerate(encoded_tokens):\n",
    "            \n",
    "            if token not in special_ids:\n",
    "                tokenCharStart = encoded_txt.token_to_chars(tokenIdx).start\n",
    "                if tokenCharStart in list(taskCharMap.keys()):\n",
    "                    labelIds[tokenIdx] = taskCharMap[tokenCharStart]\n",
    "                    decoded[tokenIdx] = tokenizer.decode(token)\n",
    "                else:\n",
    "                    labelIds[tokenIdx] = 0\n",
    "            else:\n",
    "                labelIds[tokenIdx] = 0\n",
    "        \n",
    "\n",
    "        ############### Sub Spans Token Insertion and labeling ###############\n",
    "\n",
    "        encoded_txt['doc_or_sent_id'] = doc\n",
    "        encoded_txt['labels'] = labelIds\n",
    "        \n",
    "        toks_with_labels.append(encoded_txt)\n",
    "    \n",
    "    # return toks_with_labels\n",
    "    return pd.DataFrame.from_dict(toks_with_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "################# TOKENIZE #################\n",
    "\n",
    "stage1_train_ds = tokenize_and_align_labels(\n",
    "    doc_list=train_docs,\n",
    "    txt=combo_txt,\n",
    "    processed_annotation=combo_annot_processed,\n",
    "    tokenizer=tokenizer,\n",
    "    taskLabelMap=task_map)\n",
    "# stage1_train_ds.to_csv(interimpath+'stage1_train_ds.csv')\n",
    "stage1_n_train = stage1_train_ds.shape[0]\n",
    "\n",
    "\n",
    "stage1_dev_ds = tokenize_and_align_labels(\n",
    "    doc_list=dev_docs,\n",
    "    txt=combo_txt,\n",
    "    processed_annotation=combo_annot_processed,\n",
    "    tokenizer=tokenizer,\n",
    "    taskLabelMap=task_map)\n",
    "# stage1_dev_ds.to_csv(interimpath+'stage1_dev_ds.csv')\n",
    "stage1_n_dev = stage1_dev_ds.shape[0]\n",
    "\n",
    "stage1_test_ds = tokenize_and_align_labels(\n",
    "    doc_list=test_docs,\n",
    "    txt=combo_txt,\n",
    "    processed_annotation=combo_annot_processed,\n",
    "    tokenizer=tokenizer,\n",
    "    taskLabelMap=task_map)\n",
    "# stage1_test_ds.to_csv(interimpath+'stage1_test_ds.csv')\n",
    "stage1_n_test = stage1_test_ds.shape[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "314"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stage1_n_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Matt\n",
    "# def shorten_txt_encoding(txt, shorten_by : int):       \n",
    "#     pass...\n",
    "\n",
    "# generate a list of docIds that have token collision after shortening\n",
    "\n",
    "# toks = list(stage1_dev_ds.sample(1)['input_ids'])\n",
    "\n",
    "# print(toks[0])\n",
    "\n",
    "# tokenizer.decode(toks[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batchify(tokenized_dataset, batch_size, device):\n",
    "    num_examples = int(tokenized_dataset.shape[0] / batch_size)\n",
    "    batch_sizes = [batch_size for x in range(num_examples)]\n",
    "    last_batch_size = tokenized_dataset.shape[0] % batch_size\n",
    "    if last_batch_size:\n",
    "        batch_sizes.append(last_batch_size)\n",
    "    # print(batch_sizes)\n",
    "\n",
    "    batched_dataset = []\n",
    "\n",
    "    idf_to_torch = lambda df : torch.tensor(np.array([list(map(int,r)) for r in df])).to(device)\n",
    "\n",
    "    for idx, size in enumerate(batch_sizes):\n",
    "        start = sum(batch_sizes[:idx])\n",
    "        end = sum(batch_sizes[:idx]) + size - 1\n",
    "        # print(start,end,idx)\n",
    "        input_ids = idf_to_torch(tokenized_dataset['input_ids'].loc[start:end])\n",
    "        attention_mask = idf_to_torch(tokenized_dataset['attention_mask'].loc[start:end])\n",
    "        labels = idf_to_torch(tokenized_dataset['labels'].loc[start:end])\n",
    "        doc_or_sent_id = list(tokenized_dataset['doc_or_sent_id'].loc[start:end])\n",
    "        \n",
    "        batch = {\n",
    "            'input_ids':input_ids,\n",
    "            'labels':labels,\n",
    "            'attention_mask':attention_mask,\n",
    "            'doc_or_sent_id':doc_or_sent_id\n",
    "\n",
    "        }\n",
    "        \n",
    "        batched_dataset.append(batch)\n",
    "\n",
    "    return batched_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "################# batchify ####################\n",
    "\n",
    "batched_train_ds = batchify(stage1_train_ds[['attention_mask','input_ids','labels','doc_or_sent_id']], batch_size, device)\n",
    "batched_dev_ds = batchify(stage1_dev_ds[['attention_mask','input_ids','labels','doc_or_sent_id']], batch_size, device)\n",
    "batched_test_ds = batchify(stage1_test_ds[['attention_mask','input_ids','labels','doc_or_sent_id']], batch_size, device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[    0, 44105,     4,  ...,     1,     1,     1],\n",
       "         [    0,   970,    32,  ...,     1,     1,     1],\n",
       "         [    0,   170,   220,  ...,     1,     1,     1],\n",
       "         ...,\n",
       "         [    0, 14699,    12,  ...,     1,     1,     1],\n",
       "         [    0,   104, 40275,  ...,     1,     1,     1],\n",
       "         [    0, 33837,  3024,  ...,     1,     1,     1]], device='cuda:0'),\n",
       " 'labels': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [0, 3, 3,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0'),\n",
       " 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0'),\n",
       " 'doc_or_sent_id': ['S0016236113008041-3290',\n",
       "  'S0006322312001096-1271',\n",
       "  'S0022000014000026-7850',\n",
       "  'S0019103513005058-4158',\n",
       "  'S0019103512004009-4492',\n",
       "  'S0019103512002801-1342',\n",
       "  'S0016236113008041-3171',\n",
       "  'S0378383912000130-1096',\n",
       "  'S0016236113008041-2924',\n",
       "  'S0925443913001385-1429']}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batched_train_ds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Quantity': 1, 'MeasuredProperty': 2, 'MeasuredEntity': 3, 'Qualifier': 4}\n"
     ]
    }
   ],
   "source": [
    "demo_batch = 2\n",
    "\n",
    "demo_batch = batched_train_ds[demo_batch]\n",
    "\n",
    "\n",
    "demo_doc = demo_batch['doc_or_sent_id'][0]\n",
    "demo_ids = demo_batch['input_ids'].cpu().numpy()[0]\n",
    "demo_tokens = tokenizer.decode(demo_batch['input_ids'].cpu().numpy()[0])\n",
    "demo_labels = demo_batch['labels'].cpu().numpy()[0]\n",
    "demo_mask = demo_batch['attention_mask'].cpu().numpy()[0]\n",
    "latch_print = False\n",
    "labeled_tokens = ''\n",
    "for id, lab in zip(demo_ids, demo_labels):\n",
    "    if lab:\n",
    "        labeled_tokens = labeled_tokens + tokenizer.decode(id) + ' '\n",
    "\n",
    "print(task_map)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'S0925443913001385-1646'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "demo_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[    0  1121   645     7  3094   549     5 18838  7205  1092 31425    67\n",
      " 26914  1022    11 15229     8  6559     9     5 43261 14966   366 38868\n",
      "   739     8   650  2849 46210     6 43261   784  2459  1626    31 43318\n",
      " 19961  1001  3662 13651    36 40747     8   797    43    58 13484  1070\n",
      "    15    16 36557 15557 43164 14170 12003 20676    36   698  2383   541\n",
      "  4234    25    11  8526     4   646  3706 48610   318  6559     9  1169\n",
      "     5   739  2849 19304    50     5  1445 14966   366  4399    21  2132\n",
      "   172     5  3854     9  1736 14966   366 38868 17792    74   464   624\n",
      "     5 43141  4392     4   374  1966 18838  7205  1092    31     5  3186\n",
      "    21 12246  8065    11    70 44807    53 44012    11     5 44807  4292\n",
      "    19 41601    12 10463   791   131   959    24    21 28840 11640    31\n",
      "     5   481  3716    36   506 29866   112     8   132     6 20001     4\n",
      "   195   322   152    21    11  5709     7     5   797    14 22495    10\n",
      "  3716     9   481 18838  7205  1092     6    61    34    57   431     7\n",
      " 10754    19 22808   500 11674   646  4419  8174 18838  7205   246    21\n",
      "    67  2829  2906    11  2087  4590    53  2442    11 44807  4292    19\n",
      "     5   739  2849 19304     4    20 18838  7205  1092 31425  7284    55\n",
      "  6473   352    15     5   650 14966   366 38868  2849 19304     6    19\n",
      "   211   591   246  4100 32512     8 18838  3888  1366   387   303    11\n",
      "   795  5353   129    11 44807   204     8   195    53  3680    19  1122\n",
      "  5204   194  1389     8  3854  4392  1118     7   797     4  1773 22808\n",
      "   500 11674     8 18838  7205  1092    33    57  1027    25 10754   994\n",
      "     6    52 13773   258     5  5204   194   672     8 43141  3854     9\n",
      " 22808   500 11674     7   192   114   209    58  2132    30     5 18838\n",
      "  7205  1092 31425     4  7806  1389    11     5  2087  7728    58  8065\n",
      "     7  5549   207     9   797   923    36 44105     4   195   387    43\n",
      "    53  3854    11     5 43141  1382  2743 32512    19     5  8219     9\n",
      " 13484   365     6   147  1389    58   795    87   797    36 44105     4\n",
      "   195   250  2576  9217   322     2     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1]\n"
     ]
    }
   ],
   "source": [
    "print(demo_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 3 3 3 3 3 3 3 0 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 2\n",
      " 2 2 0 4 4 1 1 0 3 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "print(demo_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>In order to determine whether the MRPL12 mutation also induced changes in composition and assembly of the mitochondrial ribosomal large and small subunits, mitochondrial lysates from cultured fibroblasts (subject and control) were fractionated on isokinetic sucrose gradients (10–30%, as in Ref. [47]). If assembly of either the large subunit or the entire ribosome was affected then the distribution of individual ribosomal proteins would change within the gradient profile. On analysis MRPL12 from the patient was substantially decreased in all fractions but detectable in the fractions consistent with mt-LSU; however it was noticeably absent from the free pool (fractions 1 and 2, Fig. 5). This was in contrast to the control that exhibited a pool of free MRPL12, which has been reported to interact with POLRMT [56]. MRPL3 was also slightly reduced in subject cells but remained in fractions consistent with the large subunit. The MRPL12 mutation impacted more modestly on the small ribosomal subunit, with DAP3 apparently unaffected and MRPS18B found in lower amounts only in fractions 4 and 5 but otherwise with similar steady state levels and distribution profile compared to control. Since POLRMT and MRPL12 have been published as interactors, we analyzed both the steady state level and gradient distribution of POLRMT to see if these were affected by the MRPL12 mutation. Overall levels in the subject sample were decreased to 63% of control value (Fig. 5B) but distribution in the gradient appeared largely unaffected with the exception of fraction 11, where levels were lower than control (Fig. 5A bottom panels).</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n"
     ]
    }
   ],
   "source": [
    "print(demo_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>docId</th>\n",
       "      <th>annotId</th>\n",
       "      <th>annotType</th>\n",
       "      <th>annotSpan</th>\n",
       "      <th>subSpanType</th>\n",
       "      <th>linkId</th>\n",
       "      <th>linkSpan</th>\n",
       "      <th>subSpan</th>\n",
       "      <th>unit</th>\n",
       "      <th>unitEncoded</th>\n",
       "      <th>misc</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>comboId</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>S0925443913001385-1646_T1-1</th>\n",
       "      <td>S0925443913001385-1646</td>\n",
       "      <td>T1-1</td>\n",
       "      <td>Quantity</td>\n",
       "      <td>[277, 283]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>%</td>\n",
       "      <td>[207]</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>S0925443913001385-1646_T2-1</th>\n",
       "      <td>S0925443913001385-1646</td>\n",
       "      <td>T2-1</td>\n",
       "      <td>MeasuredEntity</td>\n",
       "      <td>[247, 275]</td>\n",
       "      <td>HasQuantity</td>\n",
       "      <td>T1-1</td>\n",
       "      <td>[277, 283]</td>\n",
       "      <td>[247, 283]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>S0925443913001385-1646_T1-2</th>\n",
       "      <td>S0925443913001385-1646</td>\n",
       "      <td>T1-2</td>\n",
       "      <td>Quantity</td>\n",
       "      <td>[1438, 1441]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>%</td>\n",
       "      <td>[207]</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>S0925443913001385-1646_T2-2</th>\n",
       "      <td>S0925443913001385-1646</td>\n",
       "      <td>T2-2</td>\n",
       "      <td>MeasuredProperty</td>\n",
       "      <td>[1391, 1419]</td>\n",
       "      <td>HasQuantity</td>\n",
       "      <td>T1-2</td>\n",
       "      <td>[1438, 1441]</td>\n",
       "      <td>[1391, 1441]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>S0925443913001385-1646_T4-2</th>\n",
       "      <td>S0925443913001385-1646</td>\n",
       "      <td>T4-2</td>\n",
       "      <td>MeasuredEntity</td>\n",
       "      <td>[1445, 1458]</td>\n",
       "      <td>HasProperty</td>\n",
       "      <td>T2-2</td>\n",
       "      <td>[1391, 1419]</td>\n",
       "      <td>[1391, 1458]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>S0925443913001385-1646_T3-2</th>\n",
       "      <td>S0925443913001385-1646</td>\n",
       "      <td>T3-2</td>\n",
       "      <td>Qualifier</td>\n",
       "      <td>[1425, 1437]</td>\n",
       "      <td>Qualifies</td>\n",
       "      <td>T2-2</td>\n",
       "      <td>[1391, 1419]</td>\n",
       "      <td>[1391, 1437]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              docId annotId         annotType  \\\n",
       "comboId                                                                         \n",
       "S0925443913001385-1646_T1-1  S0925443913001385-1646    T1-1          Quantity   \n",
       "S0925443913001385-1646_T2-1  S0925443913001385-1646    T2-1    MeasuredEntity   \n",
       "S0925443913001385-1646_T1-2  S0925443913001385-1646    T1-2          Quantity   \n",
       "S0925443913001385-1646_T2-2  S0925443913001385-1646    T2-2  MeasuredProperty   \n",
       "S0925443913001385-1646_T4-2  S0925443913001385-1646    T4-2    MeasuredEntity   \n",
       "S0925443913001385-1646_T3-2  S0925443913001385-1646    T3-2         Qualifier   \n",
       "\n",
       "                                annotSpan  subSpanType linkId      linkSpan  \\\n",
       "comboId                                                                       \n",
       "S0925443913001385-1646_T1-1    [277, 283]          NaN    NaN           NaN   \n",
       "S0925443913001385-1646_T2-1    [247, 275]  HasQuantity   T1-1    [277, 283]   \n",
       "S0925443913001385-1646_T1-2  [1438, 1441]          NaN    NaN           NaN   \n",
       "S0925443913001385-1646_T2-2  [1391, 1419]  HasQuantity   T1-2  [1438, 1441]   \n",
       "S0925443913001385-1646_T4-2  [1445, 1458]  HasProperty   T2-2  [1391, 1419]   \n",
       "S0925443913001385-1646_T3-2  [1425, 1437]    Qualifies   T2-2  [1391, 1419]   \n",
       "\n",
       "                                  subSpan unit unitEncoded misc  \n",
       "comboId                                                          \n",
       "S0925443913001385-1646_T1-1           NaN    %       [207]  NaN  \n",
       "S0925443913001385-1646_T2-1    [247, 283]  NaN         NaN  NaN  \n",
       "S0925443913001385-1646_T1-2           NaN    %       [207]  NaN  \n",
       "S0925443913001385-1646_T2-2  [1391, 1441]  NaN         NaN  NaN  \n",
       "S0925443913001385-1646_T4-2  [1391, 1458]  NaN         NaN  NaN  \n",
       "S0925443913001385-1646_T3-2  [1391, 1437]  NaN         NaN  NaN  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combo_annot_processed.loc[combo_annot['docId']==demo_doc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " is okin etic  suc rose  grad ients 10 – 30 %,  levels  in  the  subject  sample  decreased  to  63 %  control  value \n"
     ]
    }
   ],
   "source": [
    "print(labeled_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'<s>': 0, '</s>': 2, '<unk>': 3, '<pad>': 1, '<mask>': 50264}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "special_token_map = dict(zip(tokenizer.all_special_tokens,tokenizer.all_special_ids))\n",
    "special_token_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S0925443913001385-1646_T2-1\n",
      "MeasuredEntity\n",
      "HasQuantity\n",
      "247 283\n",
      "[16, 36557, 15557, 43164, 14170, 12003, 20676, 36, 698, 2383, 541, 4234]\n",
      "[3, 3, 3, 3, 3, 3, 3, 0, 1, 1, 1, 1]\n",
      " isokinetic sucrose gradients (10–30%,\n",
      "\n",
      "S0925443913001385-1646_T2-2\n",
      "MeasuredProperty\n",
      "HasQuantity\n",
      "1391 1441\n",
      "[1389, 11, 5, 2087, 7728, 58, 8065, 7, 5549, 207]\n",
      "[2, 2, 2, 2, 2, 0, 4, 4, 1, 1]\n",
      " levels in the subject sample were decreased to 63%\n",
      "\n",
      "S0925443913001385-1646_T4-2\n",
      "MeasuredEntity\n",
      "HasProperty\n",
      "1391 1458\n",
      "[1389, 11, 5, 2087, 7728, 58, 8065, 7, 5549, 207, 9, 797, 923]\n",
      "[2, 2, 2, 2, 2, 0, 4, 4, 1, 1, 0, 3, 3]\n",
      " levels in the subject sample were decreased to 63% of control value\n",
      "\n",
      "S0925443913001385-1646_T3-2\n",
      "Qualifier\n",
      "Qualifies\n",
      "1391 1437\n",
      "[1389, 11, 5, 2087, 7728, 58, 8065, 7]\n",
      "[2, 2, 2, 2, 2, 0, 4, 4]\n",
      " levels in the subject sample were decreased to\n",
      "\n"
     ]
    }
   ],
   "source": [
    "demo_annots = combo_annot_processed.loc[combo_annot['docId']==demo_doc]\n",
    "\n",
    "demo_txt = combo_txt[demo_doc]\n",
    "\n",
    "encoded_demo_txt = tokenizer(demo_txt, padding='max_length', max_length=512, truncation=True)\n",
    "demo_token_startchar = []\n",
    "for idx, id in enumerate(encoded_demo_txt['input_ids']):\n",
    "    try: tokenCharStart = encoded_demo_txt.token_to_chars(idx).start\n",
    "    except: tokenCharStart = np.nan\n",
    "    demo_token_startchar.append(tokenCharStart)\n",
    "\n",
    "subSpan_ds = {}\n",
    "for comboId, annot in demo_annots.iterrows():\n",
    "    if isinstance(annot['subSpanType'],float): continue # nans are floats\n",
    "    print(comboId)\n",
    "    print(annot['annotType'])\n",
    "    print(annot['subSpanType'])\n",
    "    print(annot['subSpan'][0],annot['subSpan'][1])\n",
    "    subSpanRange = list(range(annot['subSpan'][0],annot['subSpan'][1]))\n",
    "    # print(subSpanRange)\n",
    "    subSpanIds = []\n",
    "    subSpanLabels = []\n",
    "    for id, label, startChar in zip(demo_ids, demo_labels, demo_token_startchar):\n",
    "        if startChar in subSpanRange:\n",
    "            subSpanIds.append(id)\n",
    "            subSpanLabels.append(label)\n",
    "    print(subSpanIds)\n",
    "    print(subSpanLabels)\n",
    "    print(tokenizer.decode(subSpanIds,skip_special_tokens=True))\n",
    "    print()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'levels in the subject sample were decreased to 63% of control value'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combo_txt[demo_doc][1391:1458]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at allenai/biomed_roberta_base were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "class Stage1model(nn.Module):\n",
    "    def __init__(self, dropout=0.1):\n",
    "        super(Stage1model, self).__init__()\n",
    "        self.mod = RobertaModel.from_pretrained(\n",
    "                    model_name,\n",
    "                    num_labels=num_classes+1,\n",
    "                    hidden_dropout_prob=dropout,\n",
    "                    output_hidden_states=True)\n",
    "        self.norm = nn.BatchNorm1d(512, eps=self.mod.config.layer_norm_eps)\n",
    "        self.drop = nn.Dropout(self.mod.config.hidden_dropout_prob)\n",
    "        self.classifier = nn.Linear(self.mod.config.hidden_size, num_classes+1)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        output = self.mod(input_ids=input_ids,attention_mask=attention_mask)\n",
    "        y_hat = output.hidden_states[-1]\n",
    "        y_hat = self.norm(y_hat)\n",
    "        y_hat = self.drop(y_hat)\n",
    "        y_hat = self.classifier(y_hat).permute(0,2,1)\n",
    "        return y_hat\n",
    "\n",
    "model = Stage1model().to(device)\n",
    "\n",
    "model_new = model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Stage1model(\n",
       "  (mod): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): RobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): RobertaPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (norm): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (drop): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=5, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# class OurBERTModel(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(OurBERTModel, self).__init__()\n",
    "#         self.mod = AutoModel.from_pretrained(model_name, num_labels=num_classes+1)\n",
    "#         self.drop = nn.Dropout(self.mod.config.hidden_dropout_prob)\n",
    "#         self.classifier = nn.Linear(self.mod.config.hidden_size, num_classes+1)\n",
    "\n",
    "#     def forward(self, text, att_mask):\n",
    "#         b, num_tokens = text.shape\n",
    "#         token_type = torch.zeros((b, num_tokens), dtype=torch.long).to(device)\n",
    "#         outputs = self.mod(text, attention_mask=att_mask, token_type_ids=token_type)\n",
    "#         return self.classifier(self.drop(outputs['last_hidden_state']))\n",
    "\n",
    "# model = OurBERTModel().to(device)\n",
    "\n",
    "# model_old = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# demo_logits = model(demo_batch['input_ids'], demo_batch['attention_mask'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# demo_logits.permute(0,2,1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# demo_batch['labels'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ypred = []\n",
    "# ytrue = []\n",
    "# for dlabels, dlogits in zip(demo_batch['labels'], demo_logits.permute(0,2,1)):\n",
    "#     print(dlabels.shape)\n",
    "#     print(dlogits.shape)\n",
    "#     for tlogits, tlabels in zip(dlogits, dlabels):\n",
    "#         print(tlabels.shape)\n",
    "#         print(tlogits.shape)\n",
    "#         ypred.append(tlogits.argmax().item())\n",
    "#         ytrue.append(tlabels.item())\n",
    "#         print(ypred)\n",
    "#         print(ytrue)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.CrossEntropyLoss(reduction='sum')\n",
    "optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "n_training_steps = n_epochs * len(batched_train_ds)\n",
    "lr_scheduler = get_scheduler(\n",
    "    name=\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=n_training_steps\n",
    ")\n",
    "\n",
    "\n",
    "def train_epoch(ds, criterion):\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    ypred = []\n",
    "    ytrue = []\n",
    "    for idx, batch in enumerate(ds):\n",
    "\n",
    "        labels = batch['labels']\n",
    "        input_ids = batch['input_ids']\n",
    "        attention_mask = batch['attention_mask']\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        logits = model(input_ids, attention_mask)\n",
    "        \n",
    "        loss = criterion(logits, labels)\n",
    "        # loss = (loss * attention_mask).sum() / (attention_mask).sum()\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "\n",
    "        progress_bar.update(1)\n",
    "            \n",
    "    return loss.item()\n",
    "\n",
    "\n",
    "\n",
    "def eval_epoch(ds, criterion):\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    ypred = []\n",
    "    ytrue = []\n",
    "    with torch.no_grad():\n",
    "        for idx, batch in enumerate(ds):\n",
    "\n",
    "            labels = batch['labels']\n",
    "            input_ids = batch['input_ids']\n",
    "            attention_mask = batch['attention_mask']\n",
    "\n",
    "            logits = model(input_ids, attention_mask)\n",
    "            \n",
    "            loss = criterion(logits, labels)\n",
    "            # loss = (loss * attention_mask).sum() / (attention_mask).sum()\n",
    "\n",
    "            for dlogits, dlabels in zip(logits.permute(0,2,1), labels):\n",
    "                    for tlogits, tlabels in zip(dlogits, dlabels):\n",
    "                        ypred.append(tlogits.argmax().item())\n",
    "                        ytrue.append(tlabels.item())\n",
    "\n",
    "            progress_bar.update(1)\n",
    "\n",
    "    acc = metrics.accuracy_score(ytrue,ypred)\n",
    "    report = classification_report(ytrue,ypred,\n",
    "                                    labels=list(task_map.values()),\n",
    "                                    target_names=list(task_map.keys()),\n",
    "                                    output_dict=True,\n",
    "                                    zero_division=0)\n",
    "\n",
    "                                    \n",
    "\n",
    "    return loss.item(), acc, report, ytrue, ypred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8d953561a514cf9b3cf2d84602310b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/730 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============ Begin Epoch 1 ============\n",
      "Train loss: 406.94586181640625\n",
      "Eval on train set loss: 400.40875244140625   accuracy: 0.9532058618630573\n",
      "Eval on dev set loss: 941.571044921875   accuracy: 0.9559895833333333\n",
      "============ Begin Epoch 2 ============\n",
      "Train loss: 220.6007080078125\n",
      "Eval on train set loss: 211.5440673828125   accuracy: 0.9694342157643312\n",
      "Eval on dev set loss: 641.9205932617188   accuracy: 0.9643446180555556\n",
      "============ Begin Epoch 3 ============\n",
      "Train loss: 214.52008056640625\n",
      "Eval on train set loss: 178.28677368164062   accuracy: 0.9715801652070064\n",
      "Eval on dev set loss: 736.715087890625   accuracy: 0.9662760416666667\n",
      "============ Begin Epoch 4 ============\n",
      "Train loss: 152.16641235351562\n",
      "Eval on train set loss: 140.12814331054688   accuracy: 0.9789199343152867\n",
      "Eval on dev set loss: 593.135986328125   accuracy: 0.9675130208333333\n",
      "============ Begin Epoch 5 ============\n",
      "Train loss: 125.63841247558594\n",
      "Eval on train set loss: 113.49224853515625   accuracy: 0.9836721238057324\n",
      "Eval on dev set loss: 581.252197265625   accuracy: 0.9693793402777777\n",
      "============ Begin Epoch 6 ============\n",
      "Train loss: 128.71694946289062\n",
      "Eval on train set loss: 109.64109802246094   accuracy: 0.984922372611465\n",
      "Eval on dev set loss: 621.4912109375   accuracy: 0.9668619791666667\n",
      "============ Begin Epoch 7 ============\n",
      "Train loss: 91.40162658691406\n",
      "Eval on train set loss: 82.73343658447266   accuracy: 0.9882999104299363\n",
      "Eval on dev set loss: 582.4826049804688   accuracy: 0.9695095486111112\n",
      "============ Begin Epoch 8 ============\n",
      "Train loss: 91.12594604492188\n",
      "Eval on train set loss: 67.14522552490234   accuracy: 0.9906200238853503\n",
      "Eval on dev set loss: 602.3134155273438   accuracy: 0.9696180555555556\n",
      "============ Begin Epoch 9 ============\n",
      "Train loss: 79.0506591796875\n",
      "Eval on train set loss: 56.16996383666992   accuracy: 0.9915032842356688\n",
      "Eval on dev set loss: 604.7386474609375   accuracy: 0.96875\n",
      "============ Begin Epoch 10 ============\n",
      "Train loss: 72.98187255859375\n",
      "Eval on train set loss: 52.08488082885742   accuracy: 0.9919635748407644\n",
      "Eval on dev set loss: 611.4227294921875   accuracy: 0.9690538194444445\n"
     ]
    }
   ],
   "source": [
    "run_report = {  'epoch':[],\n",
    "                'train_loss':[],\n",
    "                'eval_train_loss':[],\n",
    "                'eval_train_acc':[],\n",
    "                'eval_train_ytrue':[],\n",
    "                'eval_train_ypred':[],\n",
    "                'eval_train_rpt':[],\n",
    "                'eval_dev_loss':[],\n",
    "                'eval_dev_acc':[],\n",
    "                'eval_dev_ytrue':[],\n",
    "                'eval_dev_ypred':[],\n",
    "                'eval_dev_rpt':[],\n",
    "             }\n",
    "\n",
    "num_total_steps = n_epochs * (len(batched_train_ds) * 2 + len(batched_dev_ds))\n",
    "progress_bar = tqdm(range(num_total_steps))\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    run_report['epoch'].append(epoch)\n",
    "    \n",
    "    print(f\"============ Begin Epoch {epoch+1} ============\")\n",
    "\n",
    "    loss = train_epoch(batched_train_ds, criterion)\n",
    "    print(f\"Train loss: {loss}\")\n",
    "    run_report['train_loss'].append(loss)\n",
    "    \n",
    "    output = eval_epoch(batched_train_ds, criterion)\n",
    "    (loss, acc, report, ytrue, ypred) = output\n",
    "    print(f'Eval on train set loss: {loss}   accuracy: {acc}')\n",
    "    run_report['eval_train_loss'].append(loss)\n",
    "    run_report['eval_train_acc'].append(acc)\n",
    "    run_report['eval_train_ytrue'].append(ytrue)\n",
    "    run_report['eval_train_ypred'].append(ypred)\n",
    "    run_report['eval_train_rpt'].append(report)\n",
    "\n",
    "    output = eval_epoch(batched_dev_ds, criterion)\n",
    "    (loss, acc, report, ytrue, ypred) = output\n",
    "    print(f'Eval on dev set loss: {loss}   accuracy: {acc}')\n",
    "    run_report['eval_dev_loss'].append(loss)\n",
    "    run_report['eval_dev_acc'].append(acc)\n",
    "    run_report['eval_dev_ytrue'].append(ytrue)\n",
    "    run_report['eval_dev_ypred'].append(ypred)\n",
    "    run_report['eval_dev_rpt'].append(report)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[42833   145    75   206    46]\n",
      " [   73  1096     5     9     2]\n",
      " [  149    25   249    98     9]\n",
      " [  212    16   105   408     4]\n",
      " [  135    17    35    60    68]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_labels = list(task_map.keys())\n",
    "display_labels.insert(0,str('NoLabel'))\n",
    "cm = confusion_matrix(ytrue,ypred)\n",
    "print(cm)\n",
    "ConfusionMatrixDisplay(confusion_matrix=cm,display_labels=display_labels).plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tabulate_metrics(reports):\n",
    "    metrics = []\n",
    "    for epoch in reports:\n",
    "        # print(epoch)\n",
    "        epoch_metrics = {}\n",
    "        for task, rpt in task_map.items():\n",
    "            for metric, value in epoch[task].items():\n",
    "                epoch_metrics[str(task+'_'+metric)] = value\n",
    "        metrics.append(epoch_metrics)\n",
    "\n",
    "    metrics = pd.DataFrame.from_dict(metrics)\n",
    "    metrics.index.rename('epoch')\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Quantity_precision</th>\n",
       "      <th>Quantity_recall</th>\n",
       "      <th>Quantity_f1-score</th>\n",
       "      <th>Quantity_support</th>\n",
       "      <th>MeasuredProperty_precision</th>\n",
       "      <th>MeasuredProperty_recall</th>\n",
       "      <th>MeasuredProperty_f1-score</th>\n",
       "      <th>MeasuredProperty_support</th>\n",
       "      <th>MeasuredEntity_precision</th>\n",
       "      <th>MeasuredEntity_recall</th>\n",
       "      <th>MeasuredEntity_f1-score</th>\n",
       "      <th>MeasuredEntity_support</th>\n",
       "      <th>Qualifier_precision</th>\n",
       "      <th>Qualifier_recall</th>\n",
       "      <th>Qualifier_f1-score</th>\n",
       "      <th>Qualifier_support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.726627</td>\n",
       "      <td>0.892545</td>\n",
       "      <td>0.801085</td>\n",
       "      <td>4467</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1707</td>\n",
       "      <td>0.578947</td>\n",
       "      <td>0.003409</td>\n",
       "      <td>0.006778</td>\n",
       "      <td>3227</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.872340</td>\n",
       "      <td>0.954556</td>\n",
       "      <td>0.911598</td>\n",
       "      <td>4467</td>\n",
       "      <td>0.532271</td>\n",
       "      <td>0.391330</td>\n",
       "      <td>0.451047</td>\n",
       "      <td>1707</td>\n",
       "      <td>0.572028</td>\n",
       "      <td>0.506972</td>\n",
       "      <td>0.537539</td>\n",
       "      <td>3227</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000778</td>\n",
       "      <td>0.001553</td>\n",
       "      <td>1286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.928050</td>\n",
       "      <td>0.938437</td>\n",
       "      <td>0.933215</td>\n",
       "      <td>4467</td>\n",
       "      <td>0.640351</td>\n",
       "      <td>0.427651</td>\n",
       "      <td>0.512821</td>\n",
       "      <td>1707</td>\n",
       "      <td>0.657637</td>\n",
       "      <td>0.436319</td>\n",
       "      <td>0.524590</td>\n",
       "      <td>3227</td>\n",
       "      <td>0.764706</td>\n",
       "      <td>0.020218</td>\n",
       "      <td>0.039394</td>\n",
       "      <td>1286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.905762</td>\n",
       "      <td>0.974703</td>\n",
       "      <td>0.938969</td>\n",
       "      <td>4467</td>\n",
       "      <td>0.607873</td>\n",
       "      <td>0.696544</td>\n",
       "      <td>0.649195</td>\n",
       "      <td>1707</td>\n",
       "      <td>0.701610</td>\n",
       "      <td>0.661605</td>\n",
       "      <td>0.681021</td>\n",
       "      <td>3227</td>\n",
       "      <td>0.796491</td>\n",
       "      <td>0.176516</td>\n",
       "      <td>0.288988</td>\n",
       "      <td>1286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.940167</td>\n",
       "      <td>0.981419</td>\n",
       "      <td>0.960350</td>\n",
       "      <td>4467</td>\n",
       "      <td>0.645040</td>\n",
       "      <td>0.803749</td>\n",
       "      <td>0.715702</td>\n",
       "      <td>1707</td>\n",
       "      <td>0.780447</td>\n",
       "      <td>0.724822</td>\n",
       "      <td>0.751607</td>\n",
       "      <td>3227</td>\n",
       "      <td>0.853211</td>\n",
       "      <td>0.361586</td>\n",
       "      <td>0.507919</td>\n",
       "      <td>1286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.939826</td>\n",
       "      <td>0.989478</td>\n",
       "      <td>0.964013</td>\n",
       "      <td>4467</td>\n",
       "      <td>0.650240</td>\n",
       "      <td>0.873462</td>\n",
       "      <td>0.745500</td>\n",
       "      <td>1707</td>\n",
       "      <td>0.764123</td>\n",
       "      <td>0.813139</td>\n",
       "      <td>0.787870</td>\n",
       "      <td>3227</td>\n",
       "      <td>0.787166</td>\n",
       "      <td>0.572317</td>\n",
       "      <td>0.662765</td>\n",
       "      <td>1286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.954614</td>\n",
       "      <td>0.988807</td>\n",
       "      <td>0.971410</td>\n",
       "      <td>4467</td>\n",
       "      <td>0.728203</td>\n",
       "      <td>0.875806</td>\n",
       "      <td>0.795213</td>\n",
       "      <td>1707</td>\n",
       "      <td>0.870699</td>\n",
       "      <td>0.776263</td>\n",
       "      <td>0.820773</td>\n",
       "      <td>3227</td>\n",
       "      <td>0.900846</td>\n",
       "      <td>0.579316</td>\n",
       "      <td>0.705159</td>\n",
       "      <td>1286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.974574</td>\n",
       "      <td>0.986792</td>\n",
       "      <td>0.980645</td>\n",
       "      <td>4467</td>\n",
       "      <td>0.801729</td>\n",
       "      <td>0.869361</td>\n",
       "      <td>0.834177</td>\n",
       "      <td>1707</td>\n",
       "      <td>0.861929</td>\n",
       "      <td>0.853114</td>\n",
       "      <td>0.857499</td>\n",
       "      <td>3227</td>\n",
       "      <td>0.932373</td>\n",
       "      <td>0.653966</td>\n",
       "      <td>0.768739</td>\n",
       "      <td>1286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.978492</td>\n",
       "      <td>0.987911</td>\n",
       "      <td>0.983179</td>\n",
       "      <td>4467</td>\n",
       "      <td>0.832121</td>\n",
       "      <td>0.871119</td>\n",
       "      <td>0.851173</td>\n",
       "      <td>1707</td>\n",
       "      <td>0.858220</td>\n",
       "      <td>0.881624</td>\n",
       "      <td>0.869765</td>\n",
       "      <td>3227</td>\n",
       "      <td>0.894231</td>\n",
       "      <td>0.723173</td>\n",
       "      <td>0.799656</td>\n",
       "      <td>1286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.979592</td>\n",
       "      <td>0.988583</td>\n",
       "      <td>0.984067</td>\n",
       "      <td>4467</td>\n",
       "      <td>0.825619</td>\n",
       "      <td>0.898653</td>\n",
       "      <td>0.860589</td>\n",
       "      <td>1707</td>\n",
       "      <td>0.874499</td>\n",
       "      <td>0.878835</td>\n",
       "      <td>0.876662</td>\n",
       "      <td>3227</td>\n",
       "      <td>0.908046</td>\n",
       "      <td>0.737170</td>\n",
       "      <td>0.813734</td>\n",
       "      <td>1286</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Quantity_precision  Quantity_recall  Quantity_f1-score  Quantity_support  \\\n",
       "0            0.726627         0.892545           0.801085              4467   \n",
       "1            0.872340         0.954556           0.911598              4467   \n",
       "2            0.928050         0.938437           0.933215              4467   \n",
       "3            0.905762         0.974703           0.938969              4467   \n",
       "4            0.940167         0.981419           0.960350              4467   \n",
       "5            0.939826         0.989478           0.964013              4467   \n",
       "6            0.954614         0.988807           0.971410              4467   \n",
       "7            0.974574         0.986792           0.980645              4467   \n",
       "8            0.978492         0.987911           0.983179              4467   \n",
       "9            0.979592         0.988583           0.984067              4467   \n",
       "\n",
       "   MeasuredProperty_precision  MeasuredProperty_recall  \\\n",
       "0                    0.000000                 0.000000   \n",
       "1                    0.532271                 0.391330   \n",
       "2                    0.640351                 0.427651   \n",
       "3                    0.607873                 0.696544   \n",
       "4                    0.645040                 0.803749   \n",
       "5                    0.650240                 0.873462   \n",
       "6                    0.728203                 0.875806   \n",
       "7                    0.801729                 0.869361   \n",
       "8                    0.832121                 0.871119   \n",
       "9                    0.825619                 0.898653   \n",
       "\n",
       "   MeasuredProperty_f1-score  MeasuredProperty_support  \\\n",
       "0                   0.000000                      1707   \n",
       "1                   0.451047                      1707   \n",
       "2                   0.512821                      1707   \n",
       "3                   0.649195                      1707   \n",
       "4                   0.715702                      1707   \n",
       "5                   0.745500                      1707   \n",
       "6                   0.795213                      1707   \n",
       "7                   0.834177                      1707   \n",
       "8                   0.851173                      1707   \n",
       "9                   0.860589                      1707   \n",
       "\n",
       "   MeasuredEntity_precision  MeasuredEntity_recall  MeasuredEntity_f1-score  \\\n",
       "0                  0.578947               0.003409                 0.006778   \n",
       "1                  0.572028               0.506972                 0.537539   \n",
       "2                  0.657637               0.436319                 0.524590   \n",
       "3                  0.701610               0.661605                 0.681021   \n",
       "4                  0.780447               0.724822                 0.751607   \n",
       "5                  0.764123               0.813139                 0.787870   \n",
       "6                  0.870699               0.776263                 0.820773   \n",
       "7                  0.861929               0.853114                 0.857499   \n",
       "8                  0.858220               0.881624                 0.869765   \n",
       "9                  0.874499               0.878835                 0.876662   \n",
       "\n",
       "   MeasuredEntity_support  Qualifier_precision  Qualifier_recall  \\\n",
       "0                    3227             0.000000          0.000000   \n",
       "1                    3227             0.500000          0.000778   \n",
       "2                    3227             0.764706          0.020218   \n",
       "3                    3227             0.796491          0.176516   \n",
       "4                    3227             0.853211          0.361586   \n",
       "5                    3227             0.787166          0.572317   \n",
       "6                    3227             0.900846          0.579316   \n",
       "7                    3227             0.932373          0.653966   \n",
       "8                    3227             0.894231          0.723173   \n",
       "9                    3227             0.908046          0.737170   \n",
       "\n",
       "   Qualifier_f1-score  Qualifier_support  \n",
       "0            0.000000               1286  \n",
       "1            0.001553               1286  \n",
       "2            0.039394               1286  \n",
       "3            0.288988               1286  \n",
       "4            0.507919               1286  \n",
       "5            0.662765               1286  \n",
       "6            0.705159               1286  \n",
       "7            0.768739               1286  \n",
       "8            0.799656               1286  \n",
       "9            0.813734               1286  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set_metrics = tabulate_metrics(run_report['eval_train_rpt'])\n",
    "train_set_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Quantity_precision</th>\n",
       "      <th>Quantity_recall</th>\n",
       "      <th>Quantity_f1-score</th>\n",
       "      <th>Quantity_support</th>\n",
       "      <th>MeasuredProperty_precision</th>\n",
       "      <th>MeasuredProperty_recall</th>\n",
       "      <th>MeasuredProperty_f1-score</th>\n",
       "      <th>MeasuredProperty_support</th>\n",
       "      <th>MeasuredEntity_precision</th>\n",
       "      <th>MeasuredEntity_recall</th>\n",
       "      <th>MeasuredEntity_f1-score</th>\n",
       "      <th>MeasuredEntity_support</th>\n",
       "      <th>Qualifier_precision</th>\n",
       "      <th>Qualifier_recall</th>\n",
       "      <th>Qualifier_f1-score</th>\n",
       "      <th>Qualifier_support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.698305</td>\n",
       "      <td>0.869198</td>\n",
       "      <td>0.774436</td>\n",
       "      <td>1185</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>530</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.001342</td>\n",
       "      <td>0.002677</td>\n",
       "      <td>745</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.810230</td>\n",
       "      <td>0.922363</td>\n",
       "      <td>0.862668</td>\n",
       "      <td>1185</td>\n",
       "      <td>0.485531</td>\n",
       "      <td>0.284906</td>\n",
       "      <td>0.359096</td>\n",
       "      <td>530</td>\n",
       "      <td>0.428333</td>\n",
       "      <td>0.344966</td>\n",
       "      <td>0.382156</td>\n",
       "      <td>745</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.882799</td>\n",
       "      <td>0.883544</td>\n",
       "      <td>0.883172</td>\n",
       "      <td>1185</td>\n",
       "      <td>0.572954</td>\n",
       "      <td>0.303774</td>\n",
       "      <td>0.397041</td>\n",
       "      <td>530</td>\n",
       "      <td>0.491018</td>\n",
       "      <td>0.220134</td>\n",
       "      <td>0.303985</td>\n",
       "      <td>745</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.009524</td>\n",
       "      <td>0.018519</td>\n",
       "      <td>315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.830174</td>\n",
       "      <td>0.924051</td>\n",
       "      <td>0.874601</td>\n",
       "      <td>1185</td>\n",
       "      <td>0.508584</td>\n",
       "      <td>0.447170</td>\n",
       "      <td>0.475904</td>\n",
       "      <td>530</td>\n",
       "      <td>0.475432</td>\n",
       "      <td>0.480537</td>\n",
       "      <td>0.477971</td>\n",
       "      <td>745</td>\n",
       "      <td>0.609756</td>\n",
       "      <td>0.079365</td>\n",
       "      <td>0.140449</td>\n",
       "      <td>315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.853261</td>\n",
       "      <td>0.927426</td>\n",
       "      <td>0.888799</td>\n",
       "      <td>1185</td>\n",
       "      <td>0.512146</td>\n",
       "      <td>0.477358</td>\n",
       "      <td>0.494141</td>\n",
       "      <td>530</td>\n",
       "      <td>0.515942</td>\n",
       "      <td>0.477852</td>\n",
       "      <td>0.496167</td>\n",
       "      <td>745</td>\n",
       "      <td>0.626866</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.219895</td>\n",
       "      <td>315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.818713</td>\n",
       "      <td>0.945148</td>\n",
       "      <td>0.877399</td>\n",
       "      <td>1185</td>\n",
       "      <td>0.464286</td>\n",
       "      <td>0.515094</td>\n",
       "      <td>0.488372</td>\n",
       "      <td>530</td>\n",
       "      <td>0.470787</td>\n",
       "      <td>0.562416</td>\n",
       "      <td>0.512538</td>\n",
       "      <td>745</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.253968</td>\n",
       "      <td>0.351648</td>\n",
       "      <td>315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.838369</td>\n",
       "      <td>0.936709</td>\n",
       "      <td>0.884815</td>\n",
       "      <td>1185</td>\n",
       "      <td>0.508637</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.504282</td>\n",
       "      <td>530</td>\n",
       "      <td>0.576182</td>\n",
       "      <td>0.441611</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>745</td>\n",
       "      <td>0.611111</td>\n",
       "      <td>0.139683</td>\n",
       "      <td>0.227390</td>\n",
       "      <td>315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.858156</td>\n",
       "      <td>0.918987</td>\n",
       "      <td>0.887531</td>\n",
       "      <td>1185</td>\n",
       "      <td>0.526652</td>\n",
       "      <td>0.466038</td>\n",
       "      <td>0.494494</td>\n",
       "      <td>530</td>\n",
       "      <td>0.529015</td>\n",
       "      <td>0.526174</td>\n",
       "      <td>0.527591</td>\n",
       "      <td>745</td>\n",
       "      <td>0.593023</td>\n",
       "      <td>0.161905</td>\n",
       "      <td>0.254364</td>\n",
       "      <td>315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.841294</td>\n",
       "      <td>0.921519</td>\n",
       "      <td>0.879581</td>\n",
       "      <td>1185</td>\n",
       "      <td>0.539683</td>\n",
       "      <td>0.449057</td>\n",
       "      <td>0.490216</td>\n",
       "      <td>530</td>\n",
       "      <td>0.518972</td>\n",
       "      <td>0.569128</td>\n",
       "      <td>0.542894</td>\n",
       "      <td>745</td>\n",
       "      <td>0.489362</td>\n",
       "      <td>0.219048</td>\n",
       "      <td>0.302632</td>\n",
       "      <td>315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.843726</td>\n",
       "      <td>0.924895</td>\n",
       "      <td>0.882448</td>\n",
       "      <td>1185</td>\n",
       "      <td>0.530917</td>\n",
       "      <td>0.469811</td>\n",
       "      <td>0.498498</td>\n",
       "      <td>530</td>\n",
       "      <td>0.522407</td>\n",
       "      <td>0.547651</td>\n",
       "      <td>0.534731</td>\n",
       "      <td>745</td>\n",
       "      <td>0.527132</td>\n",
       "      <td>0.215873</td>\n",
       "      <td>0.306306</td>\n",
       "      <td>315</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Quantity_precision  Quantity_recall  Quantity_f1-score  Quantity_support  \\\n",
       "0            0.698305         0.869198           0.774436              1185   \n",
       "1            0.810230         0.922363           0.862668              1185   \n",
       "2            0.882799         0.883544           0.883172              1185   \n",
       "3            0.830174         0.924051           0.874601              1185   \n",
       "4            0.853261         0.927426           0.888799              1185   \n",
       "5            0.818713         0.945148           0.877399              1185   \n",
       "6            0.838369         0.936709           0.884815              1185   \n",
       "7            0.858156         0.918987           0.887531              1185   \n",
       "8            0.841294         0.921519           0.879581              1185   \n",
       "9            0.843726         0.924895           0.882448              1185   \n",
       "\n",
       "   MeasuredProperty_precision  MeasuredProperty_recall  \\\n",
       "0                    0.000000                 0.000000   \n",
       "1                    0.485531                 0.284906   \n",
       "2                    0.572954                 0.303774   \n",
       "3                    0.508584                 0.447170   \n",
       "4                    0.512146                 0.477358   \n",
       "5                    0.464286                 0.515094   \n",
       "6                    0.508637                 0.500000   \n",
       "7                    0.526652                 0.466038   \n",
       "8                    0.539683                 0.449057   \n",
       "9                    0.530917                 0.469811   \n",
       "\n",
       "   MeasuredProperty_f1-score  MeasuredProperty_support  \\\n",
       "0                   0.000000                       530   \n",
       "1                   0.359096                       530   \n",
       "2                   0.397041                       530   \n",
       "3                   0.475904                       530   \n",
       "4                   0.494141                       530   \n",
       "5                   0.488372                       530   \n",
       "6                   0.504282                       530   \n",
       "7                   0.494494                       530   \n",
       "8                   0.490216                       530   \n",
       "9                   0.498498                       530   \n",
       "\n",
       "   MeasuredEntity_precision  MeasuredEntity_recall  MeasuredEntity_f1-score  \\\n",
       "0                  0.500000               0.001342                 0.002677   \n",
       "1                  0.428333               0.344966                 0.382156   \n",
       "2                  0.491018               0.220134                 0.303985   \n",
       "3                  0.475432               0.480537                 0.477971   \n",
       "4                  0.515942               0.477852                 0.496167   \n",
       "5                  0.470787               0.562416                 0.512538   \n",
       "6                  0.576182               0.441611                 0.500000   \n",
       "7                  0.529015               0.526174                 0.527591   \n",
       "8                  0.518972               0.569128                 0.542894   \n",
       "9                  0.522407               0.547651                 0.534731   \n",
       "\n",
       "   MeasuredEntity_support  Qualifier_precision  Qualifier_recall  \\\n",
       "0                     745             0.000000          0.000000   \n",
       "1                     745             0.000000          0.000000   \n",
       "2                     745             0.333333          0.009524   \n",
       "3                     745             0.609756          0.079365   \n",
       "4                     745             0.626866          0.133333   \n",
       "5                     745             0.571429          0.253968   \n",
       "6                     745             0.611111          0.139683   \n",
       "7                     745             0.593023          0.161905   \n",
       "8                     745             0.489362          0.219048   \n",
       "9                     745             0.527132          0.215873   \n",
       "\n",
       "   Qualifier_f1-score  Qualifier_support  \n",
       "0            0.000000                315  \n",
       "1            0.000000                315  \n",
       "2            0.018519                315  \n",
       "3            0.140449                315  \n",
       "4            0.219895                315  \n",
       "5            0.351648                315  \n",
       "6            0.227390                315  \n",
       "7            0.254364                315  \n",
       "8            0.302632                315  \n",
       "9            0.306306                315  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_set_metrics = tabulate_metrics(run_report['eval_dev_rpt'])\n",
    "dev_set_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "########################## loss plot #######################\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "x = np.array(range(len(run_report['epoch'])))\n",
    "y = np.array(run_report['eval_dev_loss'])\n",
    "plt.plot(x,y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "########################## accuracy plot #######################\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "x = np.array(range(len(run_report['epoch'])))\n",
    "y = np.array(run_report['eval_dev_acc'])\n",
    "plt.plot(x,y)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 ('test')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b5060d9997a95c2acb3a42af5d14caeb5dba3e5b7e20123b9f235f707614ce30"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
