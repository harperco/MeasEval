{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from torch import nn\n",
    "from torch.optim import AdamW, Adam\n",
    "from transformers import AutoModel, AutoTokenizer, AutoModelForTokenClassification, AutoModelForSequenceClassification\n",
    "from transformers import RobertaForSequenceClassification\n",
    "from transformers import RobertaModel\n",
    "from transformers import get_scheduler\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn import metrics as metrics\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "import re\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "random.seed(42)\n",
    "reprocess_raw =  False\n",
    "\n",
    "batch_size = 8 # documents\n",
    "learning_rate = 5e-5\n",
    "n_epochs = 10\n",
    "\n",
    "task_map = {'Quantity':1}\n",
    "# task_map = {'Quantity':1,'MeasuredProperty':2,'MeasuredEntity':3,'Qualifier':4} # uncomment for multi-class\n",
    "num_classes = len(task_map)\n",
    "\n",
    "model_name = 'allenai/biomed_roberta_base'\n",
    "# model_name = 'bert-base-cased'\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "# device = 'cpu' # uncomment this to make debugging easier\n",
    "\n",
    "data_size_reduce = 1 # multiplier for making small datasets\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "currentdir = os.getcwd() # ~/MeasEval/baselines\n",
    "\n",
    "combopath_txt = os.path.join(currentdir, \"../data/raw/combo/text/\")\n",
    "combopath_annot = os.path.join(currentdir, \"../data/raw/combo/tsv/\")\n",
    "\n",
    "interimpath = os.path.join(currentdir, \"../data/interim/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_raw_txt(docs):\n",
    "    processesd_txt = {}\n",
    "    remove_markers = True\n",
    "\n",
    "    cnt_toks = {\"figs.\": 0, \"fig.\": 0, \"et al.\": 0,\n",
    "            \"ref.\": 0, \"eq.\": 0, \"e.g.\": 0,\n",
    "            \"i.e.\": 0, \"nos.\": 0, \"no.\": 0,\n",
    "            \"spp.\": 0\n",
    "            }\n",
    "    regex_end_checker = [\".*[a-zA-Z]figs\\.$\", \n",
    "                        \".*[a-zA-Z]fig\\.$\",\n",
    "                        \".*[a-zA-Z]et al\\.$\",\n",
    "                        \".*[a-zA-Z]ref\\.$\",\n",
    "                        \".*[a-zA-Z]eq\\.$\",\n",
    "                        \".*[a-zA-Z]e\\.g\\.$\",\n",
    "                        \".*[a-zA-Z]i\\.e\\.$\",\n",
    "                        \".*[a-zA-Z]nos\\.$\",\n",
    "                        \".*[a-zA-Z]no\\.$\",\n",
    "                        \".*[a-zA-Z]spp\\.$\",\n",
    "                        # figs., fig., et al., Ref., Eq., e.g., i.e., Nos., No., spp.\n",
    "                    ]\n",
    "\n",
    "    assert len(cnt_toks) == len(regex_end_checker)\n",
    "\n",
    "    for docId, doc in docs.items():\n",
    "        flag = False\n",
    "        sentences = sent_tokenize(doc)\n",
    "\n",
    "        fixed_sentence_tokens = []\n",
    "        curr_len = 0\n",
    "        for s in sentences:\n",
    "            if flag == True:\n",
    "                assert s[0] != ' '\n",
    "                white_length = doc[curr_len:].find(s[0])\n",
    "\n",
    "                prev_len = len(fixed_sentence_tokens[-1])\n",
    "                fixed_sentence_tokens[-1] = fixed_sentence_tokens[-1] + (\" \"*white_length) + s\n",
    "\n",
    "                assert fixed_sentence_tokens[-1][prev_len+white_length] == doc[curr_len+white_length], (fixed_sentence_tokens[-1], doc, curr_len, tmp_this_sent_len)\n",
    "                tmp_this_sent_len = white_length + len(s)\n",
    "                assert fixed_sentence_tokens[-1][-1] == doc[curr_len+tmp_this_sent_len-1], (fixed_sentence_tokens[-1], doc, curr_len, tmp_this_sent_len)\n",
    "                curr_len += tmp_this_sent_len\n",
    "            else:\n",
    "                if len(fixed_sentence_tokens) != 0:\n",
    "                    assert s[0] != ' '\n",
    "                    white_length = doc[curr_len:].find(s[0])\n",
    "                    fixed_sentence_tokens.append( (\" \"*white_length) + s )\n",
    "                else:\n",
    "                    fixed_sentence_tokens.append(s)\n",
    "                assert fixed_sentence_tokens[-1][0] == doc[curr_len], (fixed_sentence_tokens, doc, curr_len, tmp_this_sent_len)\n",
    "                tmp_this_sent_len = len(fixed_sentence_tokens[-1])\n",
    "                assert fixed_sentence_tokens[-1][-1] == doc[curr_len+tmp_this_sent_len-1], (fixed_sentence_tokens[-1], doc, curr_len, tmp_this_sent_len)\n",
    "                curr_len += tmp_this_sent_len\n",
    "\n",
    "            lower_cased_s = fixed_sentence_tokens[-1].lower()\n",
    "            flag = False\n",
    "            if remove_markers:\n",
    "                for i, k in enumerate(cnt_toks):\n",
    "                    this_regex_pattern = regex_end_checker[i]\n",
    "                    if lower_cased_s.endswith(k) and re.match(this_regex_pattern, lower_cased_s) == None:\n",
    "                        cnt_toks[k] += 1\n",
    "                        flag = True\n",
    "                        break\n",
    "\n",
    "        processesd_txt[docId] = ''.join(fixed_sentence_tokens)\n",
    "    return processesd_txt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(reprocess_raw = False):\n",
    "\n",
    "    if reprocess_raw == True:\n",
    "        docIds = []\n",
    "        combo_txt = {}\n",
    "        for fn in os.listdir(combopath_txt):\n",
    "            docIds.append(fn[:-4])\n",
    "            path = combopath_txt+fn\n",
    "            with open(path) as textfile:\n",
    "                    text = textfile.read()\n",
    "                    #[:-4] strips off the .txt to get the id\n",
    "                    combo_txt[fn[:-4]] = text\n",
    "\n",
    "        combo_annot = pd.DataFrame()\n",
    "        for fn in os.listdir(combopath_annot):\n",
    "            path = combopath_annot+fn\n",
    "            file = pd.read_csv(path,delimiter='\\t',encoding='utf-8')\n",
    "            combo_annot = pd.concat([combo_annot, file],ignore_index=True)\n",
    "\n",
    "        combo_txt = process_raw_txt(combo_txt)\n",
    "        assert docIds == list(combo_txt.keys()), (len(docIds), len(list(combo_txt.keys())))\n",
    "\n",
    "        with open(interimpath+'combo_txt.json','w') as f:\n",
    "            json.dump(combo_txt, f)\n",
    "\n",
    "        combo_annot.to_csv(interimpath+'combo_annot.csv')\n",
    "\n",
    "        return docIds, combo_txt, combo_annot\n",
    "    else:\n",
    "        combo_annot = pd.read_csv(interimpath+'combo_annot.csv')\n",
    "\n",
    "        with open(interimpath+'combo_txt.json','r') as f:\n",
    "            combo_txt = json.load(f)\n",
    "\n",
    "        docIds = list(combo_txt.keys())\n",
    "    \n",
    "        return docIds, combo_txt, combo_annot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "combo_docs, combo_txt, combo_annot = read_data(reprocess_raw = reprocess_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### train/dev/test split options\n",
    "\n",
    "random.seed(42)\n",
    "\n",
    "percent_to_test = .1\n",
    "percent_to_dev = .2\n",
    "percent_to_train =  1 - percent_to_dev - percent_to_test\n",
    "\n",
    "n_doc = len(combo_docs)\n",
    "split_train = int(np.round(n_doc * percent_to_train))\n",
    "split_dev = split_train + int(np.round(n_doc * percent_to_dev))\n",
    "\n",
    "train_docs = combo_docs[:split_train]\n",
    "dev_docs = combo_docs[split_train:split_dev]\n",
    "test_docs = combo_docs[split_dev:]\n",
    "\n",
    "train_docs = random.sample(train_docs, int(len(train_docs)*data_size_reduce))\n",
    "dev_docs = random.sample(dev_docs, int(len(dev_docs)*data_size_reduce))\n",
    "test_docs = random.sample(test_docs, int(len(test_docs)*data_size_reduce))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## Tokenizer ###########\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>docId</th>\n",
       "      <th>annotId</th>\n",
       "      <th>annotType</th>\n",
       "      <th>annotSpan</th>\n",
       "      <th>subSpanType</th>\n",
       "      <th>linkId</th>\n",
       "      <th>linkSpan</th>\n",
       "      <th>subSpan</th>\n",
       "      <th>unit</th>\n",
       "      <th>unitEncoded</th>\n",
       "      <th>misc</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>comboId</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>S0019103513005058-3094_T154-4</th>\n",
       "      <td>S0019103513005058-3094</td>\n",
       "      <td>T154-4</td>\n",
       "      <td>MeasuredEntity</td>\n",
       "      <td>[1000, 1004]</td>\n",
       "      <td>HasProperty</td>\n",
       "      <td>T144-4</td>\n",
       "      <td>[1041, 1046]</td>\n",
       "      <td>[1046, 1046]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>S0019103511004994-1511_T135-5</th>\n",
       "      <td>S0019103511004994-1511</td>\n",
       "      <td>T135-5</td>\n",
       "      <td>MeasuredProperty</td>\n",
       "      <td>[1520, 1538]</td>\n",
       "      <td>HasQuantity</td>\n",
       "      <td>T145-5</td>\n",
       "      <td>[1542, 1548]</td>\n",
       "      <td>[1548, 1548]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>S0019103512002801-1849_T63-3</th>\n",
       "      <td>S0019103512002801-1849</td>\n",
       "      <td>T63-3</td>\n",
       "      <td>Quantity</td>\n",
       "      <td>[144, 149]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>keV</td>\n",
       "      <td>[1071, 846]</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>S0960148113004989-3277_T1-1</th>\n",
       "      <td>S0960148113004989-3277</td>\n",
       "      <td>T1-1</td>\n",
       "      <td>Quantity</td>\n",
       "      <td>[130, 134]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>h</td>\n",
       "      <td>[298]</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>S0927024813001955-679_T3-1</th>\n",
       "      <td>S0927024813001955-679</td>\n",
       "      <td>T3-1</td>\n",
       "      <td>Qualifier</td>\n",
       "      <td>[7, 10]</td>\n",
       "      <td>Qualifies</td>\n",
       "      <td>T2-1</td>\n",
       "      <td>[21, 25]</td>\n",
       "      <td>[25, 25]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>S2213158213000582-1469_T1-2</th>\n",
       "      <td>S2213158213000582-1469</td>\n",
       "      <td>T1-2</td>\n",
       "      <td>Quantity</td>\n",
       "      <td>[545, 551]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ms</td>\n",
       "      <td>[4339]</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>S0032063313003218-6651_T3-1</th>\n",
       "      <td>S0032063313003218-6651</td>\n",
       "      <td>T3-1</td>\n",
       "      <td>MeasuredEntity</td>\n",
       "      <td>[360, 394]</td>\n",
       "      <td>HasQuantity</td>\n",
       "      <td>T1-1</td>\n",
       "      <td>[396, 400]</td>\n",
       "      <td>[400, 400]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                docId annotId  \\\n",
       "comboId                                                         \n",
       "S0019103513005058-3094_T154-4  S0019103513005058-3094  T154-4   \n",
       "S0019103511004994-1511_T135-5  S0019103511004994-1511  T135-5   \n",
       "S0019103512002801-1849_T63-3   S0019103512002801-1849   T63-3   \n",
       "S0960148113004989-3277_T1-1    S0960148113004989-3277    T1-1   \n",
       "S0927024813001955-679_T3-1      S0927024813001955-679    T3-1   \n",
       "S2213158213000582-1469_T1-2    S2213158213000582-1469    T1-2   \n",
       "S0032063313003218-6651_T3-1    S0032063313003218-6651    T3-1   \n",
       "\n",
       "                                      annotType     annotSpan  subSpanType  \\\n",
       "comboId                                                                      \n",
       "S0019103513005058-3094_T154-4    MeasuredEntity  [1000, 1004]  HasProperty   \n",
       "S0019103511004994-1511_T135-5  MeasuredProperty  [1520, 1538]  HasQuantity   \n",
       "S0019103512002801-1849_T63-3           Quantity    [144, 149]          NaN   \n",
       "S0960148113004989-3277_T1-1            Quantity    [130, 134]          NaN   \n",
       "S0927024813001955-679_T3-1            Qualifier       [7, 10]    Qualifies   \n",
       "S2213158213000582-1469_T1-2            Quantity    [545, 551]          NaN   \n",
       "S0032063313003218-6651_T3-1      MeasuredEntity    [360, 394]  HasQuantity   \n",
       "\n",
       "                               linkId      linkSpan       subSpan unit  \\\n",
       "comboId                                                                  \n",
       "S0019103513005058-3094_T154-4  T144-4  [1041, 1046]  [1046, 1046]  NaN   \n",
       "S0019103511004994-1511_T135-5  T145-5  [1542, 1548]  [1548, 1548]  NaN   \n",
       "S0019103512002801-1849_T63-3      NaN           NaN           NaN  keV   \n",
       "S0960148113004989-3277_T1-1       NaN           NaN           NaN    h   \n",
       "S0927024813001955-679_T3-1       T2-1      [21, 25]      [25, 25]  NaN   \n",
       "S2213158213000582-1469_T1-2       NaN           NaN           NaN   ms   \n",
       "S0032063313003218-6651_T3-1      T1-1    [396, 400]    [400, 400]  NaN   \n",
       "\n",
       "                               unitEncoded misc  \n",
       "comboId                                          \n",
       "S0019103513005058-3094_T154-4          NaN  NaN  \n",
       "S0019103511004994-1511_T135-5          NaN  NaN  \n",
       "S0019103512002801-1849_T63-3   [1071, 846]  NaN  \n",
       "S0960148113004989-3277_T1-1          [298]  NaN  \n",
       "S0927024813001955-679_T3-1             NaN  NaN  \n",
       "S2213158213000582-1469_T1-2         [4339]  NaN  \n",
       "S0032063313003218-6651_T3-1            NaN  NaN  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def process_annotation_set(annot_set):\n",
    "\n",
    "    annot_set_processed = []\n",
    "\n",
    "    annot_set['comboIds'] = annot_set[['docId','annotId']].agg('_'.join, axis=1)\n",
    "    annot_set.set_index('comboIds',inplace=True)\n",
    "\n",
    "    for comboId in list(annot_set.index):\n",
    "        \n",
    "        docId = annot_set.loc[comboId]['docId']\n",
    "        annotId = annot_set.loc[comboId]['annotId']\n",
    "\n",
    "        annotType = annot_set.loc[comboId]['annotType']\n",
    "        annotSpan = [annot_set.loc[comboId]['startOffset'],annot_set.loc[comboId]['endOffset']]\n",
    "\n",
    "        ent_annot_processed = {\n",
    "            'comboId':comboId,\n",
    "            'docId':docId,\n",
    "            'annotId':annotId,\n",
    "            'annotType':annotType,\n",
    "            'annotSpan':annotSpan,\n",
    "            'subSpanType':np.nan,\n",
    "            'linkId':np.nan,\n",
    "            'linkSpan':np.nan,\n",
    "            'subSpan':np.nan,\n",
    "            'unit':np.nan,\n",
    "            'unitEncoded':np.nan,\n",
    "            'misc':np.nan\n",
    "        }\n",
    "        \n",
    "        other = annot_set.loc[comboId]['other']\n",
    "        if isinstance(other,str):\n",
    "            otherDict = json.loads(str(other))\n",
    "\n",
    "            if annot_set.loc[comboId]['annotType'] != 'Quantity':\n",
    "\n",
    "                ent_annot_processed['subSpanType'] = list(otherDict.keys())[0]\n",
    "                link = list(otherDict.values())[0]\n",
    "\n",
    "                ent_annot_processed['linkId'] = link\n",
    "                linkIdx = docId+'_'+link\n",
    "                linkSpan = [int(annot_set.loc[linkIdx]['startOffset']),int(annot_set.loc[linkIdx]['endOffset'])]\n",
    "                ent_annot_processed['linkSpan'] = linkSpan\n",
    "\n",
    "                spanEnds = annotSpan + linkSpan\n",
    "                ent_annot_processed['subSpan'] = [max(spanEnds),max(spanEnds)]\n",
    "\n",
    "            elif 'unit' in list(otherDict.keys()):\n",
    "                unit = otherDict['unit']\n",
    "                ent_annot_processed['unit'] = unit\n",
    "                ent_annot_processed['unitEncoded'] = tokenizer.encode(unit)[1:-1]\n",
    "            else:\n",
    "                ent_annot_processed['misc'] = otherDict\n",
    "\n",
    "\n",
    "        annot_set_processed.append(ent_annot_processed)\n",
    "   \n",
    "    return pd.DataFrame.from_dict(annot_set_processed).set_index('comboId')\n",
    "\n",
    "combo_annot_processed = process_annotation_set(combo_annot)\n",
    "combo_annot_processed.to_csv(interimpath+'combo_annot_processed.csv')\n",
    "combo_annot_processed.sample(7)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "### insert special tokens for subspans (Sam)\n",
    "# will make docs longer\n",
    "\n",
    "# def char_map(doc_annot, task_map)\n",
    "\n",
    "\n",
    "def tokenize_and_align_labels(\n",
    "                                doc_list=combo_docs,\n",
    "                                txt=combo_txt,\n",
    "                                processed_annotation=combo_annot_processed,\n",
    "                                tokenizer=tokenizer,\n",
    "                                taskLabelMap=task_map\n",
    "                            ):\n",
    "\n",
    "    toks_with_labels = []\n",
    "    special_ids = tokenizer.all_special_ids\n",
    "\n",
    "    for doc in doc_list:\n",
    "        # print(doc)\n",
    "        # print(processed_annotation.loc[processed_annotation['docId'] == doc])\n",
    "        doc_annot = processed_annotation.loc[processed_annotation['docId'] == doc]\n",
    "        doc_annot.set_index('annotId',inplace=True)\n",
    "        # print(doc_annot)\n",
    "\n",
    "        encoded_txt = tokenizer(txt[doc], padding='max_length', max_length=512, truncation=True)\n",
    "        encoded_tokens = encoded_txt['input_ids']\n",
    "        # print(encoded_tokens)\n",
    "\n",
    "        ############### Label Primary Spans ###############\n",
    "\n",
    "        labelIds = np.full(len(encoded_tokens),-1)\n",
    "        taskCharMap = {} # \n",
    "        taskCharList = []\n",
    "        taskAnnotIdCharMap = {} # to check for token collision\n",
    "        \n",
    "        for task in list(taskLabelMap.keys()):\n",
    "            #print(task)\n",
    "            annotId = doc_annot.loc[doc_annot['annotType']==task].index\n",
    "            # print(annotId)\n",
    "            spans = list(doc_annot.loc[doc_annot['annotType']==task]['annotSpan'])\n",
    "            # print(spans)\n",
    "            for span in spans:\n",
    "                # print(span)\n",
    "                span = list(range(span[0],span[-1]))\n",
    "                # print(span)\n",
    "                for spanCharIdx in span:\n",
    "                    # print(spanCharIdx)\n",
    "                    taskCharMap[spanCharIdx] = taskLabelMap[task]\n",
    "                # print(taskCharMap)\n",
    "                    # taskAnnotIdCharMap[spanCharIdx] = annotId\n",
    "\n",
    "        decoded = [''] * len(encoded_tokens)\n",
    "        for tokenIdx, token in enumerate(encoded_tokens):\n",
    "            \n",
    "            if token not in special_ids:\n",
    "                tokenCharStart = encoded_txt.token_to_chars(tokenIdx).start\n",
    "                if tokenCharStart in list(taskCharMap.keys()):\n",
    "                    labelIds[tokenIdx] = taskCharMap[tokenCharStart]\n",
    "                    decoded[tokenIdx] = tokenizer.decode(token)\n",
    "                else:\n",
    "                    labelIds[tokenIdx] = 0\n",
    "            else:\n",
    "                labelIds[tokenIdx] = 0\n",
    "        \n",
    "\n",
    "        ############### Sub Spans Token Insertion and labeling ###############\n",
    "\n",
    "        encoded_txt['doc_or_sent_id'] = doc\n",
    "        encoded_txt['labels'] = labelIds\n",
    "        \n",
    "        toks_with_labels.append(encoded_txt)\n",
    "    \n",
    "    # return toks_with_labels\n",
    "    return pd.DataFrame.from_dict(toks_with_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "################# TOKENIZE #################\n",
    "\n",
    "stage1_train_ds = tokenize_and_align_labels(\n",
    "    doc_list=train_docs,\n",
    "    txt=combo_txt,\n",
    "    processed_annotation=combo_annot_processed,\n",
    "    tokenizer=tokenizer,\n",
    "    taskLabelMap=task_map)\n",
    "# stage1_train_ds.to_csv(interimpath+'stage1_train_ds.csv')\n",
    "stage1_n_train = stage1_train_ds.shape[0]\n",
    "\n",
    "\n",
    "stage1_dev_ds = tokenize_and_align_labels(\n",
    "    doc_list=dev_docs,\n",
    "    txt=combo_txt,\n",
    "    processed_annotation=combo_annot_processed,\n",
    "    tokenizer=tokenizer,\n",
    "    taskLabelMap=task_map)\n",
    "# stage1_dev_ds.to_csv(interimpath+'stage1_dev_ds.csv')\n",
    "stage1_n_dev = stage1_dev_ds.shape[0]\n",
    "\n",
    "stage1_test_ds = tokenize_and_align_labels(\n",
    "    doc_list=test_docs,\n",
    "    txt=combo_txt,\n",
    "    processed_annotation=combo_annot_processed,\n",
    "    tokenizer=tokenizer,\n",
    "    taskLabelMap=task_map)\n",
    "# stage1_test_ds.to_csv(interimpath+'stage1_test_ds.csv')\n",
    "stage1_n_test = stage1_test_ds.shape[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "314"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stage1_n_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Matt\n",
    "# def shorten_txt_encoding(txt, shorten_by : int):       \n",
    "#     pass...\n",
    "\n",
    "# generate a list of docIds that have token collision after shortening\n",
    "\n",
    "# toks = list(stage1_dev_ds.sample(1)['input_ids'])\n",
    "\n",
    "# print(toks[0])\n",
    "\n",
    "# tokenizer.decode(toks[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batchify(tokenized_dataset, batch_size, device):\n",
    "    num_examples = int(tokenized_dataset.shape[0] / batch_size)\n",
    "    batch_sizes = [batch_size for x in range(num_examples)]\n",
    "    last_batch_size = tokenized_dataset.shape[0] % batch_size\n",
    "    if last_batch_size:\n",
    "        batch_sizes.append(last_batch_size)\n",
    "    # print(batch_sizes)\n",
    "\n",
    "    batched_dataset = []\n",
    "\n",
    "    idf_to_torch = lambda df : torch.tensor(np.array([list(map(int,r)) for r in df])).to(device)\n",
    "\n",
    "    for idx, size in enumerate(batch_sizes):\n",
    "        start = sum(batch_sizes[:idx])\n",
    "        end = sum(batch_sizes[:idx]) + size - 1\n",
    "        # print(start,end,idx)\n",
    "        input_ids = idf_to_torch(tokenized_dataset['input_ids'].loc[start:end])\n",
    "        attention_mask = idf_to_torch(tokenized_dataset['attention_mask'].loc[start:end])\n",
    "        labels = idf_to_torch(tokenized_dataset['labels'].loc[start:end])\n",
    "        \n",
    "        # doc_or_sent_id = list(tokenized_dataset['doc_or_sent_id'].loc[start:end])\n",
    "        \n",
    "        batch = {\n",
    "            'input_ids':input_ids,\n",
    "            'labels':labels,\n",
    "            'attention_mask':attention_mask,\n",
    "            # 'doc_or_sent_id':doc_or_sent_id\n",
    "\n",
    "        }\n",
    "        \n",
    "        batched_dataset.append(batch)\n",
    "\n",
    "    return batched_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "################# batchify ####################\n",
    "\n",
    "batched_train_ds = batchify(stage1_train_ds[['attention_mask','input_ids','labels']], batch_size, device)\n",
    "batched_dev_ds = batchify(stage1_dev_ds[['attention_mask','input_ids','labels']], batch_size, device)\n",
    "batched_test_ds = batchify(stage1_test_ds[['attention_mask','input_ids','labels']], batch_size, device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Quantity': 1}\n"
     ]
    }
   ],
   "source": [
    "demo_batch = 2\n",
    "\n",
    "demo_batch = batched_train_ds[demo_batch]\n",
    "\n",
    "demo_ids = demo_batch['input_ids'].cpu().numpy()[0]\n",
    "demo_tokens = tokenizer.decode(demo_batch['input_ids'].cpu().numpy()[0])\n",
    "demo_labels = demo_batch['labels'].cpu().numpy()[0]\n",
    "demo_mask = demo_batch['attention_mask'].cpu().numpy()[0]\n",
    "\n",
    "labeled_tokens = ''\n",
    "for id, lab in zip(demo_ids, demo_labels):\n",
    "    if lab:\n",
    "        labeled_tokens = labeled_tokens + tokenizer.decode(id) + ' '\n",
    "\n",
    "print(task_map)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[    0 44791  2383 34098  7208    36  8756   597    43 34003    32    10\n",
      "  2849    12  4684     9 35443  9281  7823    61   311   372  4198    13\n",
      "  1123  3521     8 10875   528     7    49   239  4084   443     6   614\n",
      "  7208 16522     6     8  8859   868 12628   181  1688  1737   646   246\n",
      "  8174 11757   597  3183    32  2333  1490    62    31  4204 41985    50\n",
      " 28255 30987  4462    30  6523  3104   268     7  4960   155   495  3112\n",
      " 32480    19     5  9285     9 32426  2192  6272    31 14926  6884 24477\n",
      "     7 10969  1517 24477   976     4  3646   453   624    42 11757   597\n",
      "   284    33  4824 13113  6608   239   289   176  5814   368 24802 23549\n",
      "    36 41324    23  8930 23982  3971     6  3700    23  6791   229    43\n",
      "   646   306   742    19    10   638     9 49447  1549   885    90   207\n",
      "   746 33646  2148  6373    11   234   791    12  1866   646   245   742\n",
      "     8 11757   597    12  2619   646   401  8174   635     6   209   239\n",
      " 33646 23549  1874  8617    19  2284  5181     6     8  4634  4146    16\n",
      "    10  7708  1468     4   345    16  4634  1989  9723    15 17775  3009\n",
      "     5 11324   227 11757   597  4452     8  5814   368  5134   289   176\n",
      " 20237     6     8     5 10614     9  2167 17014 11324     8  3611     9\n",
      " 20038   624 18687   980  3372    41   505 18670    13     5   709     9\n",
      "   357  3183    14   189   483   201     7  1743     9  7708   304     4\n",
      "    96 20379 41987 10477 25871 22870    36   487  6153    43    23   874\n",
      "   158   229    34    57   341  1433     7  3094     5  3237     9   211\n",
      "   176   624    10   367   275    12 24170 13286 11757   597  3183 22690\n",
      "  4924  4204  3091   646   406  2383  1092  8174    85    34    57   303\n",
      "    14   211   176    64 23379  2024     7 11042  3091    15  4204  7872\n",
      "     6     8    14     5  5814   368  5134   211   176 20237    33 22481\n",
      " 31207  1635 10451     7    14     7   211   176    11     5  2705   194\n",
      "     4  1216  3218    33  1286 21991  9825 23437    13    49  6373   239\n",
      "  1123  5814   368 24802 23549     4  1624    34  4634  2061 26014    15\n",
      " 11757 34417    19   239   289   176 33646 23549     6   150  3183  2018\n",
      "   182   614   289   176 33646     8    73   368 14518  1950 13662  4204\n",
      "  7872    32   747  8266    13    42   892     4  9068     6   335    15\n",
      " 17014 11324   624   167   614    12 29809  5113 11757   597  1743    16\n",
      "  4378 12622     6    53    64   202   492   505 25402   414     8   801\n",
      "  2969    13     5  7757  1521     8 17775  3258     9 18303  3521  3183\n",
      "     4     2     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1]\n"
     ]
    }
   ],
   "source": [
    "print(demo_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>Metal–organic framework (MOF) complexes are a sub-class of porous solids which show great promise for gas storage and separation due to their high surface area, low framework density, and tuneable functional pore environment [3]. MOF materials are usually built up from metal ions or clusters bridged by organic linkers to afford 3D extended frameworks with the formation of cavities ranging from microporous to mesoporous region. Several members within this MOF family have achieved impressively high H2 adsorption capacities (albeit at cryogenic temperatures, typically at 77 K) [4] with a record of ∼16 wt% total uptake capacity observed in NU-100 [5] and MOF-200 [6]. However, these high uptake capacities drop dramatically with increasing temperature, and thus none is a practical material. There is thus particular emphasis on optimising the interactions between MOF hosts and adsorbed H2 molecules, and the identification of specific binding interactions and properties of gases within confined space represents an important methodology for the development of better materials that may lead us to systems of practical use. In situ neutron powder diffraction (NPD) at below 10 K has been used previously to determine the locations of D2 within a few best-behaving MOF materials incorporating exposed metal sites [7–12]. It has been found that D2 can bind directly to vacant sites on metal centres, and that the adsorbed D2 molecules have molecular separations comparable to that to D2 in the solid state. These studies have provided invaluable structural rationale for their observed high gas adsorption capacities. Research has thus focused understandably on MOFs with high H2 uptake capacities, while materials showing very low H2 uptake and/or incorporate fully coordinated metal centres are often ignored for this study. Therefore, information on binding interactions within those low-uptake MOF systems is entirely lacking, but can still give important complementary data and potential understanding for the subsequent design and optimisation of hydrogen storage materials.</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n"
     ]
    }
   ],
   "source": [
    "print(demo_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "print(demo_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 77  K  ∼ 16  w t %  below  10  K \n"
     ]
    }
   ],
   "source": [
    "print(labeled_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at allenai/biomed_roberta_base were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "class Stage1model(nn.Module):\n",
    "    def __init__(self, dropout=0.1):\n",
    "        super(Stage1model, self).__init__()\n",
    "        self.mod = RobertaModel.from_pretrained(\n",
    "                    model_name,\n",
    "                    num_labels=num_classes+1,\n",
    "                    hidden_dropout_prob=dropout,\n",
    "                    output_hidden_states=True)\n",
    "        # self.norm = nn.BatchNorm1d(512, eps=self.mod.config.layer_norm_eps)\n",
    "        self.drop = nn.Dropout(self.mod.config.hidden_dropout_prob)\n",
    "        self.classifier = nn.Linear(self.mod.config.hidden_size, num_classes+1)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        \n",
    "        output = self.mod(input_ids=input_ids,attention_mask=attention_mask)\n",
    "        \n",
    "        y_hat = output.hidden_states[-1]\n",
    "\n",
    "        # y_hat = self.norm(y_hat)\n",
    "\n",
    "        y_hat = self.drop(y_hat)\n",
    "\n",
    "        y_hat = self.classifier(y_hat).permute(0,2,1)\n",
    "        \n",
    "        return y_hat\n",
    "\n",
    "model = Stage1model().to(device)\n",
    "\n",
    "model_new = model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Stage1model(\n",
       "  (mod): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): RobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): RobertaPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (drop): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# class OurBERTModel(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(OurBERTModel, self).__init__()\n",
    "#         self.mod = AutoModel.from_pretrained(model_name, num_labels=num_classes+1)\n",
    "#         self.drop = nn.Dropout(self.mod.config.hidden_dropout_prob)\n",
    "#         self.classifier = nn.Linear(self.mod.config.hidden_size, num_classes+1)\n",
    "\n",
    "#     def forward(self, text, att_mask):\n",
    "#         b, num_tokens = text.shape\n",
    "#         token_type = torch.zeros((b, num_tokens), dtype=torch.long).to(device)\n",
    "#         outputs = self.mod(text, attention_mask=att_mask, token_type_ids=token_type)\n",
    "#         return self.classifier(self.drop(outputs['last_hidden_state']))\n",
    "\n",
    "# model = OurBERTModel().to(device)\n",
    "\n",
    "# model_old = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# demo_ypred = model(demo_batch['input_ids'], demo_batch['attention_mask'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.CrossEntropyLoss(reduction='sum')\n",
    "optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "n_training_steps = n_epochs * len(batched_train_ds)\n",
    "lr_scheduler = get_scheduler(\n",
    "    name=\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=n_training_steps\n",
    ")\n",
    "\n",
    "\n",
    "def train_epoch(ds, criterion):\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    ypred = []\n",
    "    ytrue = []\n",
    "    for idx, batch in enumerate(ds):\n",
    "\n",
    "        labels = batch['labels']\n",
    "        input_ids = batch['input_ids']\n",
    "        attention_mask = batch['attention_mask']\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        logits = model(input_ids, attention_mask)\n",
    "        \n",
    "        loss = criterion(logits, labels)\n",
    "        # loss = (loss * attention_mask).sum() / (attention_mask).sum()\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "\n",
    "        progress_bar.update(1)\n",
    "            \n",
    "    return loss.item()\n",
    "\n",
    "\n",
    "\n",
    "def eval_epoch(ds, criterion):\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    ypred = []\n",
    "    ytrue = []\n",
    "    with torch.no_grad():\n",
    "        for idx, batch in enumerate(ds):\n",
    "\n",
    "            labels = batch['labels']\n",
    "            input_ids = batch['input_ids']\n",
    "            attention_mask = batch['attention_mask']\n",
    "\n",
    "            logits = model(input_ids, attention_mask)\n",
    "            \n",
    "            loss = criterion(logits, labels)\n",
    "            # loss = (loss * attention_mask).sum() / (attention_mask).sum()\n",
    "\n",
    "            for dlogits, dlabels in zip(logits, labels):\n",
    "                    for tlogits, tlabels in zip(dlogits, dlabels):\n",
    "                        ypred.append(tlogits.argmax().item())\n",
    "                        ytrue.append(tlabels.item())\n",
    "\n",
    "            progress_bar.update(1)\n",
    "\n",
    "    acc = metrics.accuracy_score(ytrue,ypred)\n",
    "    report = classification_report(ytrue,ypred,\n",
    "                                    labels=list(task_map.values()),\n",
    "                                    target_names=list(task_map.keys()),\n",
    "                                    output_dict=True,\n",
    "                                    zero_division=0)\n",
    "\n",
    "                                    \n",
    "\n",
    "    return loss.item(), acc, report, ytrue, ypred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c20191de057a45628c755eba4335fd1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/920 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============ Begin Epoch 1 ============\n",
      "Train loss: 11.881114959716797\n",
      "Eval on train set loss: 11.02413558959961   accuracy: 0.006369426751592357\n",
      "Eval on dev set loss: 19.690397262573242   accuracy: 0.022222222222222223\n",
      "============ Begin Epoch 2 ============\n",
      "Train loss: 7.627496719360352\n",
      "Eval on train set loss: 11.794892311096191   accuracy: 0.0031847133757961785\n",
      "Eval on dev set loss: 14.194269180297852   accuracy: 0.016666666666666666\n",
      "============ Begin Epoch 3 ============\n",
      "Train loss: 2.078975200653076\n",
      "Eval on train set loss: 1.855098843574524   accuracy: 0.004777070063694267\n",
      "Eval on dev set loss: 18.64334487915039   accuracy: 0.016666666666666666\n",
      "============ Begin Epoch 4 ============\n",
      "Train loss: 1.509434700012207\n",
      "Eval on train set loss: 0.8831247091293335   accuracy: 0.006369426751592357\n",
      "Eval on dev set loss: 26.66785430908203   accuracy: 0.027777777777777776\n",
      "============ Begin Epoch 5 ============\n",
      "Train loss: 2.530911922454834\n",
      "Eval on train set loss: 0.9968278408050537   accuracy: 0.011146496815286623\n",
      "Eval on dev set loss: 26.220239639282227   accuracy: 0.027777777777777776\n",
      "============ Begin Epoch 6 ============\n",
      "Train loss: 0.4027114808559418\n",
      "Eval on train set loss: 0.27416348457336426   accuracy: 0.014331210191082803\n",
      "Eval on dev set loss: 27.865928649902344   accuracy: 0.027777777777777776\n",
      "============ Begin Epoch 7 ============\n",
      "Train loss: 0.08343545347452164\n",
      "Eval on train set loss: 0.14487963914871216   accuracy: 0.009554140127388535\n",
      "Eval on dev set loss: 25.66779136657715   accuracy: 0.03333333333333333\n",
      "============ Begin Epoch 8 ============\n",
      "Train loss: 0.404758095741272\n",
      "Eval on train set loss: 0.23781339824199677   accuracy: 0.007961783439490446\n",
      "Eval on dev set loss: 33.8847770690918   accuracy: 0.03333333333333333\n",
      "============ Begin Epoch 9 ============\n",
      "Train loss: 0.041185688227415085\n",
      "Eval on train set loss: 0.022352103143930435   accuracy: 0.006369426751592357\n",
      "Eval on dev set loss: 33.01164627075195   accuracy: 0.03333333333333333\n",
      "============ Begin Epoch 10 ============\n",
      "Train loss: 0.02873777598142624\n",
      "Eval on train set loss: 0.013863665983080864   accuracy: 0.006369426751592357\n",
      "Eval on dev set loss: 30.046199798583984   accuracy: 0.03333333333333333\n"
     ]
    }
   ],
   "source": [
    "run_report = {  'epoch':[],\n",
    "                'train_loss':[],\n",
    "                'eval_train_loss':[],\n",
    "                'eval_train_acc':[],\n",
    "                'eval_train_ytrue':[],\n",
    "                'eval_train_ypred':[],\n",
    "                'eval_train_rpt':[],\n",
    "                'eval_dev_loss':[],\n",
    "                'eval_dev_acc':[],\n",
    "                'eval_dev_ytrue':[],\n",
    "                'eval_dev_ypred':[],\n",
    "                'eval_dev_rpt':[],\n",
    "             }\n",
    "\n",
    "num_total_steps = n_epochs * (len(batched_train_ds) * 2 + len(batched_dev_ds))\n",
    "progress_bar = tqdm(range(num_total_steps))\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    run_report['epoch'].append(epoch)\n",
    "    \n",
    "    print(f\"============ Begin Epoch {epoch+1} ============\")\n",
    "\n",
    "    loss = train_epoch(batched_train_ds, criterion)\n",
    "    print(f\"Train loss: {loss}\")\n",
    "    run_report['train_loss'].append(loss)\n",
    "    \n",
    "    output = eval_epoch(batched_train_ds, criterion)\n",
    "    (loss, acc, report, ytrue, ypred) = output\n",
    "    print(f'Eval on train set loss: {loss}   accuracy: {acc}')\n",
    "    run_report['eval_train_loss'].append(loss)\n",
    "    run_report['eval_train_acc'].append(acc)\n",
    "    run_report['eval_train_ytrue'].append(ytrue)\n",
    "    run_report['eval_train_ypred'].append(ypred)\n",
    "    run_report['eval_train_rpt'].append(report)\n",
    "\n",
    "    output = eval_epoch(batched_dev_ds, criterion)\n",
    "    (loss, acc, report, ytrue, ypred) = output\n",
    "    print(f'Eval on dev set loss: {loss}   accuracy: {acc}')\n",
    "    run_report['eval_dev_loss'].append(loss)\n",
    "    run_report['eval_dev_acc'].append(acc)\n",
    "    run_report['eval_dev_ytrue'].append(ytrue)\n",
    "    run_report['eval_dev_ypred'].append(ypred)\n",
    "    run_report['eval_dev_rpt'].append(report)\n",
    "    \n",
    "\n",
    "# run_report = pd.DataFrame.from_dict(run_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### todo: save reports and results to files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "########################## loss plot #######################\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "x = np.array(range(len(run_report['epoch'])))\n",
    "y = np.array(run_report['eval_dev_loss'])\n",
    "plt.plot(x,y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "########################## accuracy plot #######################\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "x = np.array(range(len(run_report['epoch'])))\n",
    "y = np.array(run_report['eval_dev_acc'])\n",
    "plt.plot(x,y)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 ('test')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b5060d9997a95c2acb3a42af5d14caeb5dba3e5b7e20123b9f235f707614ce30"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
