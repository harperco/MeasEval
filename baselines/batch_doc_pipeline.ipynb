{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from torch import nn\n",
    "from torch.optim import AdamW, Adam\n",
    "from transformers import AutoModel, AutoTokenizer, AutoModelForTokenClassification, AutoModelForSequenceClassification\n",
    "from transformers import RobertaForSequenceClassification\n",
    "from transformers import RobertaModel\n",
    "from transformers import get_scheduler\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn import metrics as metrics\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "import re\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "random.seed(42)\n",
    "reprocess_raw =  False\n",
    "\n",
    "batch_size = 10 # documents\n",
    "learning_rate = 5e-5\n",
    "n_epochs = 10\n",
    "\n",
    "# task_map = {'Quantity':1}\n",
    "task_map = {'Quantity':1,'MeasuredProperty':2,'MeasuredEntity':3,'Qualifier':4} # uncomment for multi-class\n",
    "num_classes = len(task_map)\n",
    "\n",
    "model_name = 'allenai/biomed_roberta_base'\n",
    "# model_name = 'bert-base-cased'\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "# device = 'cpu' # uncomment this to make debugging easier\n",
    "\n",
    "data_size_reduce = 1 # multiplier for making small datasets\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "currentdir = os.getcwd() # ~/MeasEval/baselines\n",
    "\n",
    "combopath_txt = os.path.join(currentdir, \"../data/raw/combo/text/\")\n",
    "combopath_annot = os.path.join(currentdir, \"../data/raw/combo/tsv/\")\n",
    "\n",
    "interimpath = os.path.join(currentdir, \"../data/interim/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_raw_txt(docs):\n",
    "    processesd_txt = {}\n",
    "    remove_markers = True\n",
    "\n",
    "    cnt_toks = {\"figs.\": 0, \"fig.\": 0, \"et al.\": 0,\n",
    "            \"ref.\": 0, \"eq.\": 0, \"e.g.\": 0,\n",
    "            \"i.e.\": 0, \"nos.\": 0, \"no.\": 0,\n",
    "            \"spp.\": 0\n",
    "            }\n",
    "    regex_end_checker = [\".*[a-zA-Z]figs\\.$\", \n",
    "                        \".*[a-zA-Z]fig\\.$\",\n",
    "                        \".*[a-zA-Z]et al\\.$\",\n",
    "                        \".*[a-zA-Z]ref\\.$\",\n",
    "                        \".*[a-zA-Z]eq\\.$\",\n",
    "                        \".*[a-zA-Z]e\\.g\\.$\",\n",
    "                        \".*[a-zA-Z]i\\.e\\.$\",\n",
    "                        \".*[a-zA-Z]nos\\.$\",\n",
    "                        \".*[a-zA-Z]no\\.$\",\n",
    "                        \".*[a-zA-Z]spp\\.$\",\n",
    "                        # figs., fig., et al., Ref., Eq., e.g., i.e., Nos., No., spp.\n",
    "                    ]\n",
    "\n",
    "    assert len(cnt_toks) == len(regex_end_checker)\n",
    "\n",
    "    for docId, doc in docs.items():\n",
    "        flag = False\n",
    "        sentences = sent_tokenize(doc)\n",
    "\n",
    "        fixed_sentence_tokens = []\n",
    "        curr_len = 0\n",
    "        for s in sentences:\n",
    "            if flag == True:\n",
    "                assert s[0] != ' '\n",
    "                white_length = doc[curr_len:].find(s[0])\n",
    "\n",
    "                prev_len = len(fixed_sentence_tokens[-1])\n",
    "                fixed_sentence_tokens[-1] = fixed_sentence_tokens[-1] + (\" \"*white_length) + s\n",
    "\n",
    "                assert fixed_sentence_tokens[-1][prev_len+white_length] == doc[curr_len+white_length], (fixed_sentence_tokens[-1], doc, curr_len, tmp_this_sent_len)\n",
    "                tmp_this_sent_len = white_length + len(s)\n",
    "                assert fixed_sentence_tokens[-1][-1] == doc[curr_len+tmp_this_sent_len-1], (fixed_sentence_tokens[-1], doc, curr_len, tmp_this_sent_len)\n",
    "                curr_len += tmp_this_sent_len\n",
    "            else:\n",
    "                if len(fixed_sentence_tokens) != 0:\n",
    "                    assert s[0] != ' '\n",
    "                    white_length = doc[curr_len:].find(s[0])\n",
    "                    fixed_sentence_tokens.append( (\" \"*white_length) + s )\n",
    "                else:\n",
    "                    fixed_sentence_tokens.append(s)\n",
    "                assert fixed_sentence_tokens[-1][0] == doc[curr_len], (fixed_sentence_tokens, doc, curr_len, tmp_this_sent_len)\n",
    "                tmp_this_sent_len = len(fixed_sentence_tokens[-1])\n",
    "                assert fixed_sentence_tokens[-1][-1] == doc[curr_len+tmp_this_sent_len-1], (fixed_sentence_tokens[-1], doc, curr_len, tmp_this_sent_len)\n",
    "                curr_len += tmp_this_sent_len\n",
    "\n",
    "            lower_cased_s = fixed_sentence_tokens[-1].lower()\n",
    "            flag = False\n",
    "            if remove_markers:\n",
    "                for i, k in enumerate(cnt_toks):\n",
    "                    this_regex_pattern = regex_end_checker[i]\n",
    "                    if lower_cased_s.endswith(k) and re.match(this_regex_pattern, lower_cased_s) == None:\n",
    "                        cnt_toks[k] += 1\n",
    "                        flag = True\n",
    "                        break\n",
    "\n",
    "        processesd_txt[docId] = ''.join(fixed_sentence_tokens)\n",
    "    return processesd_txt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(reprocess_raw = False):\n",
    "\n",
    "    if reprocess_raw == True:\n",
    "        docIds = []\n",
    "        combo_txt = {}\n",
    "        for fn in os.listdir(combopath_txt):\n",
    "            docIds.append(fn[:-4])\n",
    "            path = combopath_txt+fn\n",
    "            with open(path) as textfile:\n",
    "                    text = textfile.read()\n",
    "                    #[:-4] strips off the .txt to get the id\n",
    "                    combo_txt[fn[:-4]] = text\n",
    "\n",
    "        combo_annot = pd.DataFrame()\n",
    "        for fn in os.listdir(combopath_annot):\n",
    "            path = combopath_annot+fn\n",
    "            file = pd.read_csv(path,delimiter='\\t',encoding='utf-8')\n",
    "            combo_annot = pd.concat([combo_annot, file],ignore_index=True)\n",
    "\n",
    "        combo_txt = process_raw_txt(combo_txt)\n",
    "        assert docIds == list(combo_txt.keys()), (len(docIds), len(list(combo_txt.keys())))\n",
    "\n",
    "        with open(interimpath+'combo_txt.json','w') as f:\n",
    "            json.dump(combo_txt, f)\n",
    "\n",
    "        combo_annot.to_csv(interimpath+'combo_annot.csv')\n",
    "\n",
    "        return docIds, combo_txt, combo_annot\n",
    "    else:\n",
    "        combo_annot = pd.read_csv(interimpath+'combo_annot.csv')\n",
    "\n",
    "        with open(interimpath+'combo_txt.json','r') as f:\n",
    "            combo_txt = json.load(f)\n",
    "\n",
    "        docIds = list(combo_txt.keys())\n",
    "    \n",
    "        return docIds, combo_txt, combo_annot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "combo_docs, combo_txt, combo_annot = read_data(reprocess_raw = reprocess_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### train/dev/test split options\n",
    "\n",
    "random.seed(42)\n",
    "\n",
    "percent_to_test = .1\n",
    "percent_to_dev = .2\n",
    "percent_to_train =  1 - percent_to_dev - percent_to_test\n",
    "\n",
    "n_doc = len(combo_docs)\n",
    "split_train = int(np.round(n_doc * percent_to_train))\n",
    "split_dev = split_train + int(np.round(n_doc * percent_to_dev))\n",
    "\n",
    "train_docs = combo_docs[:split_train]\n",
    "dev_docs = combo_docs[split_train:split_dev]\n",
    "test_docs = combo_docs[split_dev:]\n",
    "\n",
    "train_docs = random.sample(train_docs, int(len(train_docs)*data_size_reduce))\n",
    "dev_docs = random.sample(dev_docs, int(len(dev_docs)*data_size_reduce))\n",
    "test_docs = random.sample(test_docs, int(len(test_docs)*data_size_reduce))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## Tokenizer ###########\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>docId</th>\n",
       "      <th>annotId</th>\n",
       "      <th>annotType</th>\n",
       "      <th>annotSpan</th>\n",
       "      <th>subSpanType</th>\n",
       "      <th>linkId</th>\n",
       "      <th>linkSpan</th>\n",
       "      <th>subSpan</th>\n",
       "      <th>unit</th>\n",
       "      <th>unitEncoded</th>\n",
       "      <th>misc</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>comboId</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>S0006322312001096-1197_T71-6</th>\n",
       "      <td>S0006322312001096-1197</td>\n",
       "      <td>T71-6</td>\n",
       "      <td>MeasuredProperty</td>\n",
       "      <td>[721, 730]</td>\n",
       "      <td>HasQuantity</td>\n",
       "      <td>T21-6</td>\n",
       "      <td>[732, 735]</td>\n",
       "      <td>[735, 735]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>S0019103512004009-5033_T1-6</th>\n",
       "      <td>S0019103512004009-5033</td>\n",
       "      <td>T1-6</td>\n",
       "      <td>Quantity</td>\n",
       "      <td>[549, 555]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>μbar</td>\n",
       "      <td>[47049, 4901]</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>S0167880913001229-1021_T3-5</th>\n",
       "      <td>S0167880913001229-1021</td>\n",
       "      <td>T3-5</td>\n",
       "      <td>MeasuredEntity</td>\n",
       "      <td>[144, 148]</td>\n",
       "      <td>HasQuantity</td>\n",
       "      <td>T1-5</td>\n",
       "      <td>[238, 245]</td>\n",
       "      <td>[245, 245]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>S0019103511004994-1399_T1-3</th>\n",
       "      <td>S0019103511004994-1399</td>\n",
       "      <td>T1-3</td>\n",
       "      <td>Quantity</td>\n",
       "      <td>[120, 129]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>km</td>\n",
       "      <td>[7203]</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>S2213671113000738-667_T4-1</th>\n",
       "      <td>S2213671113000738-667</td>\n",
       "      <td>T4-1</td>\n",
       "      <td>Qualifier</td>\n",
       "      <td>[379, 388]</td>\n",
       "      <td>Qualifies</td>\n",
       "      <td>T3-1</td>\n",
       "      <td>[345, 368]</td>\n",
       "      <td>[388, 388]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>S0019103512004009-3976_T2-1</th>\n",
       "      <td>S0019103512004009-3976</td>\n",
       "      <td>T2-1</td>\n",
       "      <td>MeasuredProperty</td>\n",
       "      <td>[92, 96]</td>\n",
       "      <td>HasQuantity</td>\n",
       "      <td>T1-1</td>\n",
       "      <td>[104, 117]</td>\n",
       "      <td>[117, 117]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>S1388248113001951-339_T3-2</th>\n",
       "      <td>S1388248113001951-339</td>\n",
       "      <td>T3-2</td>\n",
       "      <td>MeasuredProperty</td>\n",
       "      <td>[121, 125]</td>\n",
       "      <td>HasQuantity</td>\n",
       "      <td>T1-2</td>\n",
       "      <td>[129, 139]</td>\n",
       "      <td>[139, 139]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               docId annotId  \\\n",
       "comboId                                                        \n",
       "S0006322312001096-1197_T71-6  S0006322312001096-1197   T71-6   \n",
       "S0019103512004009-5033_T1-6   S0019103512004009-5033    T1-6   \n",
       "S0167880913001229-1021_T3-5   S0167880913001229-1021    T3-5   \n",
       "S0019103511004994-1399_T1-3   S0019103511004994-1399    T1-3   \n",
       "S2213671113000738-667_T4-1     S2213671113000738-667    T4-1   \n",
       "S0019103512004009-3976_T2-1   S0019103512004009-3976    T2-1   \n",
       "S1388248113001951-339_T3-2     S1388248113001951-339    T3-2   \n",
       "\n",
       "                                     annotType   annotSpan  subSpanType  \\\n",
       "comboId                                                                   \n",
       "S0006322312001096-1197_T71-6  MeasuredProperty  [721, 730]  HasQuantity   \n",
       "S0019103512004009-5033_T1-6           Quantity  [549, 555]          NaN   \n",
       "S0167880913001229-1021_T3-5     MeasuredEntity  [144, 148]  HasQuantity   \n",
       "S0019103511004994-1399_T1-3           Quantity  [120, 129]          NaN   \n",
       "S2213671113000738-667_T4-1           Qualifier  [379, 388]    Qualifies   \n",
       "S0019103512004009-3976_T2-1   MeasuredProperty    [92, 96]  HasQuantity   \n",
       "S1388248113001951-339_T3-2    MeasuredProperty  [121, 125]  HasQuantity   \n",
       "\n",
       "                             linkId    linkSpan     subSpan  unit  \\\n",
       "comboId                                                             \n",
       "S0006322312001096-1197_T71-6  T21-6  [732, 735]  [735, 735]   NaN   \n",
       "S0019103512004009-5033_T1-6     NaN         NaN         NaN  μbar   \n",
       "S0167880913001229-1021_T3-5    T1-5  [238, 245]  [245, 245]   NaN   \n",
       "S0019103511004994-1399_T1-3     NaN         NaN         NaN    km   \n",
       "S2213671113000738-667_T4-1     T3-1  [345, 368]  [388, 388]   NaN   \n",
       "S0019103512004009-3976_T2-1    T1-1  [104, 117]  [117, 117]   NaN   \n",
       "S1388248113001951-339_T3-2     T1-2  [129, 139]  [139, 139]   NaN   \n",
       "\n",
       "                                unitEncoded misc  \n",
       "comboId                                           \n",
       "S0006322312001096-1197_T71-6            NaN  NaN  \n",
       "S0019103512004009-5033_T1-6   [47049, 4901]  NaN  \n",
       "S0167880913001229-1021_T3-5             NaN  NaN  \n",
       "S0019103511004994-1399_T1-3          [7203]  NaN  \n",
       "S2213671113000738-667_T4-1              NaN  NaN  \n",
       "S0019103512004009-3976_T2-1             NaN  NaN  \n",
       "S1388248113001951-339_T3-2              NaN  NaN  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def process_annotation_set(annot_set):\n",
    "\n",
    "    annot_set_processed = []\n",
    "\n",
    "    annot_set['comboIds'] = annot_set[['docId','annotId']].agg('_'.join, axis=1)\n",
    "    annot_set.set_index('comboIds',inplace=True)\n",
    "\n",
    "    for comboId in list(annot_set.index):\n",
    "        \n",
    "        docId = annot_set.loc[comboId]['docId']\n",
    "        annotId = annot_set.loc[comboId]['annotId']\n",
    "\n",
    "        annotType = annot_set.loc[comboId]['annotType']\n",
    "        annotSpan = [annot_set.loc[comboId]['startOffset'],annot_set.loc[comboId]['endOffset']]\n",
    "\n",
    "        ent_annot_processed = {\n",
    "            'comboId':comboId,\n",
    "            'docId':docId,\n",
    "            'annotId':annotId,\n",
    "            'annotType':annotType,\n",
    "            'annotSpan':annotSpan,\n",
    "            'subSpanType':np.nan,\n",
    "            'linkId':np.nan,\n",
    "            'linkSpan':np.nan,\n",
    "            'subSpan':np.nan,\n",
    "            'unit':np.nan,\n",
    "            'unitEncoded':np.nan,\n",
    "            'misc':np.nan\n",
    "        }\n",
    "        \n",
    "        other = annot_set.loc[comboId]['other']\n",
    "        if isinstance(other,str):\n",
    "            otherDict = json.loads(str(other))\n",
    "\n",
    "            if annot_set.loc[comboId]['annotType'] != 'Quantity':\n",
    "\n",
    "                ent_annot_processed['subSpanType'] = list(otherDict.keys())[0]\n",
    "                link = list(otherDict.values())[0]\n",
    "\n",
    "                ent_annot_processed['linkId'] = link\n",
    "                linkIdx = docId+'_'+link\n",
    "                linkSpan = [int(annot_set.loc[linkIdx]['startOffset']),int(annot_set.loc[linkIdx]['endOffset'])]\n",
    "                ent_annot_processed['linkSpan'] = linkSpan\n",
    "\n",
    "                spanEnds = annotSpan + linkSpan\n",
    "                ent_annot_processed['subSpan'] = [max(spanEnds),max(spanEnds)]\n",
    "\n",
    "            elif 'unit' in list(otherDict.keys()):\n",
    "                unit = otherDict['unit']\n",
    "                ent_annot_processed['unit'] = unit\n",
    "                ent_annot_processed['unitEncoded'] = tokenizer.encode(unit)[1:-1]\n",
    "            else:\n",
    "                ent_annot_processed['misc'] = otherDict\n",
    "\n",
    "\n",
    "        annot_set_processed.append(ent_annot_processed)\n",
    "   \n",
    "    return pd.DataFrame.from_dict(annot_set_processed).set_index('comboId')\n",
    "\n",
    "combo_annot_processed = process_annotation_set(combo_annot)\n",
    "combo_annot_processed.to_csv(interimpath+'combo_annot_processed.csv')\n",
    "combo_annot_processed.sample(7)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "### insert special tokens for subspans (Sam)\n",
    "# will make docs longer\n",
    "\n",
    "# def char_map(doc_annot, task_map)\n",
    "\n",
    "\n",
    "def tokenize_and_align_labels(\n",
    "                                doc_list=combo_docs,\n",
    "                                txt=combo_txt,\n",
    "                                processed_annotation=combo_annot_processed,\n",
    "                                tokenizer=tokenizer,\n",
    "                                taskLabelMap=task_map\n",
    "                            ):\n",
    "\n",
    "    toks_with_labels = []\n",
    "    special_ids = tokenizer.all_special_ids\n",
    "\n",
    "    for doc in doc_list:\n",
    "        # print(doc)\n",
    "        # print(processed_annotation.loc[processed_annotation['docId'] == doc])\n",
    "        doc_annot = processed_annotation.loc[processed_annotation['docId'] == doc]\n",
    "        doc_annot.set_index('annotId',inplace=True)\n",
    "        # print(doc_annot)\n",
    "\n",
    "        encoded_txt = tokenizer(txt[doc], padding='max_length', max_length=512, truncation=True)\n",
    "        encoded_tokens = encoded_txt['input_ids']\n",
    "        # print(encoded_tokens)\n",
    "\n",
    "        ############### Label Primary Spans ###############\n",
    "\n",
    "        labelIds = np.full(len(encoded_tokens),-1)\n",
    "        taskCharMap = {} # \n",
    "        taskCharList = []\n",
    "        taskAnnotIdCharMap = {} # to check for token collision\n",
    "        \n",
    "        for task in list(taskLabelMap.keys()):\n",
    "            #print(task)\n",
    "            annotId = doc_annot.loc[doc_annot['annotType']==task].index\n",
    "            # print(annotId)\n",
    "            spans = list(doc_annot.loc[doc_annot['annotType']==task]['annotSpan'])\n",
    "            # print(spans)\n",
    "            for span in spans:\n",
    "                # print(span)\n",
    "                span = list(range(span[0],span[-1]))\n",
    "                # print(span)\n",
    "                for spanCharIdx in span:\n",
    "                    # print(spanCharIdx)\n",
    "                    taskCharMap[spanCharIdx] = taskLabelMap[task]\n",
    "                # print(taskCharMap)\n",
    "                    # taskAnnotIdCharMap[spanCharIdx] = annotId\n",
    "\n",
    "        decoded = [''] * len(encoded_tokens)\n",
    "        for tokenIdx, token in enumerate(encoded_tokens):\n",
    "            \n",
    "            if token not in special_ids:\n",
    "                tokenCharStart = encoded_txt.token_to_chars(tokenIdx).start\n",
    "                if tokenCharStart in list(taskCharMap.keys()):\n",
    "                    labelIds[tokenIdx] = taskCharMap[tokenCharStart]\n",
    "                    decoded[tokenIdx] = tokenizer.decode(token)\n",
    "                else:\n",
    "                    labelIds[tokenIdx] = 0\n",
    "            else:\n",
    "                labelIds[tokenIdx] = 0\n",
    "        \n",
    "\n",
    "        ############### Sub Spans Token Insertion and labeling ###############\n",
    "\n",
    "        encoded_txt['doc_or_sent_id'] = doc\n",
    "        encoded_txt['labels'] = labelIds\n",
    "        \n",
    "        toks_with_labels.append(encoded_txt)\n",
    "    \n",
    "    # return toks_with_labels\n",
    "    return pd.DataFrame.from_dict(toks_with_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "################# TOKENIZE #################\n",
    "\n",
    "stage1_train_ds = tokenize_and_align_labels(\n",
    "    doc_list=train_docs,\n",
    "    txt=combo_txt,\n",
    "    processed_annotation=combo_annot_processed,\n",
    "    tokenizer=tokenizer,\n",
    "    taskLabelMap=task_map)\n",
    "# stage1_train_ds.to_csv(interimpath+'stage1_train_ds.csv')\n",
    "stage1_n_train = stage1_train_ds.shape[0]\n",
    "\n",
    "\n",
    "stage1_dev_ds = tokenize_and_align_labels(\n",
    "    doc_list=dev_docs,\n",
    "    txt=combo_txt,\n",
    "    processed_annotation=combo_annot_processed,\n",
    "    tokenizer=tokenizer,\n",
    "    taskLabelMap=task_map)\n",
    "# stage1_dev_ds.to_csv(interimpath+'stage1_dev_ds.csv')\n",
    "stage1_n_dev = stage1_dev_ds.shape[0]\n",
    "\n",
    "stage1_test_ds = tokenize_and_align_labels(\n",
    "    doc_list=test_docs,\n",
    "    txt=combo_txt,\n",
    "    processed_annotation=combo_annot_processed,\n",
    "    tokenizer=tokenizer,\n",
    "    taskLabelMap=task_map)\n",
    "# stage1_test_ds.to_csv(interimpath+'stage1_test_ds.csv')\n",
    "stage1_n_test = stage1_test_ds.shape[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "314"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stage1_n_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Matt\n",
    "# def shorten_txt_encoding(txt, shorten_by : int):       \n",
    "#     pass...\n",
    "\n",
    "# generate a list of docIds that have token collision after shortening\n",
    "\n",
    "# toks = list(stage1_dev_ds.sample(1)['input_ids'])\n",
    "\n",
    "# print(toks[0])\n",
    "\n",
    "# tokenizer.decode(toks[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batchify(tokenized_dataset, batch_size, device):\n",
    "    num_examples = int(tokenized_dataset.shape[0] / batch_size)\n",
    "    batch_sizes = [batch_size for x in range(num_examples)]\n",
    "    last_batch_size = tokenized_dataset.shape[0] % batch_size\n",
    "    if last_batch_size:\n",
    "        batch_sizes.append(last_batch_size)\n",
    "    # print(batch_sizes)\n",
    "\n",
    "    batched_dataset = []\n",
    "\n",
    "    idf_to_torch = lambda df : torch.tensor(np.array([list(map(int,r)) for r in df])).to(device)\n",
    "\n",
    "    for idx, size in enumerate(batch_sizes):\n",
    "        start = sum(batch_sizes[:idx])\n",
    "        end = sum(batch_sizes[:idx]) + size - 1\n",
    "        # print(start,end,idx)\n",
    "        input_ids = idf_to_torch(tokenized_dataset['input_ids'].loc[start:end])\n",
    "        attention_mask = idf_to_torch(tokenized_dataset['attention_mask'].loc[start:end])\n",
    "        labels = idf_to_torch(tokenized_dataset['labels'].loc[start:end])\n",
    "        \n",
    "        # doc_or_sent_id = list(tokenized_dataset['doc_or_sent_id'].loc[start:end])\n",
    "        \n",
    "        batch = {\n",
    "            'input_ids':input_ids,\n",
    "            'labels':labels,\n",
    "            'attention_mask':attention_mask,\n",
    "            # 'doc_or_sent_id':doc_or_sent_id\n",
    "\n",
    "        }\n",
    "        \n",
    "        batched_dataset.append(batch)\n",
    "\n",
    "    return batched_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "################# batchify ####################\n",
    "\n",
    "batched_train_ds = batchify(stage1_train_ds[['attention_mask','input_ids','labels']], batch_size, device)\n",
    "batched_dev_ds = batchify(stage1_dev_ds[['attention_mask','input_ids','labels']], batch_size, device)\n",
    "batched_test_ds = batchify(stage1_test_ds[['attention_mask','input_ids','labels']], batch_size, device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Quantity': 1, 'MeasuredProperty': 2, 'MeasuredEntity': 3, 'Qualifier': 4}\n"
     ]
    }
   ],
   "source": [
    "demo_batch = 2\n",
    "\n",
    "demo_batch = batched_train_ds[demo_batch]\n",
    "\n",
    "demo_ids = demo_batch['input_ids'].cpu().numpy()[0]\n",
    "demo_tokens = tokenizer.decode(demo_batch['input_ids'].cpu().numpy()[0])\n",
    "demo_labels = demo_batch['labels'].cpu().numpy()[0]\n",
    "demo_mask = demo_batch['attention_mask'].cpu().numpy()[0]\n",
    "\n",
    "labeled_tokens = ''\n",
    "latch_print = False\n",
    "for id, lab in zip(demo_ids, demo_labels):\n",
    "    if lab:\n",
    "        labeled_tokens = labeled_tokens + tokenizer.decode(id) + ' '\n",
    "\n",
    "print(task_map)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[    0  1121   645     7  3094   549     5 18838  7205  1092 31425    67\n",
      " 26914  1022    11 15229     8  6559     9     5 43261 14966   366 38868\n",
      "   739     8   650  2849 46210     6 43261   784  2459  1626    31 43318\n",
      " 19961  1001  3662 13651    36 40747     8   797    43    58 13484  1070\n",
      "    15    16 36557 15557 43164 14170 12003 20676    36   698  2383   541\n",
      "  4234    25    11  8526     4   646  3706 48610   318  6559     9  1169\n",
      "     5   739  2849 19304    50     5  1445 14966   366  4399    21  2132\n",
      "   172     5  3854     9  1736 14966   366 38868 17792    74   464   624\n",
      "     5 43141  4392     4   374  1966 18838  7205  1092    31     5  3186\n",
      "    21 12246  8065    11    70 44807    53 44012    11     5 44807  4292\n",
      "    19 41601    12 10463   791   131   959    24    21 28840 11640    31\n",
      "     5   481  3716    36   506 29866   112     8   132     6 20001     4\n",
      "   195   322   152    21    11  5709     7     5   797    14 22495    10\n",
      "  3716     9   481 18838  7205  1092     6    61    34    57   431     7\n",
      " 10754    19 22808   500 11674   646  4419  8174 18838  7205   246    21\n",
      "    67  2829  2906    11  2087  4590    53  2442    11 44807  4292    19\n",
      "     5   739  2849 19304     4    20 18838  7205  1092 31425  7284    55\n",
      "  6473   352    15     5   650 14966   366 38868  2849 19304     6    19\n",
      "   211   591   246  4100 32512     8 18838  3888  1366   387   303    11\n",
      "   795  5353   129    11 44807   204     8   195    53  3680    19  1122\n",
      "  5204   194  1389     8  3854  4392  1118     7   797     4  1773 22808\n",
      "   500 11674     8 18838  7205  1092    33    57  1027    25 10754   994\n",
      "     6    52 13773   258     5  5204   194   672     8 43141  3854     9\n",
      " 22808   500 11674     7   192   114   209    58  2132    30     5 18838\n",
      "  7205  1092 31425     4  7806  1389    11     5  2087  7728    58  8065\n",
      "     7  5549   207     9   797   923    36 44105     4   195   387    43\n",
      "    53  3854    11     5 43141  1382  2743 32512    19     5  8219     9\n",
      " 13484   365     6   147  1389    58   795    87   797    36 44105     4\n",
      "   195   250  2576  9217   322     2     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1]\n"
     ]
    }
   ],
   "source": [
    "print(demo_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 3 3 3 3 3 3 3 0 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 2\n",
      " 2 2 0 4 4 1 1 0 3 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "print(demo_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>In order to determine whether the MRPL12 mutation also induced changes in composition and assembly of the mitochondrial ribosomal large and small subunits, mitochondrial lysates from cultured fibroblasts (subject and control) were fractionated on isokinetic sucrose gradients (10–30%, as in Ref. [47]). If assembly of either the large subunit or the entire ribosome was affected then the distribution of individual ribosomal proteins would change within the gradient profile. On analysis MRPL12 from the patient was substantially decreased in all fractions but detectable in the fractions consistent with mt-LSU; however it was noticeably absent from the free pool (fractions 1 and 2, Fig. 5). This was in contrast to the control that exhibited a pool of free MRPL12, which has been reported to interact with POLRMT [56]. MRPL3 was also slightly reduced in subject cells but remained in fractions consistent with the large subunit. The MRPL12 mutation impacted more modestly on the small ribosomal subunit, with DAP3 apparently unaffected and MRPS18B found in lower amounts only in fractions 4 and 5 but otherwise with similar steady state levels and distribution profile compared to control. Since POLRMT and MRPL12 have been published as interactors, we analyzed both the steady state level and gradient distribution of POLRMT to see if these were affected by the MRPL12 mutation. Overall levels in the subject sample were decreased to 63% of control value (Fig. 5B) but distribution in the gradient appeared largely unaffected with the exception of fraction 11, where levels were lower than control (Fig. 5A bottom panels).</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n"
     ]
    }
   ],
   "source": [
    "print(demo_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " is okin etic  suc rose  grad ients 10 – 30 %,  levels  in  the  subject  sample  decreased  to  63 %  control  value \n"
     ]
    }
   ],
   "source": [
    "print(labeled_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at allenai/biomed_roberta_base were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "class Stage1model(nn.Module):\n",
    "    def __init__(self, dropout=0.1):\n",
    "        super(Stage1model, self).__init__()\n",
    "        self.mod = RobertaModel.from_pretrained(\n",
    "                    model_name,\n",
    "                    num_labels=num_classes+1,\n",
    "                    hidden_dropout_prob=dropout,\n",
    "                    output_hidden_states=True)\n",
    "        self.norm = nn.BatchNorm1d(512, eps=self.mod.config.layer_norm_eps)\n",
    "        self.drop = nn.Dropout(self.mod.config.hidden_dropout_prob)\n",
    "        self.classifier = nn.Linear(self.mod.config.hidden_size, num_classes+1)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        output = self.mod(input_ids=input_ids,attention_mask=attention_mask)\n",
    "        y_hat = output.hidden_states[-1]\n",
    "        y_hat = self.norm(y_hat)\n",
    "        y_hat = self.drop(y_hat)\n",
    "        y_hat = self.classifier(y_hat).permute(0,2,1)\n",
    "        return y_hat\n",
    "\n",
    "model = Stage1model().to(device)\n",
    "\n",
    "model_new = model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Stage1model(\n",
       "  (mod): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): RobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): RobertaPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (norm): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (drop): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=5, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# class OurBERTModel(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(OurBERTModel, self).__init__()\n",
    "#         self.mod = AutoModel.from_pretrained(model_name, num_labels=num_classes+1)\n",
    "#         self.drop = nn.Dropout(self.mod.config.hidden_dropout_prob)\n",
    "#         self.classifier = nn.Linear(self.mod.config.hidden_size, num_classes+1)\n",
    "\n",
    "#     def forward(self, text, att_mask):\n",
    "#         b, num_tokens = text.shape\n",
    "#         token_type = torch.zeros((b, num_tokens), dtype=torch.long).to(device)\n",
    "#         outputs = self.mod(text, attention_mask=att_mask, token_type_ids=token_type)\n",
    "#         return self.classifier(self.drop(outputs['last_hidden_state']))\n",
    "\n",
    "# model = OurBERTModel().to(device)\n",
    "\n",
    "# model_old = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# demo_logits = model(demo_batch['input_ids'], demo_batch['attention_mask'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# demo_logits.permute(0,2,1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# demo_batch['labels'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ypred = []\n",
    "# ytrue = []\n",
    "# for dlabels, dlogits in zip(demo_batch['labels'], demo_logits.permute(0,2,1)):\n",
    "#     print(dlabels.shape)\n",
    "#     print(dlogits.shape)\n",
    "#     for tlogits, tlabels in zip(dlogits, dlabels):\n",
    "#         print(tlabels.shape)\n",
    "#         print(tlogits.shape)\n",
    "#         ypred.append(tlogits.argmax().item())\n",
    "#         ytrue.append(tlabels.item())\n",
    "#         print(ypred)\n",
    "#         print(ytrue)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.CrossEntropyLoss(reduction='sum')\n",
    "optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "n_training_steps = n_epochs * len(batched_train_ds)\n",
    "lr_scheduler = get_scheduler(\n",
    "    name=\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=n_training_steps\n",
    ")\n",
    "\n",
    "\n",
    "def train_epoch(ds, criterion):\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    ypred = []\n",
    "    ytrue = []\n",
    "    for idx, batch in enumerate(ds):\n",
    "\n",
    "        labels = batch['labels']\n",
    "        input_ids = batch['input_ids']\n",
    "        attention_mask = batch['attention_mask']\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        logits = model(input_ids, attention_mask)\n",
    "        \n",
    "        loss = criterion(logits, labels)\n",
    "        # loss = (loss * attention_mask).sum() / (attention_mask).sum()\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "\n",
    "        progress_bar.update(1)\n",
    "            \n",
    "    return loss.item()\n",
    "\n",
    "\n",
    "\n",
    "def eval_epoch(ds, criterion):\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    ypred = []\n",
    "    ytrue = []\n",
    "    with torch.no_grad():\n",
    "        for idx, batch in enumerate(ds):\n",
    "\n",
    "            labels = batch['labels']\n",
    "            input_ids = batch['input_ids']\n",
    "            attention_mask = batch['attention_mask']\n",
    "\n",
    "            logits = model(input_ids, attention_mask)\n",
    "            \n",
    "            loss = criterion(logits, labels)\n",
    "            # loss = (loss * attention_mask).sum() / (attention_mask).sum()\n",
    "\n",
    "            for dlogits, dlabels in zip(logits.permute(0,2,1), labels):\n",
    "                    for tlogits, tlabels in zip(dlogits, dlabels):\n",
    "                        ypred.append(tlogits.argmax().item())\n",
    "                        ytrue.append(tlabels.item())\n",
    "\n",
    "            progress_bar.update(1)\n",
    "\n",
    "    acc = metrics.accuracy_score(ytrue,ypred)\n",
    "    report = classification_report(ytrue,ypred,\n",
    "                                    labels=list(task_map.values()),\n",
    "                                    target_names=list(task_map.keys()),\n",
    "                                    output_dict=True,\n",
    "                                    zero_division=0)\n",
    "\n",
    "                                    \n",
    "\n",
    "    return loss.item(), acc, report, ytrue, ypred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16ff5f14d50a4b90a4f42680a47597bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/730 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============ Begin Epoch 1 ============\n",
      "Train loss: 427.9848327636719\n",
      "Eval on train set loss: 404.16143798828125   accuracy: 0.9547422372611465\n",
      "Eval on dev set loss: 1032.7088623046875   accuracy: 0.9568793402777778\n",
      "============ Begin Epoch 2 ============\n",
      "Train loss: 267.1221923828125\n",
      "Eval on train set loss: 233.360107421875   accuracy: 0.9661810808121019\n",
      "Eval on dev set loss: 657.6698608398438   accuracy: 0.9639322916666667\n",
      "============ Begin Epoch 3 ============\n",
      "Train loss: 182.6040496826172\n",
      "Eval on train set loss: 163.73074340820312   accuracy: 0.972961037022293\n",
      "Eval on dev set loss: 581.215576171875   accuracy: 0.9665798611111112\n",
      "============ Begin Epoch 4 ============\n",
      "Train loss: 143.7663116455078\n",
      "Eval on train set loss: 130.49398803710938   accuracy: 0.9746529160031847\n",
      "Eval on dev set loss: 629.6563110351562   accuracy: 0.9669053819444444\n",
      "============ Begin Epoch 5 ============\n",
      "Train loss: 135.86399841308594\n",
      "Eval on train set loss: 116.50422668457031   accuracy: 0.9826333598726115\n",
      "Eval on dev set loss: 541.596923828125   accuracy: 0.9665147569444444\n",
      "============ Begin Epoch 6 ============\n",
      "Train loss: 132.98492431640625\n",
      "Eval on train set loss: 96.92002868652344   accuracy: 0.9844434215764332\n",
      "Eval on dev set loss: 585.3294677734375   accuracy: 0.9656901041666667\n",
      "============ Begin Epoch 7 ============\n",
      "Train loss: 92.86408996582031\n",
      "Eval on train set loss: 60.31620788574219   accuracy: 0.9887415406050956\n",
      "Eval on dev set loss: 593.36767578125   accuracy: 0.9687282986111111\n",
      "============ Begin Epoch 8 ============\n",
      "Train loss: 73.1380386352539\n",
      "Eval on train set loss: 53.649925231933594   accuracy: 0.9900166699840764\n",
      "Eval on dev set loss: 607.914306640625   accuracy: 0.9683810763888889\n",
      "============ Begin Epoch 9 ============\n",
      "Train loss: 69.69996643066406\n",
      "Eval on train set loss: 42.178565979003906   accuracy: 0.9912544785031847\n",
      "Eval on dev set loss: 613.109130859375   accuracy: 0.9681640625\n",
      "============ Begin Epoch 10 ============\n",
      "Train loss: 69.79676818847656\n",
      "Eval on train set loss: 41.79111862182617   accuracy: 0.9912855792197452\n",
      "Eval on dev set loss: 620.725341796875   accuracy: 0.9672092013888889\n"
     ]
    }
   ],
   "source": [
    "run_report = {  'epoch':[],\n",
    "                'train_loss':[],\n",
    "                'eval_train_loss':[],\n",
    "                'eval_train_acc':[],\n",
    "                'eval_train_ytrue':[],\n",
    "                'eval_train_ypred':[],\n",
    "                'eval_train_rpt':[],\n",
    "                'eval_dev_loss':[],\n",
    "                'eval_dev_acc':[],\n",
    "                'eval_dev_ytrue':[],\n",
    "                'eval_dev_ypred':[],\n",
    "                'eval_dev_rpt':[],\n",
    "             }\n",
    "\n",
    "num_total_steps = n_epochs * (len(batched_train_ds) * 2 + len(batched_dev_ds))\n",
    "progress_bar = tqdm(range(num_total_steps))\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    run_report['epoch'].append(epoch)\n",
    "    \n",
    "    print(f\"============ Begin Epoch {epoch+1} ============\")\n",
    "\n",
    "    loss = train_epoch(batched_train_ds, criterion)\n",
    "    print(f\"Train loss: {loss}\")\n",
    "    run_report['train_loss'].append(loss)\n",
    "    \n",
    "    output = eval_epoch(batched_train_ds, criterion)\n",
    "    (loss, acc, report, ytrue, ypred) = output\n",
    "    print(f'Eval on train set loss: {loss}   accuracy: {acc}')\n",
    "    run_report['eval_train_loss'].append(loss)\n",
    "    run_report['eval_train_acc'].append(acc)\n",
    "    run_report['eval_train_ytrue'].append(ytrue)\n",
    "    run_report['eval_train_ypred'].append(ypred)\n",
    "    run_report['eval_train_rpt'].append(report)\n",
    "\n",
    "    output = eval_epoch(batched_dev_ds, criterion)\n",
    "    (loss, acc, report, ytrue, ypred) = output\n",
    "    print(f'Eval on dev set loss: {loss}   accuracy: {acc}')\n",
    "    run_report['eval_dev_loss'].append(loss)\n",
    "    run_report['eval_dev_acc'].append(acc)\n",
    "    run_report['eval_dev_ytrue'].append(ytrue)\n",
    "    run_report['eval_dev_ypred'].append(ypred)\n",
    "    run_report['eval_dev_rpt'].append(report)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[42756   143    86   278    42]\n",
      " [   66  1104     4     9     2]\n",
      " [  152    26   246    82    24]\n",
      " [  218    15   121   388     3]\n",
      " [  128    12    24    76    75]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_labels = list(task_map.keys())\n",
    "display_labels.insert(0,str('NoLabel'))\n",
    "cm = confusion_matrix(ytrue,ypred)\n",
    "print(cm)\n",
    "ConfusionMatrixDisplay(confusion_matrix=cm,display_labels=display_labels).plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tabulate_metrics(reports):\n",
    "    metrics = []\n",
    "    for epoch in reports:\n",
    "        # print(epoch)\n",
    "        epoch_metrics = {}\n",
    "        for task, rpt in task_map.items():\n",
    "            for metric, value in epoch[task].items():\n",
    "                epoch_metrics[str(task+'_'+metric)] = value\n",
    "        metrics.append(epoch_metrics)\n",
    "\n",
    "    metrics = pd.DataFrame.from_dict(metrics)\n",
    "    metrics.index.rename('epoch')\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Quantity_precision</th>\n",
       "      <th>Quantity_recall</th>\n",
       "      <th>Quantity_f1-score</th>\n",
       "      <th>Quantity_support</th>\n",
       "      <th>MeasuredProperty_precision</th>\n",
       "      <th>MeasuredProperty_recall</th>\n",
       "      <th>MeasuredProperty_f1-score</th>\n",
       "      <th>MeasuredProperty_support</th>\n",
       "      <th>MeasuredEntity_precision</th>\n",
       "      <th>MeasuredEntity_recall</th>\n",
       "      <th>MeasuredEntity_f1-score</th>\n",
       "      <th>MeasuredEntity_support</th>\n",
       "      <th>Qualifier_precision</th>\n",
       "      <th>Qualifier_recall</th>\n",
       "      <th>Qualifier_f1-score</th>\n",
       "      <th>Qualifier_support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.858197</td>\n",
       "      <td>0.833221</td>\n",
       "      <td>0.845525</td>\n",
       "      <td>4467</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1707</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3227</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.892469</td>\n",
       "      <td>0.936423</td>\n",
       "      <td>0.913917</td>\n",
       "      <td>4467</td>\n",
       "      <td>0.596888</td>\n",
       "      <td>0.247217</td>\n",
       "      <td>0.349627</td>\n",
       "      <td>1707</td>\n",
       "      <td>0.542498</td>\n",
       "      <td>0.387667</td>\n",
       "      <td>0.452196</td>\n",
       "      <td>3227</td>\n",
       "      <td>0.545455</td>\n",
       "      <td>0.004666</td>\n",
       "      <td>0.009252</td>\n",
       "      <td>1286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.895153</td>\n",
       "      <td>0.963286</td>\n",
       "      <td>0.927971</td>\n",
       "      <td>4467</td>\n",
       "      <td>0.581973</td>\n",
       "      <td>0.521968</td>\n",
       "      <td>0.550340</td>\n",
       "      <td>1707</td>\n",
       "      <td>0.655268</td>\n",
       "      <td>0.510691</td>\n",
       "      <td>0.574016</td>\n",
       "      <td>3227</td>\n",
       "      <td>0.789474</td>\n",
       "      <td>0.034992</td>\n",
       "      <td>0.067014</td>\n",
       "      <td>1286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.906894</td>\n",
       "      <td>0.974703</td>\n",
       "      <td>0.939577</td>\n",
       "      <td>4467</td>\n",
       "      <td>0.638958</td>\n",
       "      <td>0.603398</td>\n",
       "      <td>0.620669</td>\n",
       "      <td>1707</td>\n",
       "      <td>0.769063</td>\n",
       "      <td>0.437558</td>\n",
       "      <td>0.557772</td>\n",
       "      <td>3227</td>\n",
       "      <td>0.802920</td>\n",
       "      <td>0.085537</td>\n",
       "      <td>0.154603</td>\n",
       "      <td>1286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.932412</td>\n",
       "      <td>0.982091</td>\n",
       "      <td>0.956607</td>\n",
       "      <td>4467</td>\n",
       "      <td>0.638716</td>\n",
       "      <td>0.780902</td>\n",
       "      <td>0.702688</td>\n",
       "      <td>1707</td>\n",
       "      <td>0.745307</td>\n",
       "      <td>0.738147</td>\n",
       "      <td>0.741709</td>\n",
       "      <td>3227</td>\n",
       "      <td>0.760802</td>\n",
       "      <td>0.383359</td>\n",
       "      <td>0.509824</td>\n",
       "      <td>1286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.953020</td>\n",
       "      <td>0.985449</td>\n",
       "      <td>0.968963</td>\n",
       "      <td>4467</td>\n",
       "      <td>0.675913</td>\n",
       "      <td>0.856473</td>\n",
       "      <td>0.755556</td>\n",
       "      <td>1707</td>\n",
       "      <td>0.736813</td>\n",
       "      <td>0.831112</td>\n",
       "      <td>0.781127</td>\n",
       "      <td>3227</td>\n",
       "      <td>0.736788</td>\n",
       "      <td>0.552877</td>\n",
       "      <td>0.631719</td>\n",
       "      <td>1286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.959365</td>\n",
       "      <td>0.988359</td>\n",
       "      <td>0.973646</td>\n",
       "      <td>4467</td>\n",
       "      <td>0.782093</td>\n",
       "      <td>0.849443</td>\n",
       "      <td>0.814378</td>\n",
       "      <td>1707</td>\n",
       "      <td>0.840328</td>\n",
       "      <td>0.825225</td>\n",
       "      <td>0.832708</td>\n",
       "      <td>3227</td>\n",
       "      <td>0.872922</td>\n",
       "      <td>0.571540</td>\n",
       "      <td>0.690789</td>\n",
       "      <td>1286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.965442</td>\n",
       "      <td>0.988135</td>\n",
       "      <td>0.976657</td>\n",
       "      <td>4467</td>\n",
       "      <td>0.759222</td>\n",
       "      <td>0.892209</td>\n",
       "      <td>0.820361</td>\n",
       "      <td>1707</td>\n",
       "      <td>0.878264</td>\n",
       "      <td>0.833901</td>\n",
       "      <td>0.855508</td>\n",
       "      <td>3227</td>\n",
       "      <td>0.896045</td>\n",
       "      <td>0.616641</td>\n",
       "      <td>0.730539</td>\n",
       "      <td>1286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.973110</td>\n",
       "      <td>0.988359</td>\n",
       "      <td>0.980675</td>\n",
       "      <td>4467</td>\n",
       "      <td>0.815633</td>\n",
       "      <td>0.886350</td>\n",
       "      <td>0.849523</td>\n",
       "      <td>1707</td>\n",
       "      <td>0.875078</td>\n",
       "      <td>0.872637</td>\n",
       "      <td>0.873856</td>\n",
       "      <td>3227</td>\n",
       "      <td>0.874391</td>\n",
       "      <td>0.698289</td>\n",
       "      <td>0.776481</td>\n",
       "      <td>1286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.973748</td>\n",
       "      <td>0.988135</td>\n",
       "      <td>0.980889</td>\n",
       "      <td>4467</td>\n",
       "      <td>0.817549</td>\n",
       "      <td>0.895138</td>\n",
       "      <td>0.854586</td>\n",
       "      <td>1707</td>\n",
       "      <td>0.859281</td>\n",
       "      <td>0.889371</td>\n",
       "      <td>0.874067</td>\n",
       "      <td>3227</td>\n",
       "      <td>0.852861</td>\n",
       "      <td>0.730171</td>\n",
       "      <td>0.786762</td>\n",
       "      <td>1286</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Quantity_precision  Quantity_recall  Quantity_f1-score  Quantity_support  \\\n",
       "0            0.858197         0.833221           0.845525              4467   \n",
       "1            0.892469         0.936423           0.913917              4467   \n",
       "2            0.895153         0.963286           0.927971              4467   \n",
       "3            0.906894         0.974703           0.939577              4467   \n",
       "4            0.932412         0.982091           0.956607              4467   \n",
       "5            0.953020         0.985449           0.968963              4467   \n",
       "6            0.959365         0.988359           0.973646              4467   \n",
       "7            0.965442         0.988135           0.976657              4467   \n",
       "8            0.973110         0.988359           0.980675              4467   \n",
       "9            0.973748         0.988135           0.980889              4467   \n",
       "\n",
       "   MeasuredProperty_precision  MeasuredProperty_recall  \\\n",
       "0                    0.000000                 0.000000   \n",
       "1                    0.596888                 0.247217   \n",
       "2                    0.581973                 0.521968   \n",
       "3                    0.638958                 0.603398   \n",
       "4                    0.638716                 0.780902   \n",
       "5                    0.675913                 0.856473   \n",
       "6                    0.782093                 0.849443   \n",
       "7                    0.759222                 0.892209   \n",
       "8                    0.815633                 0.886350   \n",
       "9                    0.817549                 0.895138   \n",
       "\n",
       "   MeasuredProperty_f1-score  MeasuredProperty_support  \\\n",
       "0                   0.000000                      1707   \n",
       "1                   0.349627                      1707   \n",
       "2                   0.550340                      1707   \n",
       "3                   0.620669                      1707   \n",
       "4                   0.702688                      1707   \n",
       "5                   0.755556                      1707   \n",
       "6                   0.814378                      1707   \n",
       "7                   0.820361                      1707   \n",
       "8                   0.849523                      1707   \n",
       "9                   0.854586                      1707   \n",
       "\n",
       "   MeasuredEntity_precision  MeasuredEntity_recall  MeasuredEntity_f1-score  \\\n",
       "0                  0.000000               0.000000                 0.000000   \n",
       "1                  0.542498               0.387667                 0.452196   \n",
       "2                  0.655268               0.510691                 0.574016   \n",
       "3                  0.769063               0.437558                 0.557772   \n",
       "4                  0.745307               0.738147                 0.741709   \n",
       "5                  0.736813               0.831112                 0.781127   \n",
       "6                  0.840328               0.825225                 0.832708   \n",
       "7                  0.878264               0.833901                 0.855508   \n",
       "8                  0.875078               0.872637                 0.873856   \n",
       "9                  0.859281               0.889371                 0.874067   \n",
       "\n",
       "   MeasuredEntity_support  Qualifier_precision  Qualifier_recall  \\\n",
       "0                    3227             0.000000          0.000000   \n",
       "1                    3227             0.545455          0.004666   \n",
       "2                    3227             0.789474          0.034992   \n",
       "3                    3227             0.802920          0.085537   \n",
       "4                    3227             0.760802          0.383359   \n",
       "5                    3227             0.736788          0.552877   \n",
       "6                    3227             0.872922          0.571540   \n",
       "7                    3227             0.896045          0.616641   \n",
       "8                    3227             0.874391          0.698289   \n",
       "9                    3227             0.852861          0.730171   \n",
       "\n",
       "   Qualifier_f1-score  Qualifier_support  \n",
       "0            0.000000               1286  \n",
       "1            0.009252               1286  \n",
       "2            0.067014               1286  \n",
       "3            0.154603               1286  \n",
       "4            0.509824               1286  \n",
       "5            0.631719               1286  \n",
       "6            0.690789               1286  \n",
       "7            0.730539               1286  \n",
       "8            0.776481               1286  \n",
       "9            0.786762               1286  "
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set_metrics = tabulate_metrics(run_report['eval_train_rpt'])\n",
    "train_set_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Quantity_precision</th>\n",
       "      <th>Quantity_recall</th>\n",
       "      <th>Quantity_f1-score</th>\n",
       "      <th>Quantity_support</th>\n",
       "      <th>MeasuredProperty_precision</th>\n",
       "      <th>MeasuredProperty_recall</th>\n",
       "      <th>MeasuredProperty_f1-score</th>\n",
       "      <th>MeasuredProperty_support</th>\n",
       "      <th>MeasuredEntity_precision</th>\n",
       "      <th>MeasuredEntity_recall</th>\n",
       "      <th>MeasuredEntity_f1-score</th>\n",
       "      <th>MeasuredEntity_support</th>\n",
       "      <th>Qualifier_precision</th>\n",
       "      <th>Qualifier_recall</th>\n",
       "      <th>Qualifier_f1-score</th>\n",
       "      <th>Qualifier_support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.837568</td>\n",
       "      <td>0.778903</td>\n",
       "      <td>0.807171</td>\n",
       "      <td>1185</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>530</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>745</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.901266</td>\n",
       "      <td>0.878651</td>\n",
       "      <td>1185</td>\n",
       "      <td>0.462617</td>\n",
       "      <td>0.186792</td>\n",
       "      <td>0.266129</td>\n",
       "      <td>530</td>\n",
       "      <td>0.445916</td>\n",
       "      <td>0.271141</td>\n",
       "      <td>0.337229</td>\n",
       "      <td>745</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.851477</td>\n",
       "      <td>0.924051</td>\n",
       "      <td>0.886281</td>\n",
       "      <td>1185</td>\n",
       "      <td>0.467312</td>\n",
       "      <td>0.364151</td>\n",
       "      <td>0.409332</td>\n",
       "      <td>530</td>\n",
       "      <td>0.480315</td>\n",
       "      <td>0.327517</td>\n",
       "      <td>0.389465</td>\n",
       "      <td>745</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>0.022222</td>\n",
       "      <td>0.042813</td>\n",
       "      <td>315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.843653</td>\n",
       "      <td>0.919831</td>\n",
       "      <td>0.880097</td>\n",
       "      <td>1185</td>\n",
       "      <td>0.519048</td>\n",
       "      <td>0.411321</td>\n",
       "      <td>0.458947</td>\n",
       "      <td>530</td>\n",
       "      <td>0.570447</td>\n",
       "      <td>0.222819</td>\n",
       "      <td>0.320463</td>\n",
       "      <td>745</td>\n",
       "      <td>0.388889</td>\n",
       "      <td>0.022222</td>\n",
       "      <td>0.042042</td>\n",
       "      <td>315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.831343</td>\n",
       "      <td>0.940084</td>\n",
       "      <td>0.882376</td>\n",
       "      <td>1185</td>\n",
       "      <td>0.490909</td>\n",
       "      <td>0.509434</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>530</td>\n",
       "      <td>0.453621</td>\n",
       "      <td>0.479195</td>\n",
       "      <td>0.466057</td>\n",
       "      <td>745</td>\n",
       "      <td>0.506329</td>\n",
       "      <td>0.126984</td>\n",
       "      <td>0.203046</td>\n",
       "      <td>315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.842663</td>\n",
       "      <td>0.940084</td>\n",
       "      <td>0.888712</td>\n",
       "      <td>1185</td>\n",
       "      <td>0.467577</td>\n",
       "      <td>0.516981</td>\n",
       "      <td>0.491039</td>\n",
       "      <td>530</td>\n",
       "      <td>0.428423</td>\n",
       "      <td>0.554362</td>\n",
       "      <td>0.483324</td>\n",
       "      <td>745</td>\n",
       "      <td>0.543478</td>\n",
       "      <td>0.238095</td>\n",
       "      <td>0.331126</td>\n",
       "      <td>315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.842305</td>\n",
       "      <td>0.937553</td>\n",
       "      <td>0.887380</td>\n",
       "      <td>1185</td>\n",
       "      <td>0.524520</td>\n",
       "      <td>0.464151</td>\n",
       "      <td>0.492492</td>\n",
       "      <td>530</td>\n",
       "      <td>0.508824</td>\n",
       "      <td>0.464430</td>\n",
       "      <td>0.485614</td>\n",
       "      <td>745</td>\n",
       "      <td>0.535714</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.225564</td>\n",
       "      <td>315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.849537</td>\n",
       "      <td>0.929114</td>\n",
       "      <td>0.887545</td>\n",
       "      <td>1185</td>\n",
       "      <td>0.511628</td>\n",
       "      <td>0.498113</td>\n",
       "      <td>0.504780</td>\n",
       "      <td>530</td>\n",
       "      <td>0.508053</td>\n",
       "      <td>0.465772</td>\n",
       "      <td>0.485994</td>\n",
       "      <td>745</td>\n",
       "      <td>0.556962</td>\n",
       "      <td>0.139683</td>\n",
       "      <td>0.223350</td>\n",
       "      <td>315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.852827</td>\n",
       "      <td>0.929114</td>\n",
       "      <td>0.889338</td>\n",
       "      <td>1185</td>\n",
       "      <td>0.520085</td>\n",
       "      <td>0.464151</td>\n",
       "      <td>0.490528</td>\n",
       "      <td>530</td>\n",
       "      <td>0.493404</td>\n",
       "      <td>0.502013</td>\n",
       "      <td>0.497671</td>\n",
       "      <td>745</td>\n",
       "      <td>0.508621</td>\n",
       "      <td>0.187302</td>\n",
       "      <td>0.273782</td>\n",
       "      <td>315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.849231</td>\n",
       "      <td>0.931646</td>\n",
       "      <td>0.888531</td>\n",
       "      <td>1185</td>\n",
       "      <td>0.511435</td>\n",
       "      <td>0.464151</td>\n",
       "      <td>0.486647</td>\n",
       "      <td>530</td>\n",
       "      <td>0.465786</td>\n",
       "      <td>0.520805</td>\n",
       "      <td>0.491762</td>\n",
       "      <td>745</td>\n",
       "      <td>0.513699</td>\n",
       "      <td>0.238095</td>\n",
       "      <td>0.325380</td>\n",
       "      <td>315</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Quantity_precision  Quantity_recall  Quantity_f1-score  Quantity_support  \\\n",
       "0            0.837568         0.778903           0.807171              1185   \n",
       "1            0.857143         0.901266           0.878651              1185   \n",
       "2            0.851477         0.924051           0.886281              1185   \n",
       "3            0.843653         0.919831           0.880097              1185   \n",
       "4            0.831343         0.940084           0.882376              1185   \n",
       "5            0.842663         0.940084           0.888712              1185   \n",
       "6            0.842305         0.937553           0.887380              1185   \n",
       "7            0.849537         0.929114           0.887545              1185   \n",
       "8            0.852827         0.929114           0.889338              1185   \n",
       "9            0.849231         0.931646           0.888531              1185   \n",
       "\n",
       "   MeasuredProperty_precision  MeasuredProperty_recall  \\\n",
       "0                    0.000000                 0.000000   \n",
       "1                    0.462617                 0.186792   \n",
       "2                    0.467312                 0.364151   \n",
       "3                    0.519048                 0.411321   \n",
       "4                    0.490909                 0.509434   \n",
       "5                    0.467577                 0.516981   \n",
       "6                    0.524520                 0.464151   \n",
       "7                    0.511628                 0.498113   \n",
       "8                    0.520085                 0.464151   \n",
       "9                    0.511435                 0.464151   \n",
       "\n",
       "   MeasuredProperty_f1-score  MeasuredProperty_support  \\\n",
       "0                   0.000000                       530   \n",
       "1                   0.266129                       530   \n",
       "2                   0.409332                       530   \n",
       "3                   0.458947                       530   \n",
       "4                   0.500000                       530   \n",
       "5                   0.491039                       530   \n",
       "6                   0.492492                       530   \n",
       "7                   0.504780                       530   \n",
       "8                   0.490528                       530   \n",
       "9                   0.486647                       530   \n",
       "\n",
       "   MeasuredEntity_precision  MeasuredEntity_recall  MeasuredEntity_f1-score  \\\n",
       "0                  0.000000               0.000000                 0.000000   \n",
       "1                  0.445916               0.271141                 0.337229   \n",
       "2                  0.480315               0.327517                 0.389465   \n",
       "3                  0.570447               0.222819                 0.320463   \n",
       "4                  0.453621               0.479195                 0.466057   \n",
       "5                  0.428423               0.554362                 0.483324   \n",
       "6                  0.508824               0.464430                 0.485614   \n",
       "7                  0.508053               0.465772                 0.485994   \n",
       "8                  0.493404               0.502013                 0.497671   \n",
       "9                  0.465786               0.520805                 0.491762   \n",
       "\n",
       "   MeasuredEntity_support  Qualifier_precision  Qualifier_recall  \\\n",
       "0                     745             0.000000          0.000000   \n",
       "1                     745             0.000000          0.000000   \n",
       "2                     745             0.583333          0.022222   \n",
       "3                     745             0.388889          0.022222   \n",
       "4                     745             0.506329          0.126984   \n",
       "5                     745             0.543478          0.238095   \n",
       "6                     745             0.535714          0.142857   \n",
       "7                     745             0.556962          0.139683   \n",
       "8                     745             0.508621          0.187302   \n",
       "9                     745             0.513699          0.238095   \n",
       "\n",
       "   Qualifier_f1-score  Qualifier_support  \n",
       "0            0.000000                315  \n",
       "1            0.000000                315  \n",
       "2            0.042813                315  \n",
       "3            0.042042                315  \n",
       "4            0.203046                315  \n",
       "5            0.331126                315  \n",
       "6            0.225564                315  \n",
       "7            0.223350                315  \n",
       "8            0.273782                315  \n",
       "9            0.325380                315  "
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_set_metrics = tabulate_metrics(run_report['eval_dev_rpt'])\n",
    "dev_set_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "########################## loss plot #######################\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "x = np.array(range(len(run_report['epoch'])))\n",
    "y = np.array(run_report['eval_dev_loss'])\n",
    "plt.plot(x,y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "########################## accuracy plot #######################\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "x = np.array(range(len(run_report['epoch'])))\n",
    "y = np.array(run_report['eval_dev_acc'])\n",
    "plt.plot(x,y)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 ('test')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b5060d9997a95c2acb3a42af5d14caeb5dba3e5b7e20123b9f235f707614ce30"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
