{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import RobertaConfig, RobertaTokenizerFast, RobertaForTokenClassification, RobertaModel\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# from transformers import RobertaTokenizer, RobertaModel\n",
    "\n",
    "random.seed(42)\n",
    "reshuffle_docs = False\n",
    "\n",
    "percent_to_test = .1\n",
    "percent_to_dev = .2\n",
    "percent_to_train =  1 - percent_to_dev - percent_to_test\n",
    "\n",
    "batch_size = 10 # documents\n",
    "learning_rate = 2e-4\n",
    "n_epochs = 10\n",
    "\n",
    "model_name = 'allenai/biomed_roberta_base'\n",
    "dropout = .03\n",
    "\n",
    "# annot_types = ['Quantity', 'MeasuredEntity', 'MeasuredProperty', 'Qualifier']\n",
    "\n",
    "dummy_run = True\n",
    "\n",
    "small_size = .2 # multiplier for making small datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "currentdir = os.getcwd() # ~/MeasEval/baselines\n",
    "\n",
    "combopath_txt = os.path.join(currentdir, \"../data/raw/combo/text/\")\n",
    "combopath_annot = os.path.join(currentdir, \"../data/raw/combo/tsv/\")\n",
    "\n",
    "interimpath = os.path.join(currentdir, \"../data/interim/\")\n",
    "\n",
    "\n",
    "\n",
    "if reshuffle_docs == True:\n",
    "    docIds = []\n",
    "    combo_txt = {}\n",
    "    for fn in os.listdir(combopath_txt):\n",
    "        docIds.append(fn[:-4])\n",
    "        path = combopath_txt+fn\n",
    "        with open(path) as textfile:\n",
    "                text = textfile.read()\n",
    "                #[:-4] strips off the .txt to get the id\n",
    "                combo_txt[fn[:-4]] = text\n",
    "\n",
    "    combo_annot = pd.DataFrame()\n",
    "    for fn in os.listdir(combopath_annot):\n",
    "        path = combopath_annot+fn\n",
    "        file = pd.read_csv(path,delimiter='\\t',encoding='utf-8')\n",
    "        combo_annot = pd.concat([combo_annot, file],ignore_index=True)\n",
    "\n",
    "    random.shuffle(docIds)\n",
    "\n",
    "    n_doc = len(docIds)\n",
    "    split_train = int(np.round(n_doc * percent_to_train))\n",
    "    split_dev = split_train + int(np.round(n_doc * percent_to_dev))\n",
    "\n",
    "    docs_train = docIds[:split_train]\n",
    "    docs_dev = docIds[split_train:split_dev]\n",
    "    docs_test = docIds[split_dev:]\n",
    "\n",
    "    train_annot = combo_annot.loc[combo_annot['docId'].isin(docs_train)]\n",
    "    dev_annot = combo_annot.loc[combo_annot['docId'].isin(docs_dev)]\n",
    "    test_annot = combo_annot.loc[combo_annot['docId'].isin(docs_test)]\n",
    "\n",
    "    # save data\n",
    "    train_annot.to_csv(interimpath+'train_annot.csv')\n",
    "    dev_annot.to_csv(interimpath+'dev_annot.csv')\n",
    "    test_annot.to_csv(interimpath+'test_annot.csv')\n",
    "\n",
    "    train_txt = {d: combo_txt[d] for d in docs_train}\n",
    "    dev_txt = {d: combo_txt[d] for d in docs_dev}\n",
    "    test_txt = {d: combo_txt[d] for d in docs_test}\n",
    "    \n",
    "    with open(interimpath+'train_txt.json','w') as f:\n",
    "        json.dump(train_txt, f)\n",
    "    with open(interimpath+'dev_txt.json','w') as f:\n",
    "        json.dump(dev_txt, f)\n",
    "    with open(interimpath+'test_txt.json','w') as f:\n",
    "        json.dump(test_txt, f)\n",
    "\n",
    "else:\n",
    "    train_annot = pd.read_csv(interimpath+'train_annot.csv')\n",
    "    dev_annot = pd.read_csv(interimpath+'dev_annot.csv')\n",
    "    test_annot = pd.read_csv(interimpath+'test_annot.csv')\n",
    "\n",
    "    with open(interimpath+'train_txt.json','r') as f:\n",
    "        train_txt = json.load(f)\n",
    "    with open(interimpath+'dev_txt.json','r') as f:\n",
    "        dev_txt = json.load(f)\n",
    "    with open(interimpath+'test_txt.json','r') as f:\n",
    "        test_txt = json.load(f)\n",
    "\n",
    "train_docIds = list(set(train_annot['docId']))\n",
    "dev_docIds = list(set(dev_annot['docId']))\n",
    "test_docIds = list(set(test_annot['docId']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## Tokenizer & Model ###########\n",
    "\n",
    "# config = RobertaConfig.from_pretrained(model_name, problem_type='multi_label_classification', num_labels=2, label2id={'QUANT':1,'NOT QUANT':0})\n",
    "\n",
    "\n",
    "\n",
    "tokenizer = RobertaTokenizerFast.from_pretrained(model_name)\n",
    "# data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer, return_tensors=\"pt\", padding='max_length')\n",
    "\n",
    "\n",
    "########## Optimizer & Loss ###########\n",
    "\n",
    "# criterion = torch.nn.CrossEntropyLoss()\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_align_labels(docs_or_sents, txt, annotation, type=type, tokenizer=tokenizer):\n",
    "    \n",
    "    toks_with_labels = []\n",
    "\n",
    "    for doc in docs_or_sents:\n",
    "        # print(doc)\n",
    "\n",
    "        encoded_txt = tokenizer(txt[doc], padding='max_length', max_length=512, truncation=True)\n",
    "        # print(encoded_txt)\n",
    "\n",
    "        encoded_tokens = encoded_txt['input_ids']\n",
    "        # print(encoded_tokens)\n",
    "\n",
    "        doc_annot = annotation.loc[annotation['docId'] == doc]\n",
    "        # print(doc_annot)\n",
    "\n",
    "        annot_spans = np.array(doc_annot[['startOffset','endOffset']])\n",
    "        # print(f'annot_spans={annot_spans}')\n",
    "\n",
    "        label_ids = np.full(len(encoded_tokens),0)\n",
    "        special_ids = tokenizer.all_special_ids\n",
    "        # print(label_ids.shape)\n",
    "\n",
    "        for token_idx, token in enumerate(encoded_tokens):\n",
    "            # decoded_token = tokenizer.decode(token)\n",
    "            # print(f\"token index: {token_idx}\")\n",
    "            # print(f\"decoded token: {decoded_token}\")\n",
    "\n",
    "            if token in special_ids:\n",
    "                label_ids[token_idx] = 0\n",
    "                # print('special token')\n",
    "\n",
    "            else:\n",
    "                token_start_char = encoded_txt.token_to_chars(token_idx).start\n",
    "                token_end_char = encoded_txt.token_to_chars(token_idx).end\n",
    "                # print(f\"token span: {[token_start_char,token_end_char]}\")\n",
    "                for start, end in annot_spans:\n",
    "                    if start <= token_start_char <= end:\n",
    "                        label_ids[token_idx] = 1\n",
    "                        # print(f'{type} entity found spanning {[start,end]}')\n",
    "                        break\n",
    "                    else:\n",
    "                        label_ids[token_idx] = 0\n",
    "                        # print(\"no entity found\")\n",
    "        \n",
    "        encoded_txt['doc_or_sent_id'] = doc\n",
    "        encoded_txt['labels'] = list(label_ids)\n",
    "        toks_with_labels.append(encoded_txt)\n",
    "    # return toks_with_labels\n",
    "    return pd.DataFrame.from_dict(toks_with_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "################# TOKENIZE #################\n",
    "small_train_docIds = random.sample(train_docIds, int(len(train_docIds)*small_size))\n",
    "\n",
    "quant_small_train_dataset = tokenize_and_align_labels(\n",
    "    docs_or_sents=small_train_docIds,\n",
    "    txt=train_txt,\n",
    "    annotation=train_annot,\n",
    "    type='Quantity',\n",
    "    tokenizer=tokenizer)\n",
    "\n",
    "\n",
    "small_dev_docIds = random.sample(dev_docIds, int(len(dev_docIds)*small_size))\n",
    "\n",
    "quant_small_dev_dataset = tokenize_and_align_labels(\n",
    "    docs_or_sents=small_dev_docIds,\n",
    "    txt=dev_txt,\n",
    "    annotation=dev_annot,\n",
    "    type='Quantity',\n",
    "    tokenizer=tokenizer)\n",
    "\n",
    "test_docIds = test_docIds\n",
    "\n",
    "quant_test_dataset = tokenize_and_align_labels(\n",
    "    docs_or_sents=test_docIds,\n",
    "    txt=test_txt,\n",
    "    annotation=test_annot,\n",
    "    type='Quantity',\n",
    "    tokenizer=tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batchify(tokenized_dataset, batch_size):\n",
    "    num_examples = int(tokenized_dataset.shape[0] / batch_size)\n",
    "    batch_sizes = [batch_size for x in range(num_examples)]\n",
    "    last_batch_size = tokenized_dataset.shape[0] % batch_size\n",
    "    if last_batch_size:\n",
    "        batch_sizes.append(last_batch_size)\n",
    "    print(batch_sizes)\n",
    "\n",
    "    batched_dataset = []\n",
    "\n",
    "    for idx, size in enumerate(batch_sizes):\n",
    "        start = sum(batch_sizes[:idx])\n",
    "        end = sum(batch_sizes[:idx]) + size - 1\n",
    "        # print(start,end,idx)\n",
    "        input_ids = torch.LongTensor(tokenized_dataset['input_ids'].loc[start:end].tolist())\n",
    "        attention_mask = torch.LongTensor(tokenized_dataset['attention_mask'].loc[start:end].tolist())\n",
    "        labels = torch.LongTensor(tokenized_dataset['labels'].loc[start:end].tolist())\n",
    "        print(labels.shape)\n",
    "        doc_or_sent_id = list(tokenized_dataset['doc_or_sent_id'].loc[start:end])\n",
    "        \n",
    "        batch = {\n",
    "            'input_ids':input_ids,\n",
    "            'attention_mask':attention_mask,\n",
    "            'labels':labels,\n",
    "            'doc_or_sent_id':doc_or_sent_id\n",
    "        }\n",
    "        \n",
    "        batched_dataset.append(batch)\n",
    "\n",
    "    return batched_dataset\n",
    "\n",
    "# batched_dev_ds = batchify(quant_small_dev_dataset, batch_size=batch_size)\n",
    "# print(batched_dev_ds[0]['labels'][7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10, 10, 10, 10, 10, 10]\n",
      "torch.Size([10, 512])\n",
      "torch.Size([10, 512])\n",
      "torch.Size([10, 512])\n",
      "torch.Size([10, 512])\n",
      "torch.Size([10, 512])\n",
      "torch.Size([10, 512])\n",
      "[10, 7]\n",
      "torch.Size([10, 512])\n",
      "torch.Size([7, 512])\n",
      "[10, 10, 10, 10, 2]\n",
      "torch.Size([10, 512])\n",
      "torch.Size([10, 512])\n",
      "torch.Size([10, 512])\n",
      "torch.Size([10, 512])\n",
      "torch.Size([2, 512])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "################# batchify ####################\n",
    "batched_quant_small_train_ds = batchify(quant_small_train_dataset, batch_size=batch_size)\n",
    "batched_quant_small_dev_ds = batchify(quant_small_dev_dataset, batch_size=batch_size)\n",
    "batched_quant_small_test_ds = batchify(quant_test_dataset, batch_size=batch_size)\n",
    "\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = RobertaConfig.from_pretrained(model_name)\n",
    "model = RobertaForTokenClassification._from_config(config)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "from torch.optim import AdamW\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "from transformers import get_scheduler\n",
    "\n",
    "num_epochs = n_epochs\n",
    "num_training_steps = num_epochs * len(batched_quant_small_train_ds)\n",
    "lr_scheduler = get_scheduler(\n",
    "    name=\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps\n",
    ")\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "import torchmetrics\n",
    "\n",
    "f1 = torchmetrics.F1Score()\n",
    "acc = torchmetrics.Accuracy(mdmc_average='samplewise')\n",
    "\n",
    "from tqdm.auto import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c322ae1b2c543a3953f058347dc7388",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/60 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.9563)\n",
      "tensor(0.9428)\n",
      "{'epochs': [1], 'batches': [1, 2, 3, 4, 5, 6], 'train_losses': [1.5162665843963623, 0.564725399017334, 0.759817898273468, 11.09130859375, 3.1185202598571777, 0.4187241196632385], 'eval_losses': []}\n",
      "tensor(0.9563)\n",
      "tensor(0.9428)\n",
      "{'epochs': [1, 2], 'batches': [1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6], 'train_losses': [1.5162665843963623, 0.564725399017334, 0.759817898273468, 11.09130859375, 3.1185202598571777, 0.4187241196632385, 0.388182133436203, 0.38244616985321045, 1.0842511653900146, 0.7320479154586792, 0.6392818689346313, 0.5434420108795166], 'eval_losses': []}\n",
      "tensor(0.9563)\n",
      "tensor(0.9428)\n",
      "{'epochs': [1, 2, 3], 'batches': [1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6], 'train_losses': [1.5162665843963623, 0.564725399017334, 0.759817898273468, 11.09130859375, 3.1185202598571777, 0.4187241196632385, 0.388182133436203, 0.38244616985321045, 1.0842511653900146, 0.7320479154586792, 0.6392818689346313, 0.5434420108795166, 0.2714674174785614, 0.20074506103992462, 0.4244077801704407, 0.31923848390579224, 0.39636170864105225, 0.3861631751060486], 'eval_losses': []}\n",
      "tensor(0.9563)\n",
      "tensor(0.9428)\n",
      "{'epochs': [1, 2, 3, 4], 'batches': [1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6], 'train_losses': [1.5162665843963623, 0.564725399017334, 0.759817898273468, 11.09130859375, 3.1185202598571777, 0.4187241196632385, 0.388182133436203, 0.38244616985321045, 1.0842511653900146, 0.7320479154586792, 0.6392818689346313, 0.5434420108795166, 0.2714674174785614, 0.20074506103992462, 0.4244077801704407, 0.31923848390579224, 0.39636170864105225, 0.3861631751060486, 0.2653798460960388, 0.1991930454969406, 0.4320772588253021, 0.3452964127063751, 0.34895482659339905, 0.35054779052734375], 'eval_losses': []}\n",
      "tensor(0.9563)\n",
      "tensor(0.9428)\n",
      "{'epochs': [1, 2, 3, 4, 5], 'batches': [1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6], 'train_losses': [1.5162665843963623, 0.564725399017334, 0.759817898273468, 11.09130859375, 3.1185202598571777, 0.4187241196632385, 0.388182133436203, 0.38244616985321045, 1.0842511653900146, 0.7320479154586792, 0.6392818689346313, 0.5434420108795166, 0.2714674174785614, 0.20074506103992462, 0.4244077801704407, 0.31923848390579224, 0.39636170864105225, 0.3861631751060486, 0.2653798460960388, 0.1991930454969406, 0.4320772588253021, 0.3452964127063751, 0.34895482659339905, 0.35054779052734375, 0.21880045533180237, 0.19238509237766266, 0.4793774485588074, 0.34117117524147034, 0.32041630148887634, 0.30450209975242615], 'eval_losses': []}\n",
      "tensor(0.9563)\n",
      "tensor(0.9428)\n",
      "{'epochs': [1, 2, 3, 4, 5, 6], 'batches': [1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6], 'train_losses': [1.5162665843963623, 0.564725399017334, 0.759817898273468, 11.09130859375, 3.1185202598571777, 0.4187241196632385, 0.388182133436203, 0.38244616985321045, 1.0842511653900146, 0.7320479154586792, 0.6392818689346313, 0.5434420108795166, 0.2714674174785614, 0.20074506103992462, 0.4244077801704407, 0.31923848390579224, 0.39636170864105225, 0.3861631751060486, 0.2653798460960388, 0.1991930454969406, 0.4320772588253021, 0.3452964127063751, 0.34895482659339905, 0.35054779052734375, 0.21880045533180237, 0.19238509237766266, 0.4793774485588074, 0.34117117524147034, 0.32041630148887634, 0.30450209975242615, 0.20955102145671844, 0.1999109834432602, 0.38722503185272217, 0.30755406618118286, 0.3078443109989166, 0.30687302350997925], 'eval_losses': []}\n",
      "tensor(0.9563)\n",
      "tensor(0.9428)\n",
      "{'epochs': [1, 2, 3, 4, 5, 6, 7], 'batches': [1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6], 'train_losses': [1.5162665843963623, 0.564725399017334, 0.759817898273468, 11.09130859375, 3.1185202598571777, 0.4187241196632385, 0.388182133436203, 0.38244616985321045, 1.0842511653900146, 0.7320479154586792, 0.6392818689346313, 0.5434420108795166, 0.2714674174785614, 0.20074506103992462, 0.4244077801704407, 0.31923848390579224, 0.39636170864105225, 0.3861631751060486, 0.2653798460960388, 0.1991930454969406, 0.4320772588253021, 0.3452964127063751, 0.34895482659339905, 0.35054779052734375, 0.21880045533180237, 0.19238509237766266, 0.4793774485588074, 0.34117117524147034, 0.32041630148887634, 0.30450209975242615, 0.20955102145671844, 0.1999109834432602, 0.38722503185272217, 0.30755406618118286, 0.3078443109989166, 0.30687302350997925, 0.23997899889945984, 0.22099407017230988, 0.38658231496810913, 0.30750975012779236, 0.2992277145385742, 0.29673200845718384], 'eval_losses': []}\n",
      "tensor(0.9563)\n",
      "tensor(0.9428)\n",
      "{'epochs': [1, 2, 3, 4, 5, 6, 7, 8], 'batches': [1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6], 'train_losses': [1.5162665843963623, 0.564725399017334, 0.759817898273468, 11.09130859375, 3.1185202598571777, 0.4187241196632385, 0.388182133436203, 0.38244616985321045, 1.0842511653900146, 0.7320479154586792, 0.6392818689346313, 0.5434420108795166, 0.2714674174785614, 0.20074506103992462, 0.4244077801704407, 0.31923848390579224, 0.39636170864105225, 0.3861631751060486, 0.2653798460960388, 0.1991930454969406, 0.4320772588253021, 0.3452964127063751, 0.34895482659339905, 0.35054779052734375, 0.21880045533180237, 0.19238509237766266, 0.4793774485588074, 0.34117117524147034, 0.32041630148887634, 0.30450209975242615, 0.20955102145671844, 0.1999109834432602, 0.38722503185272217, 0.30755406618118286, 0.3078443109989166, 0.30687302350997925, 0.23997899889945984, 0.22099407017230988, 0.38658231496810913, 0.30750975012779236, 0.2992277145385742, 0.29673200845718384, 0.21056470274925232, 0.19232964515686035, 0.40269508957862854, 0.30883726477622986, 0.30615854263305664, 0.3003033995628357], 'eval_losses': []}\n",
      "tensor(0.9563)\n",
      "tensor(0.9428)\n",
      "{'epochs': [1, 2, 3, 4, 5, 6, 7, 8, 9], 'batches': [1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6], 'train_losses': [1.5162665843963623, 0.564725399017334, 0.759817898273468, 11.09130859375, 3.1185202598571777, 0.4187241196632385, 0.388182133436203, 0.38244616985321045, 1.0842511653900146, 0.7320479154586792, 0.6392818689346313, 0.5434420108795166, 0.2714674174785614, 0.20074506103992462, 0.4244077801704407, 0.31923848390579224, 0.39636170864105225, 0.3861631751060486, 0.2653798460960388, 0.1991930454969406, 0.4320772588253021, 0.3452964127063751, 0.34895482659339905, 0.35054779052734375, 0.21880045533180237, 0.19238509237766266, 0.4793774485588074, 0.34117117524147034, 0.32041630148887634, 0.30450209975242615, 0.20955102145671844, 0.1999109834432602, 0.38722503185272217, 0.30755406618118286, 0.3078443109989166, 0.30687302350997925, 0.23997899889945984, 0.22099407017230988, 0.38658231496810913, 0.30750975012779236, 0.2992277145385742, 0.29673200845718384, 0.21056470274925232, 0.19232964515686035, 0.40269508957862854, 0.30883726477622986, 0.30615854263305664, 0.3003033995628357, 0.20578332245349884, 0.18942400813102722, 0.40695151686668396, 0.3112231194972992, 0.30142122507095337, 0.2988133728504181], 'eval_losses': []}\n",
      "tensor(0.9563)\n",
      "tensor(0.9428)\n",
      "{'epochs': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10], 'batches': [1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6], 'train_losses': [1.5162665843963623, 0.564725399017334, 0.759817898273468, 11.09130859375, 3.1185202598571777, 0.4187241196632385, 0.388182133436203, 0.38244616985321045, 1.0842511653900146, 0.7320479154586792, 0.6392818689346313, 0.5434420108795166, 0.2714674174785614, 0.20074506103992462, 0.4244077801704407, 0.31923848390579224, 0.39636170864105225, 0.3861631751060486, 0.2653798460960388, 0.1991930454969406, 0.4320772588253021, 0.3452964127063751, 0.34895482659339905, 0.35054779052734375, 0.21880045533180237, 0.19238509237766266, 0.4793774485588074, 0.34117117524147034, 0.32041630148887634, 0.30450209975242615, 0.20955102145671844, 0.1999109834432602, 0.38722503185272217, 0.30755406618118286, 0.3078443109989166, 0.30687302350997925, 0.23997899889945984, 0.22099407017230988, 0.38658231496810913, 0.30750975012779236, 0.2992277145385742, 0.29673200845718384, 0.21056470274925232, 0.19232964515686035, 0.40269508957862854, 0.30883726477622986, 0.30615854263305664, 0.3003033995628357, 0.20578332245349884, 0.18942400813102722, 0.40695151686668396, 0.3112231194972992, 0.30142122507095337, 0.2988133728504181, 0.20925530791282654, 0.19164486229419708, 0.3975028693675995, 0.3079817295074463, 0.30151134729385376, 0.29849207401275635], 'eval_losses': []}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "progress_bar = tqdm(range(num_training_steps))\n",
    "\n",
    "logger = {\n",
    "    'epochs':[],\n",
    "    'batches':[],\n",
    "    'train_losses':[],\n",
    "    'eval_losses':[]\n",
    "}\n",
    "\n",
    "def train_once(ds=batched_quant_small_train_ds):\n",
    "    model.train()\n",
    "    for idx, batch in enumerate(batched_quant_small_train_ds):\n",
    "        logger['batches'].append(idx+1)\n",
    "        labels = batch['labels']\n",
    "        input_ids = batch['input_ids']\n",
    "        attention_mask = batch['attention_mask']\n",
    "        \n",
    "        outputs = model(input_ids, attention_mask)\n",
    "        logits = outputs.logits\n",
    "        \n",
    "        train_loss = criterion(logits.permute(0,2,1), labels)\n",
    "        logger['train_losses'].append(train_loss.item())\n",
    "        \n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        progress_bar.update(1)\n",
    "\n",
    "\n",
    "def eval_once(ds=batched_quant_small_dev_ds):\n",
    "    model.eval()\n",
    "    for idx, batch in enumerate(ds):\n",
    "        labels = batch['labels']\n",
    "        input_ids = batch['input_ids']\n",
    "        attention_mask = batch['attention_mask']\n",
    "\n",
    "        output = model(input_ids, attention_mask, labels=labels)\n",
    "        logits = output.logits.permute(0,2,1)\n",
    "        \n",
    "        # logger['eval_losses'].append(output.loss).item()\n",
    "        \n",
    "        predictions = torch.argmax(logits, dim=1)\n",
    "        # print(predictions)\n",
    "        batch['predictions'] = predictions\n",
    "        print(acc(labels,predictions))\n",
    "\n",
    "        # classification_report(labels[0], predictions[0])\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    logger['epochs'].append(epoch+1)\n",
    "    \n",
    "    train_once()\n",
    "    eval_once()\n",
    "\n",
    "    print(logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[    0,  1121,   937,  ...,     1,     1,     1],\n",
       "         [    0, 27526,  9021,  ...,     1,     1,     1],\n",
       "         [    0,   133,   511,  ...,     1,     1,     1],\n",
       "         ...,\n",
       "         [    0, 30383,    58,  ...,     1,     1,     1],\n",
       "         [    0, 20823,  3899,  ...,     1,     1,     1],\n",
       "         [    0, 46444,     7,  ...,     1,     1,     1]]),\n",
       " 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0]]),\n",
       " 'labels': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0]]),\n",
       " 'doc_or_sent_id': ['S0921818113002245-1752',\n",
       "  'S0378383912000130-1048',\n",
       "  'S0038071713001971-1427',\n",
       "  'S1359645413009816-2227',\n",
       "  'S0950705113001895-23699',\n",
       "  'S0019103512003995-1283',\n",
       "  'S2211124713006475-741',\n",
       "  'S037842901300244X-1654',\n",
       "  'S2213158213001253-2433',\n",
       "  'S0950705113001895-23682'],\n",
       " 'predictions': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0]])}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batched_quant_small_dev_ds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.9563)\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[492,   0],\n",
       "       [ 20,   0]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = batched_quant_small_dev_ds[0]\n",
    "\n",
    "labels = batch['labels']\n",
    "input_ids = batch['input_ids']\n",
    "attention_mask = batch['attention_mask']\n",
    "\n",
    "output = model(input_ids, attention_mask, labels=labels)\n",
    "logits = output.logits.permute(0,2,1)\n",
    "\n",
    "# logger['eval_losses'].append(output.loss).item()\n",
    "\n",
    "predictions = torch.argmax(logits, dim=1)\n",
    "# print(predictions)\n",
    "batch['predictions'] = predictions\n",
    "print(acc(labels,predictions))\n",
    "print(labels)\n",
    "confusion_matrix(labels[0],predictions[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# np.array(logger['train_losses'])\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "y = np.array(logger['train_losses'])\n",
    "x = np.array(range(len(logger['batches'])))\n",
    "plt.plot(x,y)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 ('test')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b5060d9997a95c2acb3a42af5d14caeb5dba3e5b7e20123b9f235f707614ce30"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
