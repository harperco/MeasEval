{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "94dcc700",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import numpy as np\n",
    "import random\n",
    "import re\n",
    "import os\n",
    "#from os import path\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "import torch\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "import torch, torch.nn as nn\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "import json\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "else:\n",
    "    device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1fc48db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Parameters ####\n",
    "\n",
    "basepath = \"../data/raw/\"\n",
    "\n",
    "bert_type = 'roberta-base'\n",
    "batch_size = 128\n",
    "\n",
    "output_folder = \"../outputs/\"\n",
    "\n",
    "test_label_folder = \"../outputs/modifiers_span_predictions_roberta-base_2022-07-13_01_48_39_2022-07-13_01_57_02/\"\n",
    "\n",
    "n_epochs = 10\n",
    "\n",
    "tasks = ['mp', 'me', 'qual']\n",
    "\n",
    "train_doc_path = basepath + \"train/text/\"\n",
    "trial_doc_path = basepath + \"trial/txt/\"\n",
    "eval_doc_path = basepath + \"eval/text/\"\n",
    "\n",
    "train_label_path = basepath + \"train/tsv/\"\n",
    "trial_label_path = basepath + \"trial/tsv/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a64b265b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_modifiers(data_filepath: str, output_filepath: str, quant_filepath: str):\n",
    "\n",
    "    # Extracted keywords and units for faster processing   \n",
    "    list_units = ['varves','hour','s−1','g mol−1','mg/l','kgs-1','beach materials','kW','g/m3','scale heights','μs','×','cm−1','mm2','mM'\n",
    "            'Mbps','mbar','hr','passages','participants','μbar','day','m2 g−1','mg','oN','μg/m','kg','GPa','μM','women'\n",
    "            'stems/ha','nbar','vertices','U/ml','m thick','month','ppt','W m−2','centimeters','second','mg/ml','mWm−2','°C','μg/L','wt.%'\n",
    "            'times','months','ppq','M','km','mBar','cm2/Vs','Ma','H','point','monomer units','mA/cm2','nT','cm3 g−1','m'\n",
    "            '°','metre','°N','bp','v/v','Mt','cores','Whitehall II participants','Rs','m/s','mdeg','Torr','Å/s','occasions','clones'\n",
    "            'men','%','kHz','MPa','mH','MW','orders of magnitude','year-old','m s−1','UT','hours','h','μL','weeks','discrete particles'\n",
    "            '% per year','wt%','cm−3','kV','fold','lines of code','days','nJ','years','km/h','week','km s−1','ppm','mmol/L','year'\n",
    "            'Rp','elderly participants','g cm−3','bit','AU','MeV','mm per side','° latitude','mm3','yrs','pairs per mm','K/min',\n",
    "            'Mg ha−1 year−1','wt. %','K''men','%','kHz','MPa','mH','MW','orders of magnitude','year-old','m s−1','UT','hours','h','μL','weeks','discrete particles'\n",
    "            'byr','kR','s','mA g− 1','°S','min','mbsl','m−2','°C/min','W/m2','minutes','percentage points per year','nm','vol%','degrees'\n",
    "            'mg cm− 2','pH','keV','mA','ka','employees','fold per passage','mm','RRh','eV','ms','μg','kg s−1','w/w','item'\n",
    "            'μg/ml','μm','p0','g/L','horizons','V','Hz','SDG vertices','cm','items','percent','mg/L','ppm by mass','‰','g'\n",
    "            'Saturn radii RS'\n",
    "        ]\n",
    "\n",
    "    set_units = set(list_units)\n",
    "    print(set_units)\n",
    "\n",
    "    # IsApproximate IsCount IsRange IsList IsMean IsMedian IsMeanHasSD IsMeanHasTolerance IsRangeHasTolerance HasTolerance                 \n",
    "\n",
    "    revlist = sorted(list(set_units), reverse = True, key = len) # List of units\n",
    "    # print(revlist)\n",
    "\n",
    "    # we set non alphabet as the word boundary in our regex\n",
    "    listcounts = ['[^a-z]half[^a-z]', '[^a-z]quarter[^a-z]', '[^a-z]one[^a-z]', '[^a-z]two[^a-z]', '[^a-z]three[^a-z]', '[^a-z]four[^a-z]', '[^a-z]five[^a-z]', '[^a-z]six[^a-z]', '[^a-z]seven[^a-z]', '[^a-z]eight[^a-z]', '[^a-z]nine[^a-z]', '[^a-z]ten[^a-z]', \n",
    "                  '[^a-z]eleven[^a-z]', '[^a-z]twelve[^a-z]', '[^a-z]thirteen[^a-z]','[^a-z]fourteen[^a-z]','[^a-z]fifteen[^a-z]','[^a-z]sixteen[^a-z]','[^a-z]seventeen[^a-z]','[^a-z]eighteen[^a-z]','[^a-z]nineteen[^a-z]','[^a-z]twenty[^a-z]',\n",
    "                  '[^a-z]thirty[^a-z]','[^a-z]forty[^a-z]','[^a-z]fifty[^a-z]','[^a-z]sixty[^a-z]','[^a-z]seventy[^a-z]','[^a-z]eighty[^a-z]','[^a-z]ninety[^a-z]','[^a-z]hundred[^a-z]',\n",
    "                  '[^a-z]thousand[^a-z]','[^a-z]million[^a-z]','[^a-z]billion[^a-z]','[^a-z]trillion[^a-z]']\n",
    "\n",
    "    listcounts2 = []\n",
    "    listcounts3 = []\n",
    "    listcounts4 = []\n",
    "    for item in listcounts:\n",
    "        temp = \"^\"+item[6:]\n",
    "        listcounts2.append(temp)\n",
    "\n",
    "    # print(listcounts2)\n",
    "\n",
    "    for item in listcounts:\n",
    "        temp = item[0:len(item)-6] + \"$\"\n",
    "        listcounts3.append(temp)\n",
    "\n",
    "    # print(listcounts3)\n",
    "\n",
    "    for item in listcounts:\n",
    "        temp = \"^\"+item[6:len(item)-6]+\"$\"\n",
    "        listcounts4.append(temp)\n",
    "\n",
    "    # print(listcounts4)\n",
    "\n",
    "    listcounts.extend(listcounts2)\n",
    "    listcounts.extend(listcounts3)\n",
    "    listcounts.extend(listcounts4)\n",
    "    # print(listcounts)\n",
    "\n",
    "    listapproximate = ['∼','~', 'about', 'around', 'close to', 'the order of','approximately', 'nominally', 'near', 'roughly', 'almost', 'approximation',\n",
    "    '≈', 'circa']\n",
    "\n",
    "    listmean = ['average', 'mean']\n",
    "\n",
    "    listmedian = ['median']\n",
    "\n",
    "    listrange = ['to', 'from', 'below', 'beyond', 'above', 'between', 'up to', '<', '>', 'upper',\n",
    "    'greater', 'lesser', 'bigger', 'smaller', 'more than', 'less than', '≥', '≤', 'within',\n",
    "    'throughout', 'at least', 'or more', 'or less', 'past', 'higher', 'almost', 'high', 'before', 'after',\n",
    "    'over', 'under', 'range', 'ranging', 'top', 'at most', 'down to', '⩽', '⩾', '≳', 'as much as','±']\n",
    "\n",
    "    listhyphen = ['−', '-']\n",
    "\n",
    "    listlist = ['or', 'and']\n",
    "\n",
    "\n",
    "\n",
    "    '''\n",
    "    We set any non alphabet Character as the word boundary.\n",
    "    Since scientific document can have non-unicode\n",
    "    ''' \n",
    "\n",
    "\n",
    "    def findmodifier(sent, start_off, end_off):\n",
    "        Numpresent = False\n",
    "        IsUnit = False\n",
    "        IsApproximate = False\n",
    "        IsCount = True\n",
    "        IsRange = False\n",
    "        IsList = False \n",
    "        Mean = False\n",
    "        IsMean = False\n",
    "        IsMedian = False\n",
    "        Tolerance = False\n",
    "        Range = False\n",
    "        IsMeanHasSD = False\n",
    "        IsMeanHasTolerance = False\n",
    "        IsRangeHasTolerance = False\n",
    "        HasTolerance = False\n",
    "        Unit = \"\"\n",
    "\n",
    "        # We find if plus-minus exists\n",
    "\n",
    "\n",
    "        t_sent = sent[start_off:end_off]\n",
    "        print(\"Span is: \", t_sent)\n",
    "\n",
    "        if re.search(\"[^a-z][0-9]|^[0-9]\", t_sent.lower()) is not None:\n",
    "            Numpresent = True\n",
    "        else:\n",
    "            for item in listcounts:\n",
    "                if re.search(item, t_sent.lower()) is not None:\n",
    "                    Numpresent = True\n",
    "\n",
    "        if Numpresent is False:\n",
    "            IsCount = False\n",
    "            return [\"null\", IsApproximate, IsCount, IsRange, IsList, IsMean, IsMedian, IsMeanHasSD, IsMeanHasTolerance, IsRangeHasTolerance, HasTolerance]\n",
    "\n",
    "        for unit in revlist:\n",
    "            if re.search(\"[^a-zA-Z]\"+unit+\"[^a-zA-Z]\"+'|'+\"[^a-zA-Z]\"+unit+\"$\", sent[start_off:end_off]) is not None:\n",
    "                Unit+=unit\n",
    "                IsUnit = True\n",
    "                IsCount = False\n",
    "                break\n",
    "\n",
    "        if '±' in t_sent:\n",
    "            IsCount = False\n",
    "            Tolerance = True\n",
    "\n",
    "        for item in listmean:\n",
    "            tmp = re.search(\"[^a-z]\"+item+\"[^a-z]\"+'|'+\"[^a-z]\"+item+\"$\" +'|'+\"^\"+item+\"[^a-z]\", sent[start_off:end_off].lower())\n",
    "            if tmp is not None:\n",
    "                Mean = True\n",
    "\n",
    "\n",
    "        if re.search(\"[^a-z]\"+\"sd\"+\"[^a-z]\"+\"|\"+\"[^a-z]\"+\"sd\"+\"$\", sent[start_off-20:end_off].lower()) is True:\n",
    "            IsMeanHasSD = True\n",
    "\n",
    "        for item in listrange:\n",
    "            tmp = re.search(\"[^a-z]\"+item+\"[^a-z]\"+'|'+\"[^a-z]\"+item+\"$\" +'|'+\"^\"+item+\"[^a-z]\", t_sent.lower())# sent[max(0,start_off-10):end_off].lower())\n",
    "            if tmp is not None:\n",
    "                Range = True\n",
    "\n",
    "        tmp = re.search(\"[−|-]\", t_sent)\n",
    "        if tmp is not None:\n",
    "            tmp1 = re.search(\"[0-9]+\", sent[max(0,tmp.start()+start_off-10):tmp.start()+start_off])\n",
    "            if tmp1 is not None:\n",
    "                Range = True\n",
    "\n",
    "        if re.search('[^a-z]'+'median'+'[^a-z]'+'|'+'^'+'median'+'[^a-z]'+'|'+'[^a-z]'+'median'+'$', t_sent.lower()):\n",
    "            IsMedian = True\n",
    "\n",
    "        tmp = re.search(\"[^a-z]and[^a-z]|[^a-z]or[^a-z]\", t_sent.lower())\n",
    "        if tmp is not None:\n",
    "            tmp1 = re.search(\"[0-9]+\", sent[start_off+tmp.end():end_off])\n",
    "            tmp2 = re.search(\"[0-9]+\", sent[max(0,start_off-1):(tmp.start()+start_off)])\n",
    "            if tmp1 is not None and tmp2 is not None:\n",
    "                if Range is not True:\n",
    "                    IsList = True\n",
    "                IsCount = False\n",
    "\n",
    "\n",
    "        for item in listapproximate:\n",
    "            tmp = re.search(\"[^a-z]\"+item+\"[^a-z]\"+'|'+\"[^a-z]\"+item+\"$\" +'|'+\"^\"+item+\"[^a-z]\", sent[max(0,start_off-30):end_off].lower())\n",
    "            if tmp is not None:\n",
    "                IsApproximate = True\n",
    "\n",
    "        if Range and not Tolerance:\n",
    "            IsRange = True\n",
    "        if IsMean and Tolerance and not IsMeanHasSD:\n",
    "            IsMeanHasTolerance = True\n",
    "        if Tolerance and Range:\n",
    "            IsRangeHasTolerance = True\n",
    "        if Tolerance and not IsMeanHasSD and not IsMeanHasTolerance and not IsRangeHasTolerance:\n",
    "            HasTolerance = True\n",
    "\n",
    "        if IsUnit is True:\n",
    "            return [Unit, IsApproximate, IsCount, IsRange, IsList, IsMean, IsMedian, IsMeanHasSD, IsMeanHasTolerance, IsRangeHasTolerance, HasTolerance]\n",
    "        else:\n",
    "            return [\"null\", IsApproximate, IsCount, IsRange, IsList, IsMean, IsMedian, IsMeanHasSD, IsMeanHasTolerance, IsRangeHasTolerance, HasTolerance]\n",
    "\n",
    "\n",
    "    def convert_output_to_dict(preds):\n",
    "        label_dict = {}\n",
    "        if preds[0] != \"null\":\n",
    "            label_dict['unit'] = preds[0]\n",
    "\n",
    "        mods = []\n",
    "\n",
    "        mods_order_ = [\"IsApproximate\", \"IsCount\", \"IsRange\", \"IsList\", \"IsMean\", \"IsMedian\",\n",
    "                        \"IsMeanHasSD\", \"IsMeanHasTolerance\", \"IsRangeHasTolerance\",\"HasTolerance\"\n",
    "                    ]\n",
    "\n",
    "        for i, bool_value in enumerate(preds):\n",
    "            if i == 0:\n",
    "                pass\n",
    "            else:\n",
    "                if bool_value:\n",
    "                    mods.append(mods_order_[i-1])\n",
    "\n",
    "        if len(mods) > 0:\n",
    "            label_dict[\"mods\"] = mods\n",
    "\n",
    "        if len(label_dict) == 0:\n",
    "            return \"\"\n",
    "\n",
    "        return str(label_dict)\n",
    "\n",
    "    import json\n",
    "\n",
    "    def label_and_dump(text_path, quant_path, save_path, trial_or_train=False):\n",
    "        all_text = {f[:-4]: open(text_path + f, encoding=\"utf-8\", errors='replace').read() for f in os.listdir(text_path)}\n",
    "\n",
    "\n",
    "        if trial_or_train:\n",
    "            trim_tsv = lambda x: x[:-4] if x[-4:] == \".tsv\" else x\n",
    "            quant_labels = {trim_tsv(k):v for k, v in json.load(open(quant_path)).items()}\n",
    "        else:\n",
    "            quant_labels = json.load(open(quant_path))\n",
    "\n",
    "        '''\n",
    "        Quant_labels:\n",
    "        Dict{\n",
    "                Doc1_id: {sent1_offset: [list of quant offsets for sent1],\n",
    "                        sent2_offset: [list of quant offsets for sent2],\n",
    "                            ....\n",
    "                        }\n",
    "                Doc2_id ....\n",
    "                ....\n",
    "        }\n",
    "        Output:\n",
    "        Dict{\n",
    "                Doc1_id: [[docId, annotSet, annotType, startOffset, endOffset, annotId, text, other, sentence_start, sentence_end]\n",
    "                            repeat same for second quant identified\n",
    "                            ....\n",
    "                        ]\n",
    "                Doc2_id ....\n",
    "                ....\n",
    "        }\n",
    "        '''\n",
    "        assert set(sorted(all_text.keys())) == set(sorted(quant_labels.keys())), (quant_labels.keys(), \"   |||||    \", all_text.keys())\n",
    "\n",
    "        task2_output = {}\n",
    "        for docid in quant_labels.keys():\n",
    "            doc_txt = all_text[docid]\n",
    "            ann_id = 1\n",
    "            this_doc_labels = []\n",
    "            this_sents_offs = sorted([int(x) for x in list(quant_labels[docid].keys())])\n",
    "            for i, sent_offset in enumerate(this_sents_offs):\n",
    "                this_sentence_start = sent_offset\n",
    "                this_sentence_end = this_sents_offs[i+1] if (i +1) < len(this_sents_offs) else len(doc_txt)\n",
    "                sent = doc_txt[this_sentence_start:this_sentence_end]\n",
    "\n",
    "                sent_offset_str = str(sent_offset)\n",
    "                single_sent_quant_offs = quant_labels[docid][sent_offset_str]\n",
    "\n",
    "                for single_quant_offs in single_sent_quant_offs:\n",
    "                    others_val = convert_output_to_dict(findmodifier(sent, single_quant_offs[0], single_quant_offs[1]))\n",
    "\n",
    "                    this_offset = (single_quant_offs[0] + sent_offset, single_quant_offs[1] + sent_offset)\n",
    "                    this_doc_labels.append([docid, ann_id, \"Quantity\",\n",
    "                                            this_offset[0], this_offset[1], ann_id,\n",
    "                                            doc_txt[this_offset[0]:this_offset[1]],\n",
    "                                            others_val, this_sentence_start, this_sentence_end\n",
    "                                            ])\n",
    "                    ann_id += 1\n",
    "\n",
    "            task2_output[docid] = this_doc_labels\n",
    "\n",
    "        json.dump(task2_output, open(save_path, 'w+'), indent=2)\n",
    "\n",
    "    train_doc_path = data_filepath + \"train/text/\"\n",
    "    trial_doc_path = data_filepath + \"trial/txt/\"\n",
    "    test_doc_path = data_filepath + \"eval/text/\"\n",
    "    folder_name = output_filepath + bert_type.replace('/', '_') + '_' + datetime.today().strftime('%Y-%m-%d_%H_%M_%S')\n",
    "    if os.path.isdir(folder_name):\n",
    "        os.system(\"rm -rf \" + folder_name)\n",
    "    os.mkdir(folder_name)\n",
    "\n",
    "    print(\"\\n\\n\\n====================================================\")\n",
    "    print(\"====================== Train =======================\")\n",
    "    print(\"====================================================\\n\\n\\n\")\n",
    "\n",
    "\n",
    "    label_and_dump(train_doc_path, quant_filepath + \"train_spans.json\",\n",
    "                    os.path.join(folder_name, \"train_labels.json\"), True\n",
    "                )\n",
    "\n",
    "    print(\"\\n\\n\\n====================================================\")\n",
    "    print(\"======================= Trial ======================\")\n",
    "    print(\"====================================================\\n\\n\\n\")\n",
    "\n",
    "    label_and_dump(trial_doc_path, quant_filepath + \"trial_spans.json\",\n",
    "                    os.path.join(folder_name, \"trial_labels.json\"), True\n",
    "                )\n",
    "\n",
    "    print(\"\\n\\n\\n====================================================\")\n",
    "    print(\"======================= Test =======================\")\n",
    "    print(\"====================================================\\n\\n\\n\")\n",
    "\n",
    "    label_and_dump(test_doc_path, quant_filepath + \"test_spans.json\",\n",
    "                    os.path.join(folder_name + \"/test_labels.json\"), False\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e566a697",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'weeks', 'kgs-1', 'W/m2', 'min', 'womenstems/ha', 'mbar', 'μm', 'nT', 'mBar', 'mg', 'h', 'kg', 'hours', 'bit', '°C/min', 'yearRp', 'cm', 'cm2/Vs', 'SDG vertices', '°S', 'μbar', 'cm−1', 'ppt', 'mH', 'nm', 'cores', 'μL', 'pH', 'metre', 'mg/l', 'hr', 'MPa', 'kg s−1', 'km/h', 'mdeg', 'p0', 'ppq', 'H', 'm/s', 'mm3', 'lines of code', 'mm', 'ka', 's−1', 'clonesmen', 'mm per side', 'μM', 'Mg ha−1 year−1', 'M', 'beach materials', 'mA', 'Torr', 'ppm', 'μg/m', '° latitude', 'second', 'RRh', 'mm2', 'cm−3', 'Rs', 'year-old', 'yrs', 'wt.%times', 'minutes', 'g mol−1', 'm s−1', 'ms', 'MW', 'orders of magnitude', 'mbsl', 'kHz', 'participants', 'kV', 'μg', 'm thick', 'days', 'V', 'mWm−2', 'Whitehall II participants', '°N', 'vol%', 'items', 'μg/L', 'km s−1', 'horizons', 'elderly participants', 'mg/L', 'mg/ml', 'itemμg/ml', 'kW', 's', 'percentage points per year', 'U/ml', 'gSaturn radii RS', 'Mt', 'employees', 'month', 'centimeters', 'percent', 'K/min', 'Hz', 'nJ', 'kR', 'keV', 'w/w', 'UT', 'years', 'mMMbps', 'wt. %', 'day', 'GPa', 'scale heights', '×', 'mmol/L', 'nbar', 'Ma', 'g/m3', 'week', 'g/L', 'Å/s', 'hour', 'point', '‰', 'pairs per mm', 'vertices', 'months', 'wt%', '%', 'passages', '°C', 'monomer units', 'v/v', 'varves', 'occasions', 'degreesmg cm− 2', 'eV', 'W m−2', 'km', 'fold per passage', 'μs', 'AU', 'discrete particlesbyr', 'mA g− 1', 'discrete particles% per year', 'Kmen', 'cm3 g−1', 'g cm−3', 'bp', 'MeV', 'm−2', 'fold', 'ppm by mass', 'm2 g−1', 'mA/cm2', 'oN', 'm°'}\n",
      "Bert type: span_predictions_roberta-base_2022-07-13_01_48_39\n",
      "\n",
      "\n",
      "\n",
      "====================================================\n",
      "====================== Train =======================\n",
      "====================================================\n",
      "\n",
      "\n",
      "\n",
      "Span is:  (50 μg)\n",
      "Span is:  4 weeks\n",
      "Span is:  1 month\n",
      "Span is:  approximately 95%\n",
      "Span is:  1 month\n",
      "Span is:  4 weeks\n",
      "Span is:  20 years\n",
      "Span is:  than 0.1 g/L\n",
      "Span is:  less than 5%\n",
      "Span is:  650–750 W/m2\n",
      "Span is:  700 W/m2\n",
      "Span is:  25 °C\n",
      "Span is:  (∼0.4 mm2)\n",
      "Span is:  84 days\n",
      "Span is:  1 μbar\n",
      "Span is:  to at least 3Rp,\n",
      "Span is:  8000 K\n",
      "Span is:  3Rp,\n",
      "Span is:  7000 K\n",
      "Span is:  0.047 AU\n",
      "Span is:  1083 nm\n",
      "Span is:  30 m\n",
      "Span is:  56\n",
      "Span is:  55\n",
      "Span is:  73\n",
      "Span is:  40.6 MW\n",
      "Span is:  31%\n",
      "Span is:  30.9 MW)\n",
      "Span is:  48 h\n",
      "Span is:  0–10 cm\n",
      "Span is:  (~1kR\n",
      "Span is:  ~74° latitude,\n",
      "Span is:  ~80°,\n",
      "Span is:  ~88kR\n",
      "Span is:  ~79kR\n",
      "Span is:  (~7.85mWm−2)\n",
      "Span is:  ~10%\n",
      "Span is:  (~8.8mWm−2)\n",
      "Span is:  ~15kR\n",
      "Span is:  45°\n",
      "Span is:  0.23 H\n",
      "Span is:  0.52 H,\n",
      "Span is:  0.80 H,\n",
      "Span is:  1013 cm−3 to 1016 cm−3\n",
      "Span is:  10 min\n",
      "Span is:  200 °C\n",
      "Span is:  ∼50\n",
      "Span is:  (<20  ms),\n",
      "Span is:  more than 24 h\n",
      "Span is:  0.2p0\n",
      "Span is:  ≤1.5×1012 cm−3\n",
      "Span is:  4 weeks\n",
      "Span is:  6)\n",
      "Span is:  6 months\n",
      "Span is:  50,000\n",
      "Span is:  1/10\n",
      "Span is:  2 months,\n",
      "Span is:  3.5–4 months\n",
      "Span is:  1/100\n",
      "Span is:  5–10\n",
      "Span is:  between 8000 and 9000 K\n",
      "Span is:  109–1010 kg s−1\n",
      "Span is:  10–100%\n",
      "Span is:  less than ∼10 times\n",
      "Span is:  225\n",
      "Span is:  1.167\n",
      "Span is:  ±<0.1\n",
      "Span is:  289\n",
      "Span is:  24 h\n",
      "Span is:  93:7, v/v).\n",
      "Span is:  5%\n",
      "Span is:  >250 μm\n",
      "Span is:  35\n",
      "Span is:  0.4‰\n",
      "Span is:  9438 lines\n",
      "Span is:  7538 SD\n",
      "Span is:  almost 70%\n",
      "Span is:  10 wt%\n",
      "Span is:  20 wt%\n",
      "Span is:  0.296 μm\n",
      "Span is:  0.18 μm\n",
      "Span is:  −109 °C,\n",
      "Span is:  at least 3 passages\n",
      "Span is:  within 24 h\n",
      "Span is:  3.7 ± 0.9 fold\n",
      "Span is:  3.7 × 106 fold\n",
      "Span is:  > 99%\n",
      "Span is:  13\n",
      "Span is:  46.8% ± 1.6%\n",
      "Span is:  one-day\n",
      "Span is:  (>0.5 km)\n",
      "Span is:  <10 cm).\n",
      "Span is:  order\n",
      "Span is:  100 m\n",
      "Span is:  three\n",
      "Span is:  60%\n",
      "Span is:  36 clones\n",
      "Span is:  26 (72%)\n",
      "Span is:  20\n",
      "Span is:  100%\n",
      "Span is:  0.28–0.42)\n",
      "Span is:  0.39–0.41)\n",
      "Span is:  1200 K)\n",
      "Span is:  0.64 H\n",
      "Span is:  from 78% to 95%,\n",
      "Span is:  30\n",
      "Span is:  4\n",
      "Span is:  2\n",
      "Span is:  90–95%\n",
      "Span is:  4–8 years,\n",
      "Span is:  < 0.001)\n",
      "Span is:  (9.9%–16.2%;\n",
      "Span is:  7.01)\n",
      "Span is:  (25.4%),\n",
      "Span is:  (23.8%).\n",
      "Span is:  (16.2%,\n",
      "Span is:  9.9%\n",
      "Span is:  (7.5%),\n",
      "Span is:  (14.2%).\n",
      "Span is:  near 1.4Rp\n",
      "Span is:  2–3Rp\n",
      "Span is:  30 nbar\n",
      "Span is:  3800 K\n",
      "Span is:  1000 K\n",
      "Span is:  1 μbar\n",
      "Span is:  1300 K)\n",
      "Span is:  3.4Rp\n",
      "Span is:  2–3%\n",
      "Span is:  3 months\n",
      "Span is:  2 months\n",
      "Span is:  3.5–4 months\n",
      "Span is:  60%\n",
      "Span is:  <30%\n",
      "Span is:  >30%\n",
      "Span is:  2614.7 and 2619.6 m\n",
      "Span is:  5–6 days\n",
      "Span is:  Ten-day-\n",
      "Span is:  3–4 weeks,\n",
      "Span is:  (0.1 mg/ml)\n",
      "Span is:  (10 μg/m).\n",
      "Span is:  2-4 weeks\n",
      "Span is:  5–55 Hz\n",
      "Span is:  30–130 ms\n",
      "Span is:  95.81%\n",
      "Span is:  94.38%\n",
      "Span is:  96.24%\n",
      "Span is:  93.17%\n",
      "Span is:  1.16,\n",
      "Span is:  0.25)\n",
      "Span is:  1.31,\n",
      "Span is:  0.20).\n",
      "Span is:  48 h\n",
      "Span is:  13 passages (45 days\n",
      "Span is:  3).\n",
      "Span is:  3)\n",
      "Span is:  13 passages\n",
      "Span is:  2)\n",
      "Span is:  200 μm\n",
      "Span is:  eight-year\n",
      "Span is:  42–48%\n",
      "Span is:  15–27%\n",
      "Span is:  Two days\n",
      "Span is:  2 days\n",
      "Span is:  96\n",
      "Span is:  48\n",
      "Span is:  21\n",
      "Span is:  5 mm per\n",
      "Span is:  1 Hz\n",
      "Span is:  60 × 10 × 3 mm3\n",
      "Span is:  −100 °C to 200 °C\n",
      "Span is:  4 °C/min\n",
      "Span is:  0.725,\n",
      "Span is:  room\n",
      "Span is:  1.20 g/m3\n",
      "Span is:  20 °C\n",
      "Span is:  5Rp\n",
      "Span is:  3.9%, 8%,\n",
      "Span is:  5.8%\n",
      "Span is:  40%\n",
      "Span is:  5Rp\n",
      "Span is:  3.2%, 6.7%,\n",
      "Span is:  4.6%\n",
      "Span is:  .\n",
      "Span is:  .\n",
      "Span is:  8250 K\n",
      "Span is:  260 nm\n",
      "Span is:  215–225 nm\n",
      "Span is:  297 nm\n",
      "Span is:  ≈ − 10 mdeg)\n",
      "Span is:  12 h\n",
      "Span is:  864 s\n",
      "Span is:  62\n",
      "Span is:  3.95 Saturn\n",
      "Span is:  60,268 km)\n",
      "Span is:  252 km\n",
      "Span is:  (1979\n",
      "Span is:  (1980)\n",
      "Span is:  (1981)\n",
      "Span is:  July 2004,\n",
      "Span is:  14 times\n",
      "Span is:  2005 and 2010\n",
      "Span is:  100 mg/L\n",
      "Span is:  1 mg/L\n",
      "Span is:  10 mg/L\n",
      "Span is:  0.1 mg/L\n",
      "Span is:  (1%),\n",
      "Span is:  1, 5, 10, 20 and 50 μg/L\n",
      "Span is:  1%\n",
      "Span is:  Four-year-\n",
      "Span is:  13%\n",
      "Span is:  three\n",
      "Span is:  7\n",
      "Span is:  from 4 to 88 days\n",
      "Span is:  60 day\n",
      "Span is:  120 day\n",
      "Span is:  1.3 ± 0.5 wt.%\n",
      "Span is:  ∼0.5 wt.%\n",
      "Span is:  1000 times\n",
      "Span is:  ⩽1.6%\n",
      "Span is:  <1 ppm\n",
      "Span is:  35%\n",
      "Span is:  5%\n",
      "Span is:  around six orders of magnitude\n",
      "Span is:  7\n",
      "Span is:  0.563 m\n",
      "Span is:  0.025 m\n",
      "Span is:  55.8 Ma\n",
      "Span is:  5–8 °C\n",
      "Span is:  over <20 ka\n",
      "Span is:  between −2 and −7‰\n",
      "Span is:  170 ka\n",
      "Span is:  <−60‰)\n",
      "Span is:  < 0.01)\n",
      "Span is:  0–10 cm\n",
      "Span is:  9-year\n",
      "Span is:  < 0.01)\n",
      "Span is:  20–40 cm\n",
      "Span is:  689\n",
      "Span is:  85\n",
      "Span is:  12%\n",
      "Span is:  ∼5%\n",
      "Span is:  37\n",
      "Span is:  5%\n",
      "Span is:  ∼2%\n",
      "Span is:  below 3.5Rp\n",
      "Span is:  6 × 106 kg s−1\n",
      "Span is:  >107kgs-1\n",
      "Span is:  10−4\n",
      "Span is:  12 min,\n",
      "Span is:  2-min\n",
      "Span is:  approximately 60 eV\n",
      "Span is:  530 participants\n",
      "Span is:  50%\n",
      "Span is:  90 kW\n",
      "Span is:  82%\n",
      "Span is:  81%\n",
      "Span is:  20\n",
      "Span is:  four\n",
      "Span is:  less than one day\n",
      "Span is:  1–2 days\n",
      "Span is:  3–4 days\n",
      "Span is:  5–7 days).\n",
      "Span is:  0 and 60,\n",
      "Span is:  ≥16\n",
      "Span is:  274 White\n",
      "Span is:  89%\n",
      "Span is:  86%,\n",
      "Span is:  10%\n",
      "Span is:  (100 U/ml).\n",
      "Span is:  1 m\n",
      "Span is:  Up to 20 m\n",
      "Span is:  ∼−25.9 to −26.5‰\n",
      "Span is:  4‰\n",
      "Span is:  −27.2 to −23.1‰,\n",
      "Span is:  within <0.5 m\n",
      "Span is:  0.5–0.6 nJ\n",
      "Span is:  250 kHz\n",
      "Span is:  4–7 kV,\n",
      "Span is:  55 K\n",
      "Span is:  0.4%\n",
      "Span is:  2–4 kV\n",
      "Span is:  111\n",
      "Span is:  five\n",
      "Span is:  2–7%\n",
      "Span is:  10–15%.\n",
      "Span is:  up to 25%\n",
      "Span is:  from 0.1 to 1\n",
      "Span is:  from 2.6 km s−1 to 25 km s−1\n",
      "Span is:  100×\n",
      "Span is:  30 km s−1\n",
      "Span is:  0.1\n",
      "Span is:  1\n",
      "Span is:  4Rp\n",
      "Span is:  10,500×\n",
      "Span is:  25,000×;\n",
      "Span is:  2 μm\n",
      "Span is:  0.2 μm\n",
      "Span is:  20–30 cm\n",
      "Span is:  20 cm\n",
      "Span is:  12 weeks\n",
      "Span is:  > 10 nm;\n",
      "Span is:  > 0.5 cm3 g−1)\n",
      "Span is:  > 170 m2 g−1),\n",
      "Span is:  5–40 nm\n",
      "Span is:  1 (w/w),\n",
      "Span is:  15\n",
      "Span is:  120 m to 380 m\n",
      "Span is:  greater than 1000 stems/ha\n",
      "Span is:  10\n",
      "Span is:  34–38 °C\n",
      "Span is:  50%\n",
      "Span is:  less than 5 m/s\n",
      "Span is:  ±5%\n",
      "Span is:  ±70 m/s\n",
      "Span is:  (34 and 38 °C\n",
      "Span is:  (33 m)\n",
      "Span is:  20–25%\n",
      "Span is:  ∼56 Ma)\n",
      "Span is:  54 °N)\n",
      "Span is:  before (103 yrs)\n",
      "Span is:  60%\n",
      "Span is:  much\n",
      "Span is:  80%,\n",
      "Span is:  two\n",
      "Span is:  around 10%\n",
      "Span is:  around 40%\n",
      "Span is:  about 70 MPa,\n",
      "Span is:  about 210 MPa\n",
      "Span is:  210 MPa\n",
      "Span is:  0.8 m\n",
      "Span is:  approximately 1 cm)\n",
      "Span is:  Five\n",
      "Span is:  4.8 m\n",
      "Span is:  60 or 63 mm\n",
      "Span is:  8 m\n",
      "Span is:  three metre\n",
      "Span is:  approximately 11.8 m\n",
      "Span is:  11.8 m and 11.5 m\n",
      "Span is:  five\n",
      "Span is:  11.4% to 2.4%.\n",
      "Span is:  less than 2% or 30 vertices)\n",
      "Span is:  1.5 mm\n",
      "Span is:  three\n",
      "Span is:  30-item\n",
      "Span is:  1\n",
      "Span is:  0\n",
      "Span is:  5 or more\n",
      "Span is:  0 to 4\n",
      "Span is:  (84%)\n",
      "Span is:  (84%)\n",
      "Span is:  (73%)\n",
      "Span is:  (78%)\n",
      "Span is:  274 participants\n",
      "Span is:  58 to 70 in\n",
      "Span is:  80%\n",
      "Span is:  81%,\n",
      "Span is:  4.5 kg\n",
      "Span is:  4.5 kg\n",
      "Span is:  13 kg,\n",
      "Span is:  6 kg\n",
      "Span is:  <0.01 ppm\n",
      "Span is:  <2 ppm,\n",
      "Span is:  more than 10 passages\n",
      "Span is:  between 5 and 300 K\n",
      "Span is:  between 100 and 200 ms),\n",
      "Span is:  71\n",
      "Span is:  1/100th\n",
      "Span is:  5 g\n",
      "Span is:  0.05 g\n",
      "Span is:  >250 μm\n",
      "Span is:  (>90%)\n",
      "Span is:  between 20 and 60 g\n",
      "Span is:  63 μm\n",
      "Span is:  5‰\n",
      "Span is:  from 4 m\n",
      "Span is:  54 oN)\n",
      "Span is:  0.072 m\n",
      "Span is:  1.3 mm\n",
      "Span is:  50\n",
      "Span is:  0.01–0.03 Hz,\n",
      "Span is:  above 0.003 Hz\n",
      "Span is:  approximately 1 mm\n",
      "Span is:  5.16 m\n",
      "Span is:  34 °C\n",
      "Span is:  38 °C\n",
      "Span is:  38 °C\n",
      "Span is:  34 °C\n",
      "Span is:  less than 1%\n",
      "Span is:  greater than 10%\n",
      "Span is:  80%\n",
      "Span is:  several hundred or\n",
      "Span is:  more than a thousand years\n",
      "Span is:  3.00–3.05 ka\n",
      "Span is:  (55%)\n",
      "Span is:  (63%)\n",
      "Span is:  two\n",
      "Span is:  1.5 mm\n",
      "Span is:  8.5 mm\n",
      "Span is:  600 mm\n",
      "Span is:  1.5 mm\n",
      "Span is:  8.5 mm\n",
      "Span is:  62 mm\n",
      "Span is:  62 mm\n",
      "Span is:  1:10\n",
      "Span is:  1.00\n",
      "Span is:  100%\n",
      "Span is:  1/US\n",
      "Span is:  1\n",
      "Span is:  more than 90%\n",
      "Span is:  at least 5 hours\n",
      "Span is:  −4°C\n",
      "Span is:  72 hours\n",
      "Span is:  2 hours\n",
      "Span is:  75 g\n",
      "Span is:  1.4%–3.1%).\n",
      "Span is:  ≥7.0 mmol/L\n",
      "Span is:  2-hour\n",
      "Span is:  ≥11.1 mmol/L\n",
      "Span is:  5 minutes\n",
      "Span is:  (57°44’8.47”N; 1°50’26.59”E)\n",
      "Span is:  from 2605 m to 2634 m\n",
      "Span is:  2609–2613 m\n",
      "Span is:  from 1 to 25 per mm\n",
      "Span is:  26 horizons\n",
      "Span is:  approximately 13 pairs per mm\n",
      "Span is:  1.3 mm\n",
      "Span is:  8.4 mm,\n",
      "Span is:  approximately 50%\n",
      "Span is:  0.072 m\n",
      "Span is:  2.377 m,\n",
      "Span is:  8.4 mm\n",
      "Span is:  8.4 mm\n",
      "Span is:  1.3 mm\n",
      "Span is:  1.3 mm\n",
      "Span is:  8-year\n",
      "Span is:  67-year-old\n",
      "Span is:  64-year-old\n",
      "Span is:  One year\n",
      "Span is:  64-year-old\n",
      "Span is:  1 to 2 mg\n",
      "Span is:  to −18 °C,\n",
      "Span is:  −8 °C\n",
      "Span is:  3\n",
      "Span is:  10%\n",
      "Span is:  2 mM\n",
      "Span is:  50 μg/ml\n",
      "Span is:  110 μg/ml\n",
      "Span is:  10,000 U/ml\n",
      "Span is:  10,000 μg/ml\n",
      "Span is:  (1.0–1.7 wt.%\n",
      "Span is:  1.3 wt.%\n",
      "Span is:  ∼6.8 wt.%\n",
      "Span is:  (2.7 wt.%\n",
      "Span is:  few tens of centimeters\n",
      "Span is:  ∼4.4 wt.%\n",
      "Span is:  0.1%\n",
      "Span is:  3 months\n",
      "Span is:  10 min,\n",
      "Span is:  ±12 m\n",
      "Span is:  12 s\n",
      "Span is:  approximately 0.1 mg/l\n",
      "Span is:  50 mm\n",
      "Span is:  1:10\n",
      "Span is:  range 66% to 88%)\n",
      "Span is:  4.0 V\n",
      "Span is:  12 mA g− 1\n",
      "Span is:  1/20 (12 mA g− 1)–30C (7260 mA g− 1).\n",
      "Span is:  ) 1.3 (\n",
      "Span is:  ) 2.4,\n",
      "Span is:  ) 2.2 mg cm− 2\n",
      "Span is:  10 wt.%\n",
      "Span is:  99.5%\n",
      "Span is:  69 ± 3°,\n",
      "Span is:  −0.72 V,\n",
      "Span is:  99.9%\n",
      "Span is:  7.5–80\n",
      "Span is:  8–300\n",
      "Span is:  ∼1×10−5 Torr\n",
      "Span is:  300±15 °C\n",
      "Span is:  ∼0.5–5 Å/s\n",
      "Span is:  ∼1.5–3 Å/s\n",
      "Span is:  (20–25 monomer units,\n",
      "Span is:  ≈2.0 eV),\n",
      "Span is:  w~1500–2000 g mol−1\n",
      "Span is:  (68 – 70%),\n",
      "Span is:  (30%),\n",
      "Span is:  (30%)\n",
      "Span is:  (98%)\n",
      "Span is:  10 ms to 0.1 ms),\n",
      "Span is:  around 0.5 ms\n",
      "Span is:  80 vol%\n",
      "Span is:  70 nm\n",
      "Span is:  ∼2–3 mA/cm2\n",
      "Span is:  ∼0.40–0.45 V)\n",
      "Span is:  150 °C\n",
      "Span is:  up to 100%\n",
      "Span is:  between ∼01:25:00 UT and 01:25:40 UT,\n",
      "Span is:  20 °C,\n",
      "Span is:  111 MPa\n",
      "Span is:  63 MPa\n",
      "Span is:  20 wt%\n",
      "Span is:  18 cores\n",
      "Span is:  from 50% to around 80%.\n",
      "Span is:  1.5 mm\n",
      "Span is:  125 nm\n",
      "Span is:  1 h\n",
      "Span is:  one week\n",
      "Span is:  July 2002\n",
      "Span is:  5 Mt\n",
      "Span is:  1999,\n",
      "Span is:  approximately 220 m\n",
      "Span is:  1012 mbsl\n",
      "Span is:  3 years\n",
      "Span is:  from 211 K to 284 K\n",
      "Span is:  ∼35%),\n",
      "Span is:  ∼1000 K\n",
      "Span is:  1000–2000 K\n",
      "Span is:  approximately 54%\n",
      "Span is:  35 to 55,\n",
      "Span is:  20\n",
      "Span is:  1985 to 1988\n",
      "Span is:  73%,\n",
      "Span is:  10,308\n",
      "Span is:  6895\n",
      "Span is:  3413\n",
      "Span is:  approximately every 5 years\n",
      "Span is:  (1991 to 1993,\n",
      "Span is:  8815);\n",
      "Span is:  (1997 to 1999,\n",
      "Span is:  7870);\n",
      "Span is:  (2003 to 2004,\n",
      "Span is:  6967);\n",
      "Span is:  (2008 to 2009,\n",
      "Span is:  6761).\n",
      "Span is:  more\n",
      "Span is:  10 passages\n",
      "Span is:  10%\n",
      "Span is:  8 days\n",
      "Span is:  4 days\n",
      "Span is:  43.57% ± 4.35%\n",
      "Span is:  3)\n",
      "Span is:  43.22% ± 7.13%\n",
      "Span is:  3\n",
      "Span is:  0.0307,\n",
      "Span is:  3)\n",
      "Span is:  2%\n",
      "Span is:  metre\n",
      "Span is:  (200,000\n",
      "Span is:  1 kg)\n",
      "Span is:  (5.9 million\n",
      "Span is:  1 kg)\n",
      "Span is:  0.1\n",
      "Span is:  10%\n",
      "Span is:  three\n",
      "Span is:  1.0 kg\n",
      "Span is:  6.35 kg\n",
      "Span is:  12 kg\n",
      "Span is:  35 m/s (126 km/h)\n",
      "Span is:  4.1,\n",
      "Span is:  0.03).\n",
      "Span is:  20%\n",
      "Span is:  < 0.01).\n",
      "Span is:  1.75,\n",
      "Span is:  70 or higher\n",
      "Span is:  2-year\n",
      "Span is:  6 years\n",
      "Span is:  85%\n",
      "Span is:  15%\n",
      "Span is:  ∼70%\n",
      "Span is:  52.1%\n",
      "Span is:  54\n",
      "Span is:  7\n",
      "Span is:  54\n",
      "Span is:  26\n",
      "Span is:  26\n",
      "Span is:  6 and 13 kg\n",
      "Span is:  7-month\n",
      "Span is:  7 month\n",
      "Span is:  9\n",
      "Span is:  4.8 months\n",
      "Span is:  25%\n",
      "Span is:  7.6%\n",
      "Span is:  0.097 nm\n",
      "Span is:  .4–1.0 nm\n",
      "Span is:  0.19–0.21 nm\n",
      "Span is:  3 months\n",
      "Span is:  approximately 19 m\n",
      "Span is:  1991;\n",
      "Span is:  1997, 2003,\n",
      "Span is:  2008;\n",
      "Span is:  5318 participants\n",
      "Span is:  54.8 years, 31%\n",
      "Span is:  ≥16,\n",
      "Span is:  10%\n",
      "Span is:  (<200 nm)\n",
      "Span is:  3.4+0.1 cm2/Vs,\n",
      "Span is:  up to 100%\n",
      "Span is:  200 days\n",
      "Span is:  two\n",
      "Span is:  54.3%\n",
      "Span is:  14.3%, 82.9%\n",
      "Span is:  57.1%,\n",
      "Span is:  67.9%\n",
      "Span is:  0%,\n",
      "Span is:  0.02).\n",
      "Span is:  3.19 ± 0.10 GPa\n",
      "Span is:  1.96 ± 0.08 GPa\n",
      "Span is:  20 wt%\n",
      "Span is:  192 s\n",
      "Span is:  15 or less\n",
      "Span is:  9th August 2008\n",
      "Span is:  05:28 UT and 05:47 UT\n",
      "Span is:  between 200 and >1000 m\n",
      "Span is:  >0.5 km\n",
      "Span is:  (52%)\n",
      "Span is:  11%.\n",
      "Span is:  7 to 20%\n",
      "Span is:  10 year\n",
      "Span is:  over 48 years\n",
      "Span is:  13%\n",
      "Span is:  5%\n",
      "Span is:  6 November 2011 UT,\n",
      "Span is:  two\n",
      "Span is:  30 min\n",
      "Span is:  34 °C\n",
      "Span is:  38 °C\n",
      "Span is:  between 34 °C and 38 °C\n",
      "Span is:  < 9 m\n",
      "Span is:  greater than 10%\n",
      "Span is:  better than 5%\n",
      "Span is:  95%\n",
      "Span is:  274 elderly\n",
      "Span is:  almost 90%\n",
      "Span is:  approximately 80%\n",
      "Span is:  65 years\n",
      "Span is:  ∼400eV\n",
      "Span is:  ∼10eV\n",
      "Span is:  around 100 km,\n",
      "Span is:  400 km\n",
      "Span is:  approximately 78 ± 0.6%\n",
      "Span is:  approximately 88 ± 1.1%\n",
      "Span is:  approximately 29 ± 0.9%\n",
      "Span is:  approximately 20 μg/ml\n",
      "Span is:  1 m s−1\n",
      "Span is:  between 0 and 0.5 m s−1\n",
      "Span is:  greater than 20 m s−1\n",
      "Span is:  2%\n",
      "Span is:  24 hr\n",
      "Span is:  100%.\n",
      "Span is:  1/e\n",
      "Span is:  0.51–0.85 m\n",
      "Span is:  1.5–2.6 m\n",
      "Span is:  ∼2 m\n",
      "Span is:  1 g cm−3\n",
      "Span is:  10%\n",
      "Span is:  2.84\n",
      "Span is:  95%\n",
      "Span is:  1.66–4.88)\n",
      "Span is:  2.25\n",
      "Span is:  95%\n",
      "Span is:  1.92–2.65)\n",
      "Span is:  1.34\n",
      "Span is:  95%\n",
      "Span is:  1.11–1.62)\n",
      "Span is:  2.14\n",
      "Span is:  95%\n",
      "Span is:  1.85–2.48),\n",
      "Span is:  1.37).\n",
      "Span is:  before 1200 °C\n",
      "Span is:  1.67)\n",
      "Span is:  3–4 nT\n",
      "Span is:  1.5 mm\n",
      "Span is:  8.5 mm\n",
      "Span is:  (10–30%,\n",
      "Span is:  63%\n",
      "Span is:  100%\n",
      "Span is:  3–4\n",
      "Span is:  −0.43 ± 0.05 percentage points per year\n",
      "Span is:  −1.6 ± 0.2%\n",
      "Span is:  0.02 cm s−1\n",
      "Span is:  zero\n",
      "Span is:  10 μm\n",
      "Span is:  873 varves\n",
      "Span is:  10.5 m\n",
      "Span is:  22%\n",
      "Span is:  48.4 MW to 59.0 MW\n",
      "Span is:  180 m).\n",
      "Span is:  7.83 m s−1\n",
      "Span is:  4 nm\n",
      "Span is:  2.5–10 μm\n",
      "Span is:  less than 2.5 μm\n",
      "Span is:  30%\n",
      "Span is:  (~ 30%\n",
      "Span is:  37%)\n",
      "Span is:  ~ 60–80%\n",
      "Span is:  35%\n",
      "Span is:  22%\n",
      "Span is:  10%\n",
      "Span is:  10%\n",
      "Span is:  less than a few percent\n",
      "Span is:  a few percent to around thirty percent\n",
      "Span is:  thirty percent\n",
      "Span is:  80%\n",
      "Span is:  80%\n",
      "Span is:  40%,\n",
      "Span is:  1.5 mm\n",
      "Span is:  (∼33.7 Ma,\n",
      "Span is:  up to 40%\n",
      "Span is:  0.6‰\n",
      "Span is:  1.5‰\n",
      "Span is:  ∼31.5 Ma,\n",
      "Span is:  30-degrees\n",
      "Span is:  60 μm\n",
      "Span is:  20 × 20 degrees\n",
      "Span is:  4.2 mm\n",
      "Span is:  30 degrees\n",
      "Span is:  4.5 kg, 6 kg\n",
      "Span is:  13 kg\n",
      "Span is:  8.5 mm\n",
      "Span is:  1980 mm,\n",
      "Span is:  2780 mm\n",
      "Span is:  14 days\n",
      "Span is:  125 μm\n",
      "Span is:  25 μm\n",
      "Span is:  1 m,\n",
      "Span is:  1000\n",
      "Span is:  1000 m−2)\n",
      "Span is:  from 10% to 100%\n",
      "Span is:  less than 85%,\n",
      "Span is:  more than 85%.\n",
      "Span is:  0.25 Mbps to 0.01 Mbps).\n",
      "Span is:  within 100 m\n",
      "Span is:  85/15%\n",
      "Span is:  above 1000 °C\n",
      "Span is:  1.67)\n",
      "Span is:  upto 1200 °C\n",
      "Span is:  800 °C\n",
      "Span is:  5-year\n",
      "Span is:  12.1%\n",
      "Span is:  4.6%\n",
      "Span is:  2.1%\n",
      "Span is:  5.4%\n",
      "Span is:  9.2%\n",
      "Span is:  47.4%\n",
      "Span is:  .64,\n",
      "Span is:  < .001).\n",
      "Span is:  59.9%\n",
      "Span is:  20\n",
      "Span is:  0.06–0.07 mBar\n",
      "Span is:  240 s\n",
      "Span is:  30 mA\n",
      "Span is:  1.5 nm\n",
      "Span is:  20 kV)\n",
      "Span is:  5 K/min,\n",
      "Span is:  about 30 K/min\n",
      "Span is:  65,\n",
      "Span is:  65 years\n",
      "Span is:  up to 11.25 Mg ha−1 year−1),\n",
      "Span is:  0.24).\n",
      "Span is:  0–10 cm\n",
      "Span is:  0.18,\n",
      "Span is:  0.01).\n",
      "Span is:  0–40 cm\n",
      "Span is:  +1%.\n",
      "Span is:  1065–1280 cm−1\n",
      "Span is:  about 964 cm−1\n",
      "Span is:  1031 cm−1\n",
      "Span is:  1091 cm−1\n",
      "Span is:  50%\n",
      "Span is:  3.5–6.1 (×10−4) wt.%\n",
      "Span is:  3 byr\n",
      "Span is:  1.5–2.6 m\n",
      "Span is:  six\n",
      "Span is:  1.3 mm\n",
      "Span is:  5.4 mm\n",
      "Span is:  8.4 mm\n",
      "Span is:  100 μm).\n",
      "Span is:  12 hr\n",
      "Span is:  298 K\n",
      "Span is:  25 days\n",
      "Span is:  least 10%\n",
      "Span is:  200 years\n",
      "Span is:  95%\n",
      "Span is:  10%\n",
      "Span is:  90%\n",
      "Span is:  .05,\n",
      "Span is:  1.14\n",
      "Span is:  10%\n",
      "Span is:  1.40\n",
      "Span is:  1.24,\n",
      "Span is:  5–13% per year\n",
      "Span is:  (320–500 K\n",
      "Span is:  above ∼500 K\n",
      "Span is:  3\n",
      "Span is:  200 μm\n",
      "Span is:  50 μm\n",
      "Span is:  100 μm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Span is:  40 Hz\n",
      "Span is:  2.30\n",
      "Span is:  95%\n",
      "Span is:  1.03–5.13),\n",
      "Span is:  8.5 mm\n",
      "Span is:  1.5 mm\n",
      "Span is:  1.5 mm\n",
      "Span is:  0.5 Hz\n",
      "Span is:  1.8 cm\n",
      "Span is:  greater than 70%.\n",
      "Span is:  1.5 mm\n",
      "Span is:  8.5 mm\n",
      "Span is:  8.5 mm\n",
      "Span is:  8.5 mm\n",
      "Span is:  50 μm);\n",
      "Span is:  200 μm).\n",
      "Span is:  51-bp\n",
      "Span is:  about 40%\n",
      "Span is:  9 and 21\n",
      "Span is:  6 weeks\n",
      "Span is:  2.5 years\n",
      "Span is:  115%\n",
      "Span is:  169%\n",
      "Span is:  0.001)\n",
      "Span is:  50 μm\n",
      "Span is:  7200 K\n",
      "Span is:  12.4%\n",
      "Span is:  0.13%\n",
      "Span is:  10 cm\n",
      "Span is:  20–40 cm\n",
      "Span is:  0–10 cm\n",
      "Span is:  1.5418 Å\n",
      "Span is:  5° to 85°,\n",
      "Span is:  0.1°\n",
      "Span is:  4 s\n",
      "Span is:  0.9).\n",
      "Span is:  0.64%\n",
      "Span is:  26 October 2011,\n",
      "Span is:  0.02%\n",
      "Span is:  December 16\n",
      "Span is:  48 hr\n",
      "Span is:  10%\n",
      "Span is:  about −100 °C\n",
      "Span is:  147 patients\n",
      "Span is:  before 6 months\n",
      "Span is:  before 9 months\n",
      "Span is:  8.5 mm\n",
      "Span is:  1180 and 1980 mm\n",
      "Span is:  4 weeks\n",
      "\n",
      "\n",
      "\n",
      "====================================================\n",
      "======================= Trial ======================\n",
      "====================================================\n",
      "\n",
      "\n",
      "\n",
      "Span is:  2005–2010\n",
      "Span is:  14 times\n",
      "Span is:  25 km\n",
      "Span is:  27 keV to 21 MeV\n",
      "Span is:  from 74°S to 90°S\n",
      "Span is:  490 K,\n",
      "Span is:  430 K\n",
      "Span is:  60 K\n",
      "Span is:  7 (1-bit)\n",
      "Span is:  5\n",
      "Span is:  −1\n",
      "Span is:  10%,\n",
      "Span is:  1−1/nc\n",
      "Span is:  less than Δ nodes\n",
      "Span is:  1−1/nc\n",
      "Span is:  0.2 M\n",
      "Span is:  100 °C\n",
      "Span is:  40 min\n",
      "Span is:  up to 1 week\n",
      "Span is:  ∼2\n",
      "Span is:  0.2 M\n",
      "Span is:  ∼3500\n",
      "Span is:  5%\n",
      "Span is:  95%).\n",
      "Span is:  six to eight\n",
      "Span is:  twenty eight-second\n",
      "Span is:  (1.26±0.2‰\n",
      "Span is:  1.23±0.25‰\n",
      "Span is:  104\n",
      "Span is:  3 byr\n",
      "Span is:  ∼2 m\n",
      "Span is:  0.1%\n",
      "Span is:  1.0–1.7 wt.%\n",
      "Span is:  0.2–0.4 wt.%\n",
      "Span is:  0.4–0.6 wt.%\n",
      "Span is:  1–5Rp\n",
      "Span is:  93%\n",
      "Span is:  50 eV\n",
      "Span is:  0.1\n",
      "Span is:  about 0.1–0.2 s−1\n",
      "Span is:  5–10 s,\n",
      "Span is:  magnitude 10RRh\n",
      "Span is:  ∼33.7 Ma\n",
      "Span is:  between 60% and 100%\n",
      "Span is:  ∼400 ka,\n",
      "Span is:  million-year\n",
      "Span is:  ∼2 Ma\n",
      "Span is:  top 100 m\n",
      "Span is:  from 103 yrs\n",
      "Span is:  above 2618 m\n",
      "Span is:  about 20 keV\n",
      "Span is:  several tens of Me\n",
      "Span is:  within 86 s\n",
      "Span is:  2005\n",
      "Span is:  77.45°\n",
      "Span is:  16 times\n",
      "Span is:  (5.65 s)\n",
      "Span is:  0.66 s\n",
      "Span is:  86 s\n",
      "Span is:  2618 m\n",
      "Span is:  40°N),\n",
      "Span is:  around 40%\n",
      "Span is:  55–74 year\n",
      "Span is:  2619.60, 2617.35, 2617.44, 2614.73,\n",
      "Span is:  2614.71 m\n",
      "Span is:  (2619.60, 2614.73,\n",
      "Span is:  2614.71 m)\n",
      "Span is:  2619.60\n",
      "Span is:  2614.71\n",
      "Span is:  50%\n",
      "Span is:  11,500 K\n",
      "Span is:  1.5Rp\n",
      "Span is:  0.3 nbar).\n",
      "Span is:  from 0.1 to 1,\n",
      "Span is:  1.4Rp (0.5 nbar)\n",
      "Span is:  1.9Rp (0.1 nbar)\n",
      "Span is:  from 10,000 K to 13,200 K\n",
      "Span is:  1\n",
      "Span is:  17.7 m-\n",
      "Span is:  (∼12 m)\n",
      "Span is:  (∼12.6 m)\n",
      "Span is:  1 to 20 cm\n",
      "Span is:  93.90±0.15 Ma\n",
      "Span is:  2°\n",
      "Span is:  10°,\n",
      "Span is:  0.4\n",
      "Span is:  5 s\n",
      "Span is:  500\n",
      "Span is:  (78°)\n",
      "Span is:  0°).\n",
      "Span is:  around 1000 km\n",
      "Span is:  8250 K\n",
      "Span is:  1 μbar\n",
      "Span is:  0.5\n",
      "Span is:  5–10 times\n",
      "Span is:  13 kg\n",
      "Span is:  <2 ppm\n",
      "Span is:  99.9%).\n",
      "Span is:  1:3\n",
      "Span is:  1:1\n",
      "Span is:  2 m\n",
      "Span is:  4.3 × 10−8 wt\n",
      "Span is:   %\n",
      "Span is:  \n",
      "Span is:  orders\n",
      "Span is:  0.4–0.6 wt\n",
      "Span is:  ∼1:60\n",
      "Span is:  ∼1:1000\n",
      "Span is:  2632 to 2618 m\n",
      "Span is:  5%\n",
      "Span is:  10 min\n",
      "Span is:  10 μm\n",
      "Span is:  four and a half\n",
      "Span is:  1947 and 1999\n",
      "Span is:  between 1% and 37%\n",
      "Span is:  27 days\n",
      "Span is:  9 ± 6%.\n",
      "Span is:  ∼10–11 days\n",
      "Span is:  three\n",
      "Span is:  approximately 2 h\n",
      "Span is:  10 μM\n",
      "Span is:  15 min\n",
      "Span is:  1540 cm−1\n",
      "Span is:  1550 cm−1\n",
      "Span is:  1536 cm−1\n",
      "Span is:  1540 cm−1\n",
      "Span is:  1485 cm−1\n",
      "Span is:  1260 cm−1\n",
      "Span is:  1423 cm−1\n",
      "Span is:  10%\n",
      "Span is:  about 5 km s−1,\n",
      "Span is:  two\n",
      "Span is:  ±10 min\n",
      "Span is:  (56–100 keV)\n",
      "Span is:  around 1300 m s−1 near 82° latitude\n",
      "Span is:  ∼1600 m s−1 near 78°\n",
      "Span is:  ∼30°N\n",
      "Span is:  ∼20 m\n",
      "Span is:  24 m\n",
      "Span is:  4)\n",
      "Span is:  2\n",
      "Span is:  eight\n",
      "Span is:  ∼20 to 40 ppm\n",
      "Span is:  (∼15 ppm)\n",
      "Span is:  eight\n",
      "Span is:  five\n",
      "Span is:  ∼20 ppm to ∼180 ppm\n",
      "Span is:  from ∼250 to ∼550 ppm\n",
      "Span is:  from ∼200 to 400 ppm\n",
      "Span is:  magnitude\n",
      "Span is:  eight\n",
      "Span is:  five\n",
      "Span is:  ∼50 ppm to ∼150 ppm\n",
      "Span is:  50%,\n",
      "Span is:  11 km s−1\n",
      "Span is:  5Rp\n",
      "Span is:  less than 6 K\n",
      "Span is:  2–3‰\n",
      "Span is:  ∼−27‰\n",
      "Span is:  4.3 m\n",
      "Span is:  ∼−27‰\n",
      "Span is:  10 μM\n",
      "Span is:  7–8\n",
      "Span is:  10%\n",
      "Span is:  3.91 nm\n",
      "Span is:  500 nm\n",
      "Span is:  500 nm\n",
      "Span is:  below ∼460 K\n",
      "Span is:  440 ± 50 K\n",
      "Span is:  (563–624) ± 30 K\n",
      "Span is:  to around 650 K\n",
      "Span is:  650 K\n",
      "Span is:  ⩽ 650 K)\n",
      "Span is:  > 650 K).\n",
      "Span is:  5≤2θ/°≤80\n",
      "Span is:  0.02°\n",
      "Span is:  50 min\n",
      "Span is:  1RRh\n",
      "Span is:  764 km).\n",
      "Span is:  about 8.74Rs\n",
      "Span is:  60,268 km).\n",
      "Span is:  25 ± 0.02 °C,\n",
      "Span is:  than 0.1 ppm\n",
      "Span is:  0.06 to 0.42 to 0.74 ppm\n",
      "Span is:  4.5\n",
      "Span is:  6\n",
      "Span is:  13 kg\n",
      "Span is:  <0.1 ppm\n",
      "Span is:  3 mbar\n",
      "Span is:  from 1300 K to 3500 K\n",
      "Span is:  1 μbar\n",
      "Span is:  3 mbar\n",
      "Span is:  around 450 K\n",
      "Span is:  around 180 K\n",
      "Span is:  420 ± 30 K near 29.5°N\n",
      "Span is:  488 ± 14 K\n",
      "Span is:  450 K,\n",
      "Span is:  1 μbar\n",
      "Span is:  about 8250 K\n",
      "Span is:  below 3Rp\n",
      "Span is:  approximately 6000 K\n",
      "Span is:  11,000 K\n",
      "Span is:  ∼0.8,\n",
      "Span is:  10 ppq\n",
      "Span is:  0.13\n",
      "Span is:  (1 to 600 ppt\n",
      "Span is:  from 1 to 30 ppt\n",
      "Span is:  30 ppt\n",
      "Span is:  75%\n",
      "Span is:  (25%)\n",
      "Span is:  (100 ppt)\n",
      "Span is:  20 times\n",
      "Span is:  350\n",
      "Span is:  1%\n",
      "Span is:  1 keV\n",
      "Span is:  >100 μm\n",
      "Span is:  between 50 and 100\n",
      "Span is:  200 μL\n",
      "Span is:  4.5 kg\n",
      "Span is:  6 kg\n",
      "Span is:  13 kg\n",
      "Span is:  30 nbar\n",
      "Span is:  1000 K,\n",
      "Span is:  20 eV\n",
      "Span is:  0.45 W m−2\n",
      "Span is:  0 ppm\n",
      "Span is:  1000 ppm\n",
      "Span is:  2000 ppm,\n",
      "Span is:  13 kg\n",
      "Span is:  4.5 kg,\n",
      "Span is:  22\n",
      "Span is:  0.22 ppm\n",
      "Span is:  13 kg\n",
      "Span is:  <1 ppm\n",
      "Span is:  6 kg over 4.5 kg,\n",
      "Span is:  6 kg to 13 kg\n",
      "Span is:  <0.1 ppm\n",
      "Span is:  sixteen times\n",
      "Span is:  ∼5.7 s).\n",
      "Span is:  589.3 nm\n",
      "Span is:  four\n",
      "Span is:  300–800 nm\n",
      "Span is:  3 mbar\n",
      "Span is:  30ε\n",
      "Span is:  ∼−1‰\n",
      "Span is:  up to 40%\n",
      "Span is:  12–20 μm\n",
      "Span is:  between 2 and 20 μm\n",
      "Span is:  0.92\n",
      "Span is:  ∼–1‰\n",
      "Span is:  2–10 μm\n",
      "Span is:  10–20 μm\n",
      "Span is:  1000 and 2000 ppm\n",
      "Span is:  from 0 to 1000 ppm\n",
      "Span is:  1000 ppm\n",
      "Span is:  0 and 2000 ppm,\n",
      "Span is:  1000 ppm\n",
      "Span is:  (135 ppm\n",
      "Span is:  407 ppm\n",
      "Span is:  0\n",
      "Span is:  2000 ppm\n",
      "Span is:  to 10 ppm\n",
      "Span is:  from 9.4 to 14.1 to 66.8 ppm\n",
      "Span is:  0, 1000\n",
      "Span is:  2000 ppm\n",
      "Span is:  near 1 μbar\n",
      "Span is:  about 8250 K,\n",
      "Span is:  above 3Rp\n",
      "Span is:  fewer than 20 specimens\n",
      "Span is:  0% to 80%)\n",
      "Span is:  0\n",
      "Span is:  7.2Rp\n",
      "Span is:  7.2 km s−1\n",
      "Span is:  approximately 7100 K\n",
      "Span is:  7100 K,\n",
      "Span is:  16\n",
      "Span is:  8\n",
      "Span is:  0.9mH\n",
      "Span is:  1 g cm−3,\n",
      "Span is:  1.5–2.6 m\n",
      "Span is:  between 6% and 13%,\n",
      "Span is:  6–8%\n",
      "Span is:  <1 ppm,\n",
      "Span is:  0.63\n",
      "Span is:  0.51 ppm\n",
      "Span is:  1000 ppm\n",
      "Span is:  0 and 2000 ppm\n",
      "Span is:  1 μs)\n",
      "Span is:  90–95%\n",
      "Span is:  4.5 kg\n",
      "Span is:  6 kg\n",
      "Span is:  13 kg\n",
      "\n",
      "\n",
      "\n",
      "====================================================\n",
      "======================= Test =======================\n",
      "====================================================\n",
      "\n",
      "\n",
      "\n",
      "Span is:  (\n",
      "Span is:  panel\n",
      "Span is:  P\n",
      "Span is:  10.\n",
      "Span is:  10.\n",
      "Span is:  11.\n",
      "Span is:  and (\n",
      "Span is:  10(\n",
      "Span is:  .\n",
      "Span is:  19.\n",
      "Span is:  P.\n",
      "Span is:  are usually\n",
      "Span is:  raction (N\n",
      "Span is:  99.\n",
      "Span is:  (\n",
      "Span is:  10]\n",
      "Span is:  .\n",
      "Span is:  P\n",
      "Span is:  .\n",
      "Span is:  .\n",
      "Span is:  and P\n",
      "Span is:  .\n",
      "Span is:  and\n",
      "Span is:  and\n",
      "Span is:  and\n"
     ]
    }
   ],
   "source": [
    "extract_modifiers(data_filepath=\"../data/raw/\", output_filepath= \"../outputs/modifiers_\", quant_filepath=\"../outputs/span_predictions_roberta-base_2022-07-13_01_48_39/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9b821f7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x20c62bca830>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(434)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "38199646",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Train one epoch ####\n",
    "def train(model, dataset, criterion, epoch_num):\n",
    "    # Put the model to train\n",
    "    model.train()\n",
    "\n",
    "    # Lets keep track of the losses at each update\n",
    "    train_losses = []\n",
    "    num_batch = 0\n",
    "\n",
    "    loss_scaling = {'me': 1.0, 'mp': 1.0, 'qual': 1.0}\n",
    "    print(\"Loss Scale:\", loss_scaling)\n",
    "    \n",
    "    for batch in dataset.batched_dataset:\n",
    "        # Unpack the batch\n",
    "        (texts, labels, att_masks, _, _, _, _) = batch\n",
    "\n",
    "        preds = model(texts, att_masks, labels['QUANT'])\n",
    "\n",
    "        # Take into account padded while calculating loss\n",
    "        loss = 0\n",
    "        for single_task, single_task_preds in preds.items():\n",
    "            loss_unreduced = criterion(single_task_preds.permute(0,2,1), labels[single_task.upper()])\n",
    "            loss += ((loss_unreduced * att_masks).sum() / (att_masks).sum()) * loss_scaling[single_task]\n",
    "        loss.backward()\n",
    "\n",
    "        # Update model weights\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if num_batch % 10 == 0:\n",
    "            print(\"Train loss at {}:\".format(num_batch), loss.item())\n",
    "\n",
    "        num_batch += 1\n",
    "        train_losses.append(loss.item())\n",
    "\n",
    "    return np.average(train_losses)\n",
    "\n",
    "def evaluate(model, tasks, dataset, criterion, epoch_num):\n",
    "    # Put the model to eval mode\n",
    "    model.eval()\n",
    "\n",
    "    # Keep track of predictions\n",
    "    valid_losses = []\n",
    "    predicts, gnd_truths = {}, {}\n",
    "    if 'me' in tasks:\n",
    "        predicts['me'] = []\n",
    "        gnd_truths['me'] = []\n",
    "    if 'mp' in tasks:\n",
    "        predicts['mp'] = []\n",
    "        gnd_truths['mp'] = []\n",
    "    if 'qual' in tasks:\n",
    "        predicts['qual'] = []\n",
    "        gnd_truths['qual'] = []\n",
    "\n",
    "    loss_scaling = {'me': 1.0, 'mp': 1.0, 'qual': 1.0}\n",
    "    print(\"Loss Scale:\", loss_scaling)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in dataset.batched_dataset:\n",
    "            # Unpack the batch\n",
    "            (texts, labels, att_masks, _, _, _, _) = batch\n",
    "            # Make predictions on the model\n",
    "            preds = model(texts, att_masks, labels['QUANT'])\n",
    "\n",
    "\n",
    "            # Take into account padded while calculating loss\n",
    "            loss = 0\n",
    "            for single_task, single_task_preds in preds.items():\n",
    "                loss_unreduced = criterion(single_task_preds.permute(0,2,1), labels[single_task.upper()])\n",
    "                loss += ((loss_unreduced * att_masks).sum() / (att_masks).sum()) * loss_scaling[single_task]\n",
    "\n",
    "            assert preds.keys() == predicts.keys()\n",
    "            assert preds.keys() == gnd_truths.keys()\n",
    "            \n",
    "            # Get argmax of non-padded tokens\n",
    "            for task in preds.keys():\n",
    "                for sent_preds, sent_labels, sent_att_masks in zip(preds[task], labels[task.upper()], att_masks):\n",
    "                    for token_preds, token_labels, token_masks in zip(sent_preds, sent_labels, sent_att_masks):\n",
    "                        if token_masks.item() != 0:\n",
    "                            predicts[task].append(token_preds.argmax().item())\n",
    "                            gnd_truths[task].append((token_labels.item()))\n",
    "            valid_losses.append(loss.item())\n",
    "\n",
    "            assert len(predicts) == len(gnd_truths)\n",
    "\n",
    "    # Create confusion matrix and evaluate on the predictions\n",
    "    for task in preds.keys():\n",
    "        print(\"\\nTask:\", task)\n",
    "        target_names = [\"NOT_\" + task.upper(), task.upper()]\n",
    "        \n",
    "        confuse_mat = confusion_matrix(gnd_truths[task], predicts[task])\n",
    "        classify_report = classification_report(gnd_truths[task], predicts[task],\n",
    "                                        target_names=target_names,\n",
    "                                        output_dict=True)\n",
    "\n",
    "        print(confuse_mat)\n",
    "\n",
    "        for labl in target_names:\n",
    "            print(labl, \"F1-score:\", classify_report[labl][\"f1-score\"])\n",
    "        print(\"Accu:\", classify_report[\"accuracy\"])\n",
    "        print(\"F1-Weighted\", classify_report[\"weighted avg\"][\"f1-score\"])\n",
    "        print(\"F1-Avg\", classify_report[\"macro avg\"][\"f1-score\"])\n",
    "\n",
    "    mean_valid_loss = np.average(valid_losses)\n",
    "    print(\"\\nValidation loss\", mean_valid_loss)\n",
    "\n",
    "    return mean_valid_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2d3eb19a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class METestDataset:\n",
    "    def __init__(self, text_path, quant_label_path):\n",
    "        self.text_path = text_path\n",
    "        self.quant_label_path = quant_label_path\n",
    "\n",
    "        # Load all text\n",
    "        self.textset = self.get_doc_ids()\n",
    "\n",
    "        # Load all annotations\n",
    "        all_files_with_text = list(self.textset.keys())\n",
    "        all_quant_labels = json.load(open(self.quant_label_path))\n",
    "        sorted(list(all_quant_labels.keys())) == sorted(all_files_with_text)\n",
    "        print(len(all_files_with_text), len(all_quant_labels))\n",
    "        \n",
    "        # Also preprocess - normalizing numbers\n",
    "        self.all_data = self.load_dataset(all_files_with_text, all_quant_labels)\n",
    "        print(\"Loaded and processed data\")\n",
    "\n",
    "        # Load Tokenizer and Batch\n",
    "        self.bert_tok = AutoTokenizer.from_pretrained(bert_type, use_fast=True)\n",
    "        print(\"Loaded tokenizer\")\n",
    "        # Add new tokens in tokenizer\n",
    "        new_special_tokens_dict = {\"additional_special_tokens\": [\"<E>\", \"</E>\"]}\n",
    "        self.bert_tok.add_special_tokens(new_special_tokens_dict)\n",
    "\n",
    "        self.batched_dataset = self.batch_dataset(self.all_data)\n",
    "\n",
    "    def load_dataset(self, all_files_with_text, all_quant_labels):\n",
    "\n",
    "        alldata = []\n",
    "        normalize = lambda x: re.sub(r'\\d', '0', x)\n",
    "\n",
    "        # Load annotations from files with label\n",
    "        for file_id in all_files_with_text:\n",
    "            doc_text = self.textset[file_id]\n",
    "\n",
    "            for quants in all_quant_labels[file_id]:\n",
    "                this_quant_sent = doc_text[quants[8]:quants[9]]\n",
    "                this_quant_offsets_from_sent_start = [quants[3] - quants[8], quants[4] - quants[8]]\n",
    "                alldata.append([normalize(this_quant_sent), quants, this_quant_offsets_from_sent_start])\n",
    "\n",
    "        return alldata\n",
    "\n",
    "    def get_doc_ids(self):\n",
    "        textset = {}\n",
    "        for fn in os.listdir(self.text_path):\n",
    "            with open(self.text_path+fn, encoding='utf-8', errors='replace') as textfile:\n",
    "                text = textfile.read()\n",
    "                textset[fn[:-4]] = text\n",
    "\n",
    "        return textset\n",
    "\n",
    "\n",
    "    def batch_dataset(self, sentence_mapped_data):\n",
    "        \"\"\"\n",
    "            Inputs:\n",
    "                    [ [sent, list of quant1_details from task2, quant_offset],\n",
    "                      [sent, list of quant2_details from task2],\n",
    "                      .....\n",
    "                    ]\n",
    "            Outputs:\n",
    "                Batched data: [doc_ixs, sent_offsets, sentences, token_offsets, pad_masks]\n",
    "        \"\"\"\n",
    "        flattened = sentence_mapped_data\n",
    "        print(f'Flattened doc - {len(sentence_mapped_data)}',\n",
    "                \"\\nSome examples:\", flattened[:2]\n",
    "            )\n",
    "\n",
    "        cls_token_idx = self.bert_tok.convert_tokens_to_ids(self.bert_tok.tokenize(self.bert_tok.cls_token))[0]\n",
    "        sep_token_idx = self.bert_tok.convert_tokens_to_ids(self.bert_tok.tokenize(self.bert_tok.sep_token))[0]\n",
    "        pad_token_idx = self.bert_tok.convert_tokens_to_ids(self.bert_tok.tokenize(self.bert_tok.pad_token))[0]\n",
    "\n",
    "        special_st_idx = self.bert_tok.convert_tokens_to_ids(self.bert_tok.tokenize(\"<E>\"))\n",
    "        special_end_idx = self.bert_tok.convert_tokens_to_ids(self.bert_tok.tokenize(\"</E>\"))\n",
    "        assert len(special_st_idx) == 1\n",
    "        assert len(special_end_idx) == 1\n",
    "        special_st_idx, special_end_idx = special_st_idx[0], special_end_idx[0]\n",
    "\n",
    "        print(\"CLS, SEP, PAD, <E>, </E> tokens are:\", cls_token_idx, sep_token_idx, pad_token_idx, special_st_idx, special_end_idx)\n",
    "\n",
    "        dataset = []\n",
    "        idx = 0\n",
    "        num_data = len(flattened)\n",
    "\n",
    "        while idx < num_data:\n",
    "            batch_quant_data = []\n",
    "            batch_raw_text = []\n",
    "            batch_raw_labels = []\n",
    "\n",
    "            for single_sentence, quant_data, quant_offsets_from_sent_start in \\\n",
    "                        flattened[idx:min(batch_size, num_data)]:\n",
    "                offset = quant_offsets_from_sent_start\n",
    "                single_raw_anns = [[offset[0], offset[1], 'QUANT']]\n",
    "\n",
    "                single_raw_text = single_sentence\n",
    "\n",
    "                batch_quant_data.append(quant_data)\n",
    "                batch_raw_text.append(single_raw_text)\n",
    "                batch_raw_labels.append(single_raw_anns)\n",
    "\n",
    "            batched_dict = self.bert_tok.batch_encode_plus(batch_raw_text,\n",
    "                                return_offsets_mapping=True, padding=True)\n",
    "\n",
    "\n",
    "            # Create sequence labels using token offsets\n",
    "            no_special_batch_tokens = batched_dict['input_ids']\n",
    "            no_special_batch_pads = batched_dict['attention_mask']\n",
    "            no_special_batch_offset_mapping = batched_dict['offset_mapping']\n",
    "            \n",
    "            batch_tokens = []\n",
    "            batch_pads = []\n",
    "            batch_offset_mapping = []\n",
    "            batch_labels = {\"QUANT\": []}\n",
    "\n",
    "            # Add <E> and </E>\n",
    "            for single_sent_tokens, single_sent_pad_masks, single_token_offs, single_sent_anns in \\\n",
    "                    zip(no_special_batch_tokens, no_special_batch_pads, no_special_batch_offset_mapping, batch_raw_labels):\n",
    "\n",
    "                this_tokens = []\n",
    "                this_pads = []\n",
    "                this_offset_mapping = []\n",
    "                this_labels = {\"QUANT\": []}\n",
    "\n",
    "                quant_label, entity_label, property_label, qual_label = None, None, None, None\n",
    "                for ann in single_sent_anns:\n",
    "                    if ann[2] == \"QUANT\":\n",
    "                        quant_label = [ann[0], ann[1]]\n",
    "                    else:\n",
    "                        raise ann\n",
    "\n",
    "                quant_happening = 0\n",
    "                for i, off in enumerate(single_token_offs):\n",
    "                    if off == (0,0):\n",
    "                        if quant_happening == 1: \n",
    "                            # Append for </E>, if the code reaches here then quant label has ended\n",
    "                            this_tokens.append(special_end_idx)\n",
    "                            this_pads.append(1)\n",
    "                            this_offset_mapping.append((0,0))\n",
    "                            this_labels[\"QUANT\"].append(1)\n",
    "                        quant_happening = 0\n",
    "\n",
    "                        this_tokens.append(single_sent_tokens[i])\n",
    "                        this_pads.append(single_sent_pad_masks[i])\n",
    "                        this_offset_mapping.append(off)\n",
    "                        this_labels[\"QUANT\"].append(0)\n",
    "                    else:\n",
    "                        if off[1] < quant_label[0] or off[0] > quant_label[1]:\n",
    "                            if quant_happening == 1: \n",
    "                                # Append for </E>, if the code reaches here then quant label has ended\n",
    "                                this_tokens.append(special_end_idx)\n",
    "                                this_pads.append(1)\n",
    "                                this_offset_mapping.append((0,0))\n",
    "                                this_labels[\"QUANT\"].append(1)\n",
    "                            quant_happening = 0\n",
    "                        else:\n",
    "                            if quant_happening == 0:\n",
    "                                # Append for <E>, if the code reaches here then quant label has started\n",
    "                                this_tokens.append(special_st_idx)\n",
    "                                this_pads.append(1)\n",
    "                                this_offset_mapping.append((0,0))\n",
    "                                this_labels[\"QUANT\"].append(1)\n",
    "                            quant_happening = 1\n",
    "\n",
    "                        # Check for all_quant and append\n",
    "                        this_tokens.append(single_sent_tokens[i])\n",
    "                        this_pads.append(single_sent_pad_masks[i])\n",
    "                        this_offset_mapping.append(off)\n",
    "\n",
    "                        if off[1] >= quant_label[0] and off[0] <= quant_label[1]:\n",
    "                            this_labels[\"QUANT\"].append(1)\n",
    "                        else:\n",
    "                            this_labels[\"QUANT\"].append(0)\n",
    "\n",
    "                batch_tokens.append(this_tokens)\n",
    "                batch_pads.append(this_pads)\n",
    "                batch_offset_mapping.append(this_offset_mapping)\n",
    "                batch_labels[\"QUANT\"].append(this_labels[\"QUANT\"])\n",
    "\n",
    "            max_batch_len = max([len(bt) for bt in batch_tokens])\n",
    "            min_batch_len = min([len(bt) for bt in batch_tokens])\n",
    "            assert max_batch_len == min_batch_len or max_batch_len == min_batch_len + 2\n",
    "\n",
    "            token_pad_function_maxlen = lambda x, maxlen: x if len(x) == maxlen else x + [pad_token_idx, pad_token_idx]\n",
    "            mask_pad_function_maxlen = lambda x, maxlen: x if len(x) == maxlen else x + [0, 0]\n",
    "            label_pad_function_maxlen = lambda x, maxlen: x if len(x) == maxlen else x + [0, 0]\n",
    "\n",
    "            batch_tokens = [token_pad_function_maxlen(sentence, max_batch_len) for sentence in batch_tokens]\n",
    "            batch_tokens = torch.LongTensor(batch_tokens).to(device)\n",
    "\n",
    "            batch_pads = [mask_pad_function_maxlen(sentence, max_batch_len) for sentence in batch_pads]\n",
    "            pad_masks = torch.LongTensor(batch_pads).to(device)\n",
    "\n",
    "            batch_maxlen = batch_tokens.shape[-1]\n",
    "            for k in batch_labels:\n",
    "                padded_labels = [label_pad_function_maxlen(sentence, max_batch_len) for sentence in batch_labels[k]]\n",
    "                batch_labels[k] = padded_labels\n",
    "            batch_ann_labels = {k: torch.LongTensor(batch_labels[k]).to(device)\n",
    "                                    for k in batch_labels\n",
    "                                }\n",
    "\n",
    "            b = batch_size if (idx + batch_size) < num_data else (num_data - idx)\n",
    "            assert batch_tokens.size() == torch.Size([b, batch_maxlen])\n",
    "            for vvvvv in batch_ann_labels.values():\n",
    "                assert vvvvv.size() == torch.Size([b, batch_maxlen])\n",
    "            assert pad_masks.size() == torch.Size([b, batch_maxlen])\n",
    "\n",
    "            dataset.append((batch_tokens, batch_ann_labels, pad_masks, batch_offset_mapping, batch_quant_data))\n",
    "            idx += batch_size\n",
    "\n",
    "        print(\"num_batches=\", len(dataset), \" | num_data=\", num_data)\n",
    "        return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "65244b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "typemap = {\"Quantity\": \"QUANT\",\n",
    "           \"MeasuredEntity\": \"ME\", \n",
    "           \"MeasuredProperty\": \"MP\", \n",
    "           \"Qualifier\": \"QUAL\"\n",
    "        }\n",
    "\n",
    "# Dataloader\n",
    "class MEDataset:\n",
    "    def __init__(self, text_path, label_path, bert_type):\n",
    "        self.text_path = text_path\n",
    "        self.label_path = label_path\n",
    "\n",
    "        # Load all text\n",
    "        self.textset = self.get_doc_ids()\n",
    "\n",
    "        # Load all annotations\n",
    "        files_with_label = [file_name[:-4] for file_name in os.listdir(label_path)]\n",
    "        all_files_with_or_without_label = [file_name[:-4] for file_name in os.listdir(text_path)]\n",
    "        files_without_label = list(set(all_files_with_or_without_label).difference(set(files_with_label)))\n",
    "        print(len(files_with_label), len(files_without_label), len(all_files_with_or_without_label))\n",
    "        self.all_data = self.load_dataset(files_with_label, files_without_label)\n",
    "\n",
    "        # Map ME, MP, Qual to the Quant.\n",
    "        self.mapped_data = self.map_dataset(self.all_data)\n",
    "\n",
    "        # Preprocess - sentence splitting, normalizing numbers\n",
    "        self.tokenized_data = self.tokenize_split_data(self.all_data)\n",
    "        self.all_sentence_mapped_data = self.sentence_map(self.tokenized_data, self.mapped_data)\n",
    "        print(\"Loaded and processed data\")\n",
    "\n",
    "        # Load Tokenizer and Batch\n",
    "        self.bert_tok = AutoTokenizer.from_pretrained(bert_type)\n",
    "        print(\"Loaded tokenizer\")\n",
    "        # Add new tokens in tokenizer\n",
    "        new_special_tokens_dict = {\"additional_special_tokens\": [\"<E>\", \"</E>\"]}\n",
    "        self.bert_tok.add_special_tokens(new_special_tokens_dict)\n",
    "        print(\"LEN VOCAB:\", len(self.bert_tok))\n",
    "        self.batched_dataset = self.batch_dataset(self.all_sentence_mapped_data,\n",
    "                                                  shuffle=True if \"train\" in text_path else False\n",
    "                                                )\n",
    "\n",
    "    def load_dataset(self, files_with_label, files_without_label):\n",
    "\n",
    "        alldata = []\n",
    "\n",
    "        # Load annotations from files with label\n",
    "        for fn_no_ext in files_with_label:\n",
    "            fn = fn_no_ext + \".tsv\"\n",
    "            entities = {\"QUANT\": [], \"ME\": [], \"MP\": [], \"QUAL\": []}\n",
    "            with open(self.label_path+fn, encoding='utf-8', errors='replace') as annotfile:\n",
    "                text = self.textset[fn[:-4]]\n",
    "                next(annotfile)\n",
    "                annots = annotfile.read().splitlines()\n",
    "                for a in annots:\n",
    "                    annot = a.split(\"\\t\")\n",
    "                    atype = typemap[annot[2]]\n",
    "                    start = int(annot[3])\n",
    "                    stop = int(annot[4])\n",
    "                    other = None if annot[-1] == '' else json.loads(annot[-1])\n",
    "                    ann_id = annot[1]\n",
    "                    # This is where we toss out the overlaps:\n",
    "                    overlap = False\n",
    "                    # for ent in entities[atype]:\n",
    "                    #     if ((start >= ent[0] and start <= ent[1]) or (stop >= ent[0] and stop <= ent[1]) or\n",
    "                    #         (ent[0] >= start and ent[0] <= stop) or (ent[1] >= start and ent[1] <= stop)):\n",
    "                    #         overlap = True\n",
    "                    if overlap == False:    \n",
    "                        entities[atype].append((start, stop, atype, ann_id, other))\n",
    "                alldata.append((text,\n",
    "                                {\"QUANT\": entities[\"QUANT\"],\n",
    "                                 \"ME\": entities[\"ME\"],\n",
    "                                 \"MP\": entities[\"MP\"],\n",
    "                                 \"QUAL\": entities[\"QUAL\"]\n",
    "                                },\n",
    "                                (fn,)\n",
    "                            ))\n",
    "\n",
    "        return alldata\n",
    "\n",
    "    def get_doc_ids(self):\n",
    "        textset = {}\n",
    "        for fn in os.listdir(self.text_path):\n",
    "            with open(self.text_path+fn, encoding='utf-8', errors='replace') as textfile:\n",
    "                text = textfile.read()\n",
    "                textset[fn[:-4]] = text\n",
    "\n",
    "        return textset\n",
    "\n",
    "    def map_dataset(self, all_data):\n",
    "        '''\n",
    "        Output:\n",
    "            [\n",
    "                [text,\n",
    "                 List[\n",
    "                        q1_start, q1_stop,\n",
    "                        q1_atype, q1_ann_id,\n",
    "                        [list of max length 3, containing corresponding ME, MP, QUAL]\n",
    "                 ],\n",
    "                 (filename,\n",
    "                 )\n",
    "                ]\n",
    "            ]\n",
    "        '''\n",
    "        mapped_dataset = []\n",
    "        for doc in all_data:\n",
    "            this_quant_data = []\n",
    "            for single_quant in doc[1]['QUANT']:\n",
    "                this_ann_id = single_quant[3]\n",
    "                this_quant_props = []\n",
    "                for me in doc[1]['ME']:\n",
    "                    if this_ann_id == me[-2]:\n",
    "                        this_quant_props.append(me)\n",
    "                for mp in doc[1]['MP']:\n",
    "                    if this_ann_id == mp[-2]:\n",
    "                        this_quant_props.append(mp)\n",
    "                for qual in doc[1]['QUAL']:\n",
    "                    if this_ann_id == qual[-2]:\n",
    "                        this_quant_props.append(qual)\n",
    "\n",
    "                this_quant_data.append([single_quant[0], single_quant[1],\n",
    "                                        single_quant[2], single_quant[3],\n",
    "                                        single_quant[4], this_quant_props\n",
    "                                    ])\n",
    "\n",
    "            mapped_dataset.append((doc[0], this_quant_data, doc[-1]))\n",
    "        return mapped_dataset\n",
    "\n",
    "    def tokenize_split_data(self, all_data):\n",
    "        processed_data = []\n",
    "\n",
    "        cnt_toks = {\"figs.\": 0, \"fig.\": 0, \"et al.\": 0,\n",
    "                    \"ref.\": 0, \"eq.\": 0, \"e.g.\": 0,\n",
    "                    \"i.e.\": 0, \"nos.\": 0, \"no.\": 0,\n",
    "                    \"spp.\": 0\n",
    "                    }\n",
    "        regex_end_checker = [\".*[a-zA-Z]figs\\.$\", \n",
    "                            \".*[a-zA-Z]fig\\.$\",\n",
    "                            \".*[a-zA-Z]et al\\.$\",\n",
    "                            \".*[a-zA-Z]ref\\.$\",\n",
    "                            \".*[a-zA-Z]eq\\.$\",\n",
    "                            \".*[a-zA-Z]e\\.g\\.$\",\n",
    "                            \".*[a-zA-Z]i\\.e\\.$\",\n",
    "                            \".*[a-zA-Z]nos\\.$\",\n",
    "                            \".*[a-zA-Z]no\\.$\",\n",
    "                            \".*[a-zA-Z]spp\\.$\",\n",
    "                            # figs., fig., et al., Ref., Eq., e.g., i.e., Nos., No., spp.\n",
    "                        ]\n",
    "\n",
    "        assert len(cnt_toks) == len(regex_end_checker)\n",
    "\n",
    "        # list of sentences\n",
    "        # for every tokenized sentence obtained\n",
    "            # check if ends with \"fig or Figs or et al.\"\n",
    "            # keep track of the index where the this sentence starts and end,\n",
    "\n",
    "        all_tokenized_data = []\n",
    "        for doc in all_data:\n",
    "            flag = False\n",
    "            sentences = sent_tokenize(doc[0])\n",
    "\n",
    "            fixed_sentence_tokens = []\n",
    "            curr_len = 0\n",
    "            for s in sentences:\n",
    "                if flag == True:\n",
    "                    assert s[0] != ' '\n",
    "                    white_length = doc[0][curr_len:].find(s[0])\n",
    "\n",
    "                    prev_len = len(fixed_sentence_tokens[-1])\n",
    "                    fixed_sentence_tokens[-1] = fixed_sentence_tokens[-1] + (\" \"*white_length) + s\n",
    "\n",
    "                    assert fixed_sentence_tokens[-1][prev_len+white_length] == doc[0][curr_len+white_length], (fixed_sentence_tokens[-1], doc[0], curr_len, tmp_this_sent_len)\n",
    "                    tmp_this_sent_len = white_length + len(s)\n",
    "                    assert fixed_sentence_tokens[-1][-1] == doc[0][curr_len+tmp_this_sent_len-1], (fixed_sentence_tokens[-1], doc[0], curr_len, tmp_this_sent_len)\n",
    "                    curr_len += tmp_this_sent_len\n",
    "                else:\n",
    "                    if len(fixed_sentence_tokens) != 0:\n",
    "                        assert s[0] != ' '\n",
    "                        white_length = doc[0][curr_len:].find(s[0])\n",
    "                        fixed_sentence_tokens.append( (\" \"*white_length) + s )\n",
    "                    else:\n",
    "                        fixed_sentence_tokens.append(s)\n",
    "                    assert fixed_sentence_tokens[-1][0] == doc[0][curr_len], (fixed_sentence_tokens, doc[0], curr_len, tmp_this_sent_len)\n",
    "                    tmp_this_sent_len = len(fixed_sentence_tokens[-1])\n",
    "                    assert fixed_sentence_tokens[-1][-1] == doc[0][curr_len+tmp_this_sent_len-1], (fixed_sentence_tokens[-1], doc[0], curr_len, tmp_this_sent_len)\n",
    "                    curr_len += tmp_this_sent_len\n",
    "\n",
    "                lower_cased_s = fixed_sentence_tokens[-1].lower()\n",
    "                flag = False\n",
    "                for i, k in enumerate(cnt_toks):\n",
    "                    this_regex_pattern = regex_end_checker[i]\n",
    "                    if lower_cased_s.endswith(k) and re.match(this_regex_pattern, lower_cased_s) == None:\n",
    "                        cnt_toks[k] += 1\n",
    "                        flag = True\n",
    "                        break\n",
    "\n",
    "            all_tokenized_data.append(fixed_sentence_tokens)      \n",
    "        print(\"Fixed sentence splitting:\", cnt_toks)\n",
    "        return all_tokenized_data\n",
    "\n",
    "    def sentence_map(self, all_tokenized_data, mapped_data):\n",
    "        \"\"\"\n",
    "            Inputs:\n",
    "                all_tokenized_data: list of [list of sentences]\n",
    "                mapped_data: list of Tuple[doc, quant-wise-annotations, (doc_id,)]\n",
    "            Outputs:\n",
    "                all_annotated_split_data: \n",
    "                        [\n",
    "                            Dict{'doc_id': doc_id,\n",
    "                                'sentences': [list of string]\n",
    "                                'offsets': [list of int]\n",
    "                                'annotations': [ list of [sent_ix, sent_offset, offset_adjusted annotations for a single quant]\n",
    "                                                 ... same for other quants\n",
    "                                                ],\n",
    "                    \n",
    "                                ... Repeat for the second doc\n",
    "                ]\n",
    "        \"\"\"\n",
    "\n",
    "        # in the next loop\n",
    "            # Replace all numbers by zero.\n",
    "            # check if any of the annotations are used for this falls in between the start and end. Make sure no overlap\n",
    "            # add offset as well.\n",
    "\n",
    "        normalize = lambda x: re.sub(r'\\d', '0', x)\n",
    "        all_annotated_split_data = []\n",
    "        \n",
    "        for doc, sent_splits in zip(mapped_data, all_tokenized_data):\n",
    "\n",
    "            this_offsets = []\n",
    "\n",
    "            prev_end = 0\n",
    "            for s in sent_splits:\n",
    "                this_offsets.append([prev_end, prev_end+len(s)])\n",
    "                prev_end += len(s)\n",
    "            \n",
    "            this_annotations = [] # Each element of the format: (sent_ix, anns)\n",
    "\n",
    "            for ann in doc[1]:\n",
    "                if len(ann[-1]) == 0:\n",
    "                    lowest_char_ix = ann[0]\n",
    "                    highest_char_ix = ann[1]\n",
    "                else:\n",
    "                    lowest_char_ix = min(ann[0], min(rel_ann[0] for rel_ann in ann[-1]))\n",
    "                    highest_char_ix = max(ann[1], max(rel_ann[1] for rel_ann in ann[-1]))\n",
    "                \n",
    "                iii = 0\n",
    "                # Calc low ix.\n",
    "                min_sent_ix = -1\n",
    "                while iii < len(this_offsets):\n",
    "                    if lowest_char_ix < this_offsets[iii][1]:\n",
    "                        min_sent_ix = iii\n",
    "                        break\n",
    "                    iii += 1\n",
    "\n",
    "                # Calc high sent ix\n",
    "                max_sent_ix = -1\n",
    "                while iii < len(this_offsets):\n",
    "                    if highest_char_ix <= this_offsets[iii][1]:\n",
    "                        max_sent_ix = iii\n",
    "                        break\n",
    "                    iii += 1\n",
    "                assert min_sent_ix != -1\n",
    "                assert max_sent_ix != -1\n",
    "\n",
    "                if min_sent_ix != max_sent_ix:\n",
    "                    # skip, since it will be difficult during test time.\n",
    "                    pass\n",
    "                else:\n",
    "                    # Append ann\n",
    "                    this_ann_off = this_offsets[min_sent_ix][0]\n",
    "                    offset_modified_ann = [ann[0]-this_ann_off, ann[1]-this_ann_off, ann[2], ann[3], ann[4],\n",
    "                                            [(rel_ann[0]-this_ann_off, rel_ann[1]-this_ann_off, rel_ann[2], rel_ann[3], rel_ann[4])\n",
    "                                                for rel_ann in ann[5]\n",
    "                                            ]\n",
    "                                        ]\n",
    "                    this_annotations.append([min_sent_ix, this_ann_off, offset_modified_ann])\n",
    "\n",
    "            all_annotated_split_data.append({'doc_id': doc[-1][0],\n",
    "                        'sentences': [normalize(ss) for ss in sent_splits],\n",
    "                        'offsets': this_offsets,\n",
    "                        'annotations': this_annotations\n",
    "                    })\n",
    "            # print(all_annotated_split_data[-1])\n",
    "            for ta in this_annotations:\n",
    "                assert ta[0] >= 0 and ta[0] < len(sent_splits)\n",
    "            assert len(all_annotated_split_data[-1]['offsets']) == len(all_annotated_split_data[-1]['sentences'])\n",
    "            # assert len(all_annotated_split_data[-1]['offsets']) == len(all_annotated_split_data[-1]['annotations'])\n",
    "        \n",
    "        return all_annotated_split_data\n",
    "\n",
    "    def batch_dataset(self, sentence_mapped_data, shuffle=False, batch_size=batch_size, device=device):\n",
    "        \"\"\"\n",
    "            Inputs:\n",
    "                sentence_mapped_data: \n",
    "                        [\n",
    "                            Dict{'doc_id': doc_id,\n",
    "                                'sentences': [list of string]\n",
    "                                'offsets': [list of int]\n",
    "                                'annotations': [ list of [sent_ix, sent_offset, offset_adjusted annotations for a single quant]\n",
    "                                                 ... same for other quants\n",
    "                                                ],\n",
    "                    \n",
    "                                ... Repeat for the second doc\n",
    "                ]\n",
    "            Outputs:\n",
    "                Batched data: [doc_ixs, sent_offsets, sentences, labels, pad_masks]\n",
    "        \"\"\"\n",
    "        # First flatten and remove overlapping labels and then shuffle\n",
    "        # Then Batch\n",
    "        flattened = [[doc['doc_id'], doc['sentences'][doc['annotations'][i][0]],\n",
    "                      doc['annotations'][i][1], doc['annotations'][i]]\n",
    "                    for doc in sentence_mapped_data for i in range(len(doc['annotations']))\n",
    "                ]\n",
    "\n",
    "        print(f'Flattened {len(sentence_mapped_data)} docs into {len(flattened)} data points',\n",
    "                \"\\nSome examples:\", flattened[:2])\n",
    "\n",
    "        def has_overlap(ann, datapt):\n",
    "            no_overlap = True\n",
    "            sss = sorted([ann[:2]] + [list(ff[:2]) for ff in ann[-1]], key = lambda x: x[0])\n",
    "            for i in range(len(sss)-1):\n",
    "                if sss[i][1] > sss[i+1][0]:\n",
    "                    no_overlap = False\n",
    "                    print(\"\\n\\nWARNING: Discarding due to overlap\", datapt, '\\n\\n')\n",
    "                    break\n",
    "            return no_overlap\n",
    "\n",
    "        flattened = [f for f in flattened if has_overlap(f[-1][-1], f)]\n",
    "        print(\"Length after discarding overlaps:\", len(flattened))\n",
    "        \n",
    "        if shuffle:\n",
    "            random.shuffle(flattened)\n",
    "\n",
    "        cls_token_idx = self.bert_tok.convert_tokens_to_ids(self.bert_tok.tokenize(self.bert_tok.cls_token))[0]\n",
    "        sep_token_idx = self.bert_tok.convert_tokens_to_ids(self.bert_tok.tokenize(self.bert_tok.sep_token))[0]\n",
    "        pad_token_idx = self.bert_tok.convert_tokens_to_ids(self.bert_tok.tokenize(self.bert_tok.pad_token))[0]\n",
    "        \n",
    "        special_st_idx = self.bert_tok.convert_tokens_to_ids(self.bert_tok.tokenize(\"<E>\"))\n",
    "        special_end_idx = self.bert_tok.convert_tokens_to_ids(self.bert_tok.tokenize(\"</E>\"))\n",
    "        assert len(special_st_idx) == 1\n",
    "        assert len(special_end_idx) == 1\n",
    "        special_st_idx, special_end_idx = special_st_idx[0], special_end_idx[0]\n",
    "\n",
    "        print(\"CLS, SEP, PAD, <E>, </E> tokens are:\", cls_token_idx, sep_token_idx, pad_token_idx, special_st_idx, special_end_idx)\n",
    "\n",
    "        dataset = []\n",
    "        idx = 0\n",
    "        num_data = len(flattened)\n",
    "        while idx < num_data:\n",
    "            batch_doc_ids = []\n",
    "            batch_sent_offsets = []\n",
    "            batch_raw_text = []\n",
    "            batch_raw_labels = []\n",
    "\n",
    "            for single_docid, single_sentence, single_offset, single_annotations_with_offset_sentence_idx in \\\n",
    "                        flattened[idx:min(idx+batch_size, num_data)]:\n",
    "                \n",
    "                single_annotations = single_annotations_with_offset_sentence_idx[-1]\n",
    "                single_raw_anns = [single_annotations[:3]] + [list(ff[:3]) for ff in single_annotations[-1]]\n",
    "\n",
    "                # quant_st, quant_end = (single_raw_anns[0][0], single_raw_anns[0][1])\n",
    "                single_raw_text = single_sentence#[:quant_st] + \" <E> \" + \\\n",
    "                            #single_sentence[quant_st:quant_end] + \" </E> \" + single_sentence[quant_end:]\n",
    "                \n",
    "\n",
    "                batch_doc_ids.append(single_docid)\n",
    "                batch_sent_offsets.append(single_offset)\n",
    "                batch_raw_text.append(single_raw_text)\n",
    "                batch_raw_labels.append(single_raw_anns)\n",
    "\n",
    "            batched_dict = self.bert_tok.batch_encode_plus(batch_raw_text,\n",
    "                                                        return_offsets_mapping=True,\n",
    "                                                        padding=True)\n",
    "\n",
    "\n",
    "            # Create sequence labels using token offsets\n",
    "            no_special_batch_tokens = batched_dict['input_ids']\n",
    "            no_special_batch_pads = batched_dict['attention_mask']\n",
    "            no_special_batch_offset_mapping = batched_dict['offset_mapping']\n",
    "            \n",
    "            batch_tokens = []\n",
    "            batch_pads = []\n",
    "            batch_offset_mapping = []\n",
    "            batch_labels = {\"QUANT\": [], \"ME\": [], \"MP\": [], \"QUAL\": []}\n",
    "\n",
    "            # Add <E> and </E>\n",
    "            for single_sent_tokens, single_sent_pad_masks, single_token_offs, single_sent_anns in \\\n",
    "                    zip(no_special_batch_tokens, no_special_batch_pads, no_special_batch_offset_mapping, batch_raw_labels):\n",
    "\n",
    "                this_tokens = []\n",
    "                this_pads = []\n",
    "                this_offset_mapping = []\n",
    "                this_labels = {\"QUANT\": [], \"ME\": [], \"MP\": [], \"QUAL\": []}\n",
    "\n",
    "                quant_label, entity_label, property_label, qual_label = None, None, None, None\n",
    "                for ann in single_sent_anns:\n",
    "                    if ann[2] == \"QUANT\":\n",
    "                        quant_label = [ann[0], ann[1]]\n",
    "                    elif ann[2] == \"ME\":\n",
    "                        entity_label = [ann[0], ann[1]]\n",
    "                    elif ann[2] == \"MP\":\n",
    "                        property_label = [ann[0], ann[1]]\n",
    "                    elif ann[2] == \"QUAL\":\n",
    "                        qual_label = [ann[0], ann[1]]\n",
    "                    else:\n",
    "                        raise ann\n",
    "\n",
    "                quant_happening = 0\n",
    "                for i, off in enumerate(single_token_offs):\n",
    "                    if off == (0,0):\n",
    "                        if quant_happening == 1: \n",
    "                            # Append for </E>, if the code reaches here then quant label has ended\n",
    "                            this_tokens.append(special_end_idx)\n",
    "                            this_pads.append(1)\n",
    "                            this_offset_mapping.append((0,0))\n",
    "                            this_labels[\"QUANT\"].append(1)\n",
    "                            this_labels[\"MP\"].append(0)\n",
    "                            this_labels[\"ME\"].append(0)\n",
    "                            this_labels[\"QUAL\"].append(0)\n",
    "                        quant_happening = 0\n",
    "\n",
    "                        this_tokens.append(single_sent_tokens[i])\n",
    "                        this_pads.append(single_sent_pad_masks[i])\n",
    "                        this_offset_mapping.append(off)\n",
    "                        this_labels[\"QUANT\"].append(0)\n",
    "                        this_labels[\"MP\"].append(0)\n",
    "                        this_labels[\"ME\"].append(0)\n",
    "                        this_labels[\"QUAL\"].append(0)\n",
    "                    else:\n",
    "                        if off[1] < quant_label[0] or off[0] > quant_label[1]:\n",
    "                            if quant_happening == 1: \n",
    "                                # Append for </E>, if the code reaches here then quant label has ended\n",
    "                                this_tokens.append(special_end_idx)\n",
    "                                this_pads.append(1)\n",
    "                                this_offset_mapping.append((0,0))\n",
    "                                this_labels[\"QUANT\"].append(1)\n",
    "                                this_labels[\"MP\"].append(0)\n",
    "                                this_labels[\"ME\"].append(0)\n",
    "                                this_labels[\"QUAL\"].append(0)\n",
    "                            quant_happening = 0\n",
    "                        else:\n",
    "                            if quant_happening == 0:\n",
    "                                # Append for <E>, if the code reaches here then quant label has started\n",
    "                                this_tokens.append(special_st_idx)\n",
    "                                this_pads.append(1)\n",
    "                                this_offset_mapping.append((0,0))\n",
    "                                this_labels[\"QUANT\"].append(1)\n",
    "                                this_labels[\"MP\"].append(0)\n",
    "                                this_labels[\"ME\"].append(0)\n",
    "                                this_labels[\"QUAL\"].append(0)\n",
    "                            quant_happening = 1\n",
    "\n",
    "                        # Check for all_quant and append\n",
    "                        this_tokens.append(single_sent_tokens[i])\n",
    "                        this_pads.append(single_sent_pad_masks[i])\n",
    "                        this_offset_mapping.append(off)\n",
    "\n",
    "                        if off[1] >= quant_label[0] and off[0] <= quant_label[1]:\n",
    "                            this_labels[\"QUANT\"].append(1)\n",
    "                        else:\n",
    "                            this_labels[\"QUANT\"].append(0)\n",
    "\n",
    "                        if entity_label != None and off[1] >= entity_label[0] and off[0] <= entity_label[1]:\n",
    "                            this_labels[\"ME\"].append(1)\n",
    "                        else:\n",
    "                            this_labels[\"ME\"].append(0)\n",
    "\n",
    "                        if property_label != None and off[1] >= property_label[0] and off[0] <= property_label[1]:\n",
    "                            this_labels[\"MP\"].append(1)\n",
    "                        else:\n",
    "                            this_labels[\"MP\"].append(0)\n",
    "\n",
    "                        if qual_label != None and off[1] >= qual_label[0] and off[0] <= qual_label[1]:\n",
    "                            this_labels[\"QUAL\"].append(1)\n",
    "                        else:\n",
    "                            this_labels[\"QUAL\"].append(0)\n",
    "\n",
    "                batch_tokens.append(this_tokens)\n",
    "                batch_pads.append(this_pads)\n",
    "                batch_offset_mapping.append(this_offset_mapping)\n",
    "                batch_labels[\"QUANT\"].append(this_labels[\"QUANT\"])\n",
    "                batch_labels[\"MP\"].append(this_labels[\"MP\"])\n",
    "                batch_labels[\"ME\"].append(this_labels[\"ME\"])\n",
    "                batch_labels[\"QUAL\"].append(this_labels[\"QUAL\"])\n",
    "\n",
    "            batch_tokens = torch.LongTensor(batch_tokens).to(device)\n",
    "            pad_masks = torch.LongTensor(batch_pads).to(device)\n",
    "\n",
    "            batch_maxlen = batch_tokens.shape[-1]\n",
    "            batch_ann_labels = {k: torch.LongTensor(batch_labels[k]).to(device)\n",
    "                                    for k in batch_labels\n",
    "                                }\n",
    "\n",
    "            b = batch_size if (idx + batch_size) < num_data else (num_data - idx)\n",
    "            assert batch_tokens.size() == torch.Size([b, batch_maxlen])\n",
    "            for vvvvv in batch_ann_labels.values():\n",
    "                assert vvvvv.size() == torch.Size([b, batch_maxlen])\n",
    "            assert pad_masks.size() == torch.Size([b, batch_maxlen])\n",
    "\n",
    "            dataset.append((batch_tokens, batch_ann_labels, pad_masks, batch_doc_ids, batch_sent_offsets, batch_offset_mapping, batch_raw_text))\n",
    "            idx += batch_size\n",
    "\n",
    "        print(\"num_batches=\", len(dataset), \" | num_data=\", num_data)\n",
    "        return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "fd6262ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "233 15 248\n",
      "Fixed sentence splitting: {'figs.': 4, 'fig.': 88, 'et al.': 29, 'ref.': 3, 'eq.': 3, 'e.g.': 8, 'i.e.': 3, 'nos.': 2, 'no.': 3, 'spp.': 2}\n",
      "Loaded and processed data\n",
      "Loaded tokenizer\n",
      "LEN VOCAB: 50267\n",
      "Flattened 233 docs into 855 data points \n",
      "Some examples: [['S0006322312001096-1136.tsv', 'Data were drawn from the Whitehall II study with baseline examination in 0000; follow-up screenings in 0000, 0000, and 0000; and additional disease ascertainment from hospital data and registry linkage on 0000 participants (mean age 00.0 years, 00% women) without depressive symptoms at baseline.', 0, [0, 0, [73, 77, 'QUANT', '1', None, [(49, 69, 'ME', '1', {'HasQuantity': 'T1-1'})]]]], ['S0006322312001096-1136.tsv', 'Data were drawn from the Whitehall II study with baseline examination in 0000; follow-up screenings in 0000, 0000, and 0000; and additional disease ascertainment from hospital data and registry linkage on 0000 participants (mean age 00.0 years, 00% women) without depressive symptoms at baseline.', 0, [0, 0, [103, 123, 'QUANT', '2', {'mods': ['IsList']}, [(79, 99, 'ME', '2', {'HasQuantity': 'T1-2'})]]]]]\n",
      "\n",
      "\n",
      "WARNING: Discarding due to overlap ['S0016236113008041-3012.tsv', ' It was shown that 00% of elemental Hg was emitted in the exhaust gas, as was 00% of Cl.', 114, [1, 114, [19, 22, 'QUANT', '2', {'unit': '%'}, [(26, 38, 'ME', '2', {'HasProperty': 'T4-2'}), (26, 69, 'MP', '2', {'HasQuantity': 'T1-2'})]]]] \n",
      "\n",
      "\n",
      "Length after discarding overlaps: 854\n",
      "CLS, SEP, PAD, <E>, </E> tokens are: 0 2 1 50265 50266\n",
      "num_batches= 7  | num_data= 854\n",
      "65 0 65\n",
      "Fixed sentence splitting: {'figs.': 6, 'fig.': 61, 'et al.': 22, 'ref.': 0, 'eq.': 2, 'e.g.': 5, 'i.e.': 0, 'nos.': 0, 'no.': 0, 'spp.': 4}\n",
      "Loaded and processed data\n",
      "Loaded tokenizer\n",
      "LEN VOCAB: 50267\n",
      "Flattened 65 docs into 236 data points \n",
      "Some examples: [['S0012821X12004384-1302.tsv', ' Five samples from below the CIE at 0000.00, 0000.00, 0000.00, 0000.00, and 0000.00 m (indicated in Fig. 0) contain Apectodinium, in contrast to the other samples below the CIE (Figs. 0 and 0).', 552, [2, 552, [1, 5, 'QUANT', '1', {'mods': ['IsCount']}, [(6, 13, 'ME', '1', {'HasQuantity': 'T91-1'})]]]], ['S0012821X12004384-1302.tsv', ' Five samples from below the CIE at 0000.00, 0000.00, 0000.00, 0000.00, and 0000.00 m (indicated in Fig. 0) contain Apectodinium, in contrast to the other samples below the CIE (Figs. 0 and 0).', 552, [2, 552, [36, 85, 'QUANT', '2', {'mods': ['IsList'], 'unit': 'm'}, [(6, 13, 'ME', '2', {'HasQuantity': 'T12-2'}), (19, 32, 'QUAL', '2', {'Qualifies': 'T102-2'})]]]]]\n",
      "Length after discarding overlaps: 236\n",
      "CLS, SEP, PAD, <E>, </E> tokens are: 0 2 1 50265 50266\n",
      "num_batches= 2  | num_data= 236\n",
      "Dataset created\n",
      "Model created\n",
      "124651794\n",
      "Detected 1 GPUs!\n",
      "\n",
      "\n",
      "========= Beginning 1 epoch ==========\n",
      "Loss Scale: {'me': 1.0, 'mp': 1.0, 'qual': 1.0}\n",
      "Train loss at 0: 2.106814384460449\n",
      "\n",
      "====EVALUATING On Validation set :====\n",
      "\n",
      "Loss Scale: {'me': 1.0, 'mp': 1.0, 'qual': 1.0}\n",
      "\n",
      "Task: me\n",
      "[[12163     0]\n",
      " [  781     0]]\n",
      "NOT_ME F1-score: 0.9688931373720476\n",
      "ME F1-score: 0.0\n",
      "Accu: 0.9396631644004945\n",
      "F1-Weighted 0.9104331914289412\n",
      "F1-Avg 0.4844465686860238\n",
      "\n",
      "Task: mp\n",
      "[[12563     0]\n",
      " [  381     0]]\n",
      "NOT_MP F1-score: 0.9850629239032422\n",
      "MP F1-score: 0.0\n",
      "Accu: 0.9705655129789864\n",
      "F1-Weighted 0.9560681020547305\n",
      "F1-Avg 0.4925314619516211\n",
      "\n",
      "Task: qual\n",
      "[[12623     0]\n",
      " [  321     0]]\n",
      "NOT_QUAL F1-score: 0.9874447530019166\n",
      "QUAL F1-score: 0.0\n",
      "Accu: 0.9752008652657602\n",
      "F1-Weighted 0.962956977529604\n",
      "F1-Avg 0.4937223765009583\n",
      "\n",
      "Validation loss 0.5416730344295502\n",
      "\n",
      " [ 0/10]     train_loss: 1.23008 valid_loss: 0.54167 \n",
      "\n",
      "\n",
      "\n",
      "========= Beginning 2 epoch ==========\n",
      "Loss Scale: {'me': 1.0, 'mp': 1.0, 'qual': 1.0}\n",
      "Train loss at 0: 0.5731626152992249\n",
      "\n",
      "====EVALUATING On Validation set :====\n",
      "\n",
      "Loss Scale: {'me': 1.0, 'mp': 1.0, 'qual': 1.0}\n",
      "\n",
      "Task: me\n",
      "[[12163     0]\n",
      " [  781     0]]\n",
      "NOT_ME F1-score: 0.9688931373720476\n",
      "ME F1-score: 0.0\n",
      "Accu: 0.9396631644004945\n",
      "F1-Weighted 0.9104331914289412\n",
      "F1-Avg 0.4844465686860238\n",
      "\n",
      "Task: mp\n",
      "[[12563     0]\n",
      " [  381     0]]\n",
      "NOT_MP F1-score: 0.9850629239032422\n",
      "MP F1-score: 0.0\n",
      "Accu: 0.9705655129789864\n",
      "F1-Weighted 0.9560681020547305\n",
      "F1-Avg 0.4925314619516211\n",
      "\n",
      "Task: qual\n",
      "[[12623     0]\n",
      " [  321     0]]\n",
      "NOT_QUAL F1-score: 0.9874447530019166\n",
      "QUAL F1-score: 0.0\n",
      "Accu: 0.9752008652657602\n",
      "F1-Weighted 0.962956977529604\n",
      "F1-Avg 0.4937223765009583\n",
      "\n",
      "Validation loss 0.48163968324661255\n",
      "\n",
      " [ 1/10]     train_loss: 0.55897 valid_loss: 0.48164 \n",
      "\n",
      "\n",
      "\n",
      "========= Beginning 3 epoch ==========\n",
      "Loss Scale: {'me': 1.0, 'mp': 1.0, 'qual': 1.0}\n",
      "Train loss at 0: 0.5148817896842957\n",
      "\n",
      "====EVALUATING On Validation set :====\n",
      "\n",
      "Loss Scale: {'me': 1.0, 'mp': 1.0, 'qual': 1.0}\n",
      "\n",
      "Task: me\n",
      "[[12163     0]\n",
      " [  781     0]]\n",
      "NOT_ME F1-score: 0.9688931373720476\n",
      "ME F1-score: 0.0\n",
      "Accu: 0.9396631644004945\n",
      "F1-Weighted 0.9104331914289412\n",
      "F1-Avg 0.4844465686860238\n",
      "\n",
      "Task: mp\n",
      "[[12563     0]\n",
      " [  381     0]]\n",
      "NOT_MP F1-score: 0.9850629239032422\n",
      "MP F1-score: 0.0\n",
      "Accu: 0.9705655129789864\n",
      "F1-Weighted 0.9560681020547305\n",
      "F1-Avg 0.4925314619516211\n",
      "\n",
      "Task: qual\n",
      "[[12623     0]\n",
      " [  321     0]]\n",
      "NOT_QUAL F1-score: 0.9874447530019166\n",
      "QUAL F1-score: 0.0\n",
      "Accu: 0.9752008652657602\n",
      "F1-Weighted 0.962956977529604\n",
      "F1-Avg 0.4937223765009583\n",
      "\n",
      "Validation loss 0.4752667546272278\n",
      "\n",
      " [ 2/10]     train_loss: 0.52892 valid_loss: 0.47527 \n",
      "\n",
      "\n",
      "\n",
      "========= Beginning 4 epoch ==========\n",
      "Loss Scale: {'me': 1.0, 'mp': 1.0, 'qual': 1.0}\n",
      "Train loss at 0: 0.5073829293251038\n",
      "\n",
      "====EVALUATING On Validation set :====\n",
      "\n",
      "Loss Scale: {'me': 1.0, 'mp': 1.0, 'qual': 1.0}\n",
      "\n",
      "Task: me\n",
      "[[12163     0]\n",
      " [  781     0]]\n",
      "NOT_ME F1-score: 0.9688931373720476\n",
      "ME F1-score: 0.0\n",
      "Accu: 0.9396631644004945\n",
      "F1-Weighted 0.9104331914289412\n",
      "F1-Avg 0.4844465686860238\n",
      "\n",
      "Task: mp\n",
      "[[12563     0]\n",
      " [  381     0]]\n",
      "NOT_MP F1-score: 0.9850629239032422\n",
      "MP F1-score: 0.0\n",
      "Accu: 0.9705655129789864\n",
      "F1-Weighted 0.9560681020547305\n",
      "F1-Avg 0.4925314619516211\n",
      "\n",
      "Task: qual\n",
      "[[12623     0]\n",
      " [  321     0]]\n",
      "NOT_QUAL F1-score: 0.9874447530019166\n",
      "QUAL F1-score: 0.0\n",
      "Accu: 0.9752008652657602\n",
      "F1-Weighted 0.962956977529604\n",
      "F1-Avg 0.4937223765009583\n",
      "\n",
      "Validation loss 0.4618728905916214\n",
      "\n",
      " [ 3/10]     train_loss: 0.51394 valid_loss: 0.46187 \n",
      "\n",
      "\n",
      "\n",
      "========= Beginning 5 epoch ==========\n",
      "Loss Scale: {'me': 1.0, 'mp': 1.0, 'qual': 1.0}\n",
      "Train loss at 0: 0.489233136177063\n",
      "\n",
      "====EVALUATING On Validation set :====\n",
      "\n",
      "Loss Scale: {'me': 1.0, 'mp': 1.0, 'qual': 1.0}\n",
      "\n",
      "Task: me\n",
      "[[12163     0]\n",
      " [  781     0]]\n",
      "NOT_ME F1-score: 0.9688931373720476\n",
      "ME F1-score: 0.0\n",
      "Accu: 0.9396631644004945\n",
      "F1-Weighted 0.9104331914289412\n",
      "F1-Avg 0.4844465686860238\n",
      "\n",
      "Task: mp\n",
      "[[12563     0]\n",
      " [  381     0]]\n",
      "NOT_MP F1-score: 0.9850629239032422\n",
      "MP F1-score: 0.0\n",
      "Accu: 0.9705655129789864\n",
      "F1-Weighted 0.9560681020547305\n",
      "F1-Avg 0.4925314619516211\n",
      "\n",
      "Task: qual\n",
      "[[12623     0]\n",
      " [  321     0]]\n",
      "NOT_QUAL F1-score: 0.9874447530019166\n",
      "QUAL F1-score: 0.0\n",
      "Accu: 0.9752008652657602\n",
      "F1-Weighted 0.962956977529604\n",
      "F1-Avg 0.4937223765009583\n",
      "\n",
      "Validation loss 0.43468962609767914\n",
      "\n",
      " [ 4/10]     train_loss: 0.48761 valid_loss: 0.43469 \n",
      "\n",
      "\n",
      "\n",
      "========= Beginning 6 epoch ==========\n",
      "Loss Scale: {'me': 1.0, 'mp': 1.0, 'qual': 1.0}\n",
      "Train loss at 0: 0.4620091915130615\n",
      "\n",
      "====EVALUATING On Validation set :====\n",
      "\n",
      "Loss Scale: {'me': 1.0, 'mp': 1.0, 'qual': 1.0}\n",
      "\n",
      "Task: me\n",
      "[[12161     2]\n",
      " [  781     0]]\n",
      "NOT_ME F1-score: 0.9688109938259312\n",
      "ME F1-score: 0.0\n",
      "Accu: 0.939508652657602\n",
      "F1-Weighted 0.9103560041644624\n",
      "F1-Avg 0.4844054969129656\n",
      "\n",
      "Task: mp\n",
      "[[12563     0]\n",
      " [  381     0]]\n",
      "NOT_MP F1-score: 0.9850629239032422\n",
      "MP F1-score: 0.0\n",
      "Accu: 0.9705655129789864\n",
      "F1-Weighted 0.9560681020547305\n",
      "F1-Avg 0.4925314619516211\n",
      "\n",
      "Task: qual\n",
      "[[12623     0]\n",
      " [  321     0]]\n",
      "NOT_QUAL F1-score: 0.9874447530019166\n",
      "QUAL F1-score: 0.0\n",
      "Accu: 0.9752008652657602\n",
      "F1-Weighted 0.962956977529604\n",
      "F1-Avg 0.4937223765009583\n",
      "\n",
      "Validation loss 0.4195682555437088\n",
      "\n",
      " [ 5/10]     train_loss: 0.45994 valid_loss: 0.41957 \n",
      "\n",
      "\n",
      "\n",
      "========= Beginning 7 epoch ==========\n",
      "Loss Scale: {'me': 1.0, 'mp': 1.0, 'qual': 1.0}\n",
      "Train loss at 0: 0.43649783730506897\n",
      "\n",
      "====EVALUATING On Validation set :====\n",
      "\n",
      "Loss Scale: {'me': 1.0, 'mp': 1.0, 'qual': 1.0}\n",
      "\n",
      "Task: me\n",
      "[[12159     4]\n",
      " [  777     4]]\n",
      "NOT_ME F1-score: 0.9688832224391409\n",
      "ME F1-score: 0.010139416983523447\n",
      "Accu: 0.9396631644004945\n",
      "F1-Weighted 0.911035655067321\n",
      "F1-Avg 0.4895113197113322\n",
      "\n",
      "Task: mp\n",
      "[[12563     0]\n",
      " [  381     0]]\n",
      "NOT_MP F1-score: 0.9850629239032422\n",
      "MP F1-score: 0.0\n",
      "Accu: 0.9705655129789864\n",
      "F1-Weighted 0.9560681020547305\n",
      "F1-Avg 0.4925314619516211\n",
      "\n",
      "Task: qual\n",
      "[[12623     0]\n",
      " [  321     0]]\n",
      "NOT_QUAL F1-score: 0.9874447530019166\n",
      "QUAL F1-score: 0.0\n",
      "Accu: 0.9752008652657602\n",
      "F1-Weighted 0.962956977529604\n",
      "F1-Avg 0.4937223765009583\n",
      "\n",
      "Validation loss 0.40935666859149933\n",
      "\n",
      " [ 6/10]     train_loss: 0.44372 valid_loss: 0.40936 \n",
      "\n",
      "\n",
      "\n",
      "========= Beginning 8 epoch ==========\n",
      "Loss Scale: {'me': 1.0, 'mp': 1.0, 'qual': 1.0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss at 0: 0.4228288233280182\n",
      "\n",
      "====EVALUATING On Validation set :====\n",
      "\n",
      "Loss Scale: {'me': 1.0, 'mp': 1.0, 'qual': 1.0}\n",
      "\n",
      "Task: me\n",
      "[[12142    21]\n",
      " [  765    16]]\n",
      "NOT_ME F1-score: 0.9686477861986437\n",
      "ME F1-score: 0.039119804400977995\n",
      "Accu: 0.9392768850432633\n",
      "F1-Weighted 0.9125630091757778\n",
      "F1-Avg 0.5038837952998109\n",
      "\n",
      "Task: mp\n",
      "[[12563     0]\n",
      " [  381     0]]\n",
      "NOT_MP F1-score: 0.9850629239032422\n",
      "MP F1-score: 0.0\n",
      "Accu: 0.9705655129789864\n",
      "F1-Weighted 0.9560681020547305\n",
      "F1-Avg 0.4925314619516211\n",
      "\n",
      "Task: qual\n",
      "[[12623     0]\n",
      " [  321     0]]\n",
      "NOT_QUAL F1-score: 0.9874447530019166\n",
      "QUAL F1-score: 0.0\n",
      "Accu: 0.9752008652657602\n",
      "F1-Weighted 0.962956977529604\n",
      "F1-Avg 0.4937223765009583\n",
      "\n",
      "Validation loss 0.3926658034324646\n",
      "\n",
      " [ 7/10]     train_loss: 0.42228 valid_loss: 0.39267 \n",
      "\n",
      "\n",
      "\n",
      "========= Beginning 9 epoch ==========\n",
      "Loss Scale: {'me': 1.0, 'mp': 1.0, 'qual': 1.0}\n",
      "Train loss at 0: 0.39065706729888916\n",
      "\n",
      "====EVALUATING On Validation set :====\n",
      "\n",
      "Loss Scale: {'me': 1.0, 'mp': 1.0, 'qual': 1.0}\n",
      "\n",
      "Task: me\n",
      "[[12016   147]\n",
      " [  658   123]]\n",
      "NOT_ME F1-score: 0.967588678181745\n",
      "ME F1-score: 0.23406279733587057\n",
      "Accu: 0.9378090234857849\n",
      "F1-Weighted 0.9233300477011649\n",
      "F1-Avg 0.6008257377588078\n",
      "\n",
      "Task: mp\n",
      "[[12563     0]\n",
      " [  381     0]]\n",
      "NOT_MP F1-score: 0.9850629239032422\n",
      "MP F1-score: 0.0\n",
      "Accu: 0.9705655129789864\n",
      "F1-Weighted 0.9560681020547305\n",
      "F1-Avg 0.4925314619516211\n",
      "\n",
      "Task: qual\n",
      "[[12623     0]\n",
      " [  321     0]]\n",
      "NOT_QUAL F1-score: 0.9874447530019166\n",
      "QUAL F1-score: 0.0\n",
      "Accu: 0.9752008652657602\n",
      "F1-Weighted 0.962956977529604\n",
      "F1-Avg 0.4937223765009583\n",
      "\n",
      "Validation loss 0.3775188624858856\n",
      "\n",
      " [ 8/10]     train_loss: 0.39361 valid_loss: 0.37752 \n",
      "\n",
      "\n",
      "\n",
      "========= Beginning 10 epoch ==========\n",
      "Loss Scale: {'me': 1.0, 'mp': 1.0, 'qual': 1.0}\n",
      "Train loss at 0: 0.35463589429855347\n",
      "\n",
      "====EVALUATING On Validation set :====\n",
      "\n",
      "Loss Scale: {'me': 1.0, 'mp': 1.0, 'qual': 1.0}\n",
      "\n",
      "Task: me\n",
      "[[11653   510]\n",
      " [  415   366]]\n",
      "NOT_ME F1-score: 0.9618257603895836\n",
      "ME F1-score: 0.44176222088111045\n",
      "Accu: 0.9285383189122374\n",
      "F1-Weighted 0.9304467721049637\n",
      "F1-Avg 0.701793990635347\n",
      "\n",
      "Task: mp\n",
      "[[12515    48]\n",
      " [  357    24]]\n",
      "NOT_MP F1-score: 0.9840770591704344\n",
      "MP F1-score: 0.10596026490066225\n",
      "Accu: 0.9687113720642769\n",
      "F1-Weighted 0.9582301417865668\n",
      "F1-Avg 0.5450186620355484\n",
      "\n",
      "Task: qual\n",
      "[[12623     0]\n",
      " [  321     0]]\n",
      "NOT_QUAL F1-score: 0.9874447530019166\n",
      "QUAL F1-score: 0.0\n",
      "Accu: 0.9752008652657602\n",
      "F1-Weighted 0.962956977529604\n",
      "F1-Avg 0.4937223765009583\n",
      "\n",
      "Validation loss 0.3740634173154831\n",
      "\n",
      " [ 9/10]     train_loss: 0.35681 valid_loss: 0.37406 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "############# Load dataset #############\n",
    "train_dataset = MEDataset(train_doc_path, train_label_path, bert_type=bert_type)\n",
    "valid_dataset = MEDataset(trial_doc_path, trial_label_path, bert_type=bert_type)\n",
    "\n",
    "print(\"Dataset created\")\n",
    "\n",
    "############# Create model #############\n",
    "\n",
    "class OurBERTModel(nn.Module):\n",
    "    def __init__(self, bert_type=\"bert-base-cased\", tasks=['mp', 'me', 'qual']):\n",
    "        super(OurBERTModel, self).__init__()\n",
    "        self.bert = AutoModel.from_pretrained(bert_type)\n",
    "        self.drop = nn.Dropout(self.bert.config.hidden_dropout_prob)\n",
    "        self.me_classifier = nn.Linear(self.bert.config.hidden_size, 2)\n",
    "        if \"me\" in tasks:\n",
    "            self.non_lin = nn.Softmax(2)\n",
    "            self.mp_classifier = nn.Linear(self.bert.config.hidden_size+3, 2)\n",
    "            self.qual_classifier = nn.Linear(self.bert.config.hidden_size+3, 2)\n",
    "        else:\n",
    "            self.mp_classifier = nn.Linear(self.bert.config.hidden_size, 2)\n",
    "            self.qual_classifier = nn.Linear(self.bert.config.hidden_size, 2)\n",
    "        \n",
    "\n",
    "    def forward(self, text, att_mask, token_type=None):\n",
    "        b, num_tokens = text.shape\n",
    "        if token_type == None:\n",
    "            token_type = torch.zeros((b, num_tokens), dtype=torch.long).to(device)\n",
    "        \n",
    "        if \"roberta\" in bert_type:\n",
    "            outputs = self.bert(text, attention_mask=att_mask)\n",
    "        else:\n",
    "            outputs = self.bert(text, attention_mask=att_mask, token_type_ids=token_type)\n",
    "        classifier_in = self.drop(outputs['last_hidden_state'])\n",
    "\n",
    "        tags = {}\n",
    "        if 'me' in tasks:\n",
    "            tags['me'] = self.me_classifier(classifier_in)\n",
    "            softmaxed = self.non_lin(tags['me'])\n",
    "            # Extract features from logits of me and concatenate\n",
    "            mean_feats = (att_mask.unsqueeze(-1) * softmaxed).sum(1)/att_mask.sum(1).unsqueeze(-1)\n",
    "            max_feats = (softmaxed[:, :, 1].max(1)[0]).unsqueeze(-1)\n",
    "            extra_feats = torch.cat([mean_feats, max_feats], -1).unsqueeze(1).expand(-1, num_tokens, -1)\n",
    "            \n",
    "            classifier_in = torch.cat([classifier_in, extra_feats], -1)\n",
    "\n",
    "        if \"mp\" in tasks:\n",
    "            tags['mp'] = self.mp_classifier(classifier_in)\n",
    "        if \"qual\" in tasks:\n",
    "            tags['qual'] = self.qual_classifier(classifier_in)\n",
    "\n",
    "        return tags\n",
    "\n",
    "model = OurBERTModel(bert_type=bert_type, tasks = ['mp', 'me', 'qual'])\n",
    "print(\"Model created\")\n",
    "os.system(\"nvidia-smi\")\n",
    "\n",
    "model.bert.resize_token_embeddings(len(train_dataset.bert_tok))\n",
    "# print(\"Embeddings shape:\", model.bert.embeddings.word_embeddings.weight.data.size())\n",
    "# embedding_size = model.bert.embeddings.word_embeddings.weight.size(1)\n",
    "# new_embeddings = torch.FloatTensor(2, embedding_size).uniform_(-0.1, 0.1)\n",
    "# print(\"new_embeddings shape:\", new_embeddings.size())\n",
    "# new_embedding_weight = torch.cat((model.bert.embeddings.word_embeddings.weight.data,new_embeddings), 0)\n",
    "# model.bert.embeddings.word_embeddings.weight.data = new_embedding_weight\n",
    "# print(\"Updated Embeddings shape:\", model.bert.embeddings.word_embeddings.weight.data.size())\n",
    "# Update model config vocab size\n",
    "model.bert.config.vocab_size = model.bert.config.vocab_size + 2\n",
    "\n",
    "print(sum(p.numel() for p in model.parameters()))\n",
    "model = model.to(device)\n",
    "print(\"Detected\", torch.cuda.device_count(), \"GPUs!\")\n",
    "# model = torch.nn.DataParallel(model)\n",
    "\n",
    "########## Optimizer & Loss ###########\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss(reduction='none')\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=3e-5)\n",
    "\n",
    "########## Training loop ###########\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    print(\"\\n\\n========= Beginning\", epoch+1, \"epoch ==========\")\n",
    "\n",
    "    train_loss = train(model, train_dataset, criterion, epoch)\n",
    "    # print(\"\\n====EVALUATING On Training set :====\\n\")\n",
    "    # _ = evaluate(model, train_dataset, criterion, epoch)\n",
    "\n",
    "    print(\"\\n====EVALUATING On Validation set :====\\n\")\n",
    "    valid_loss = evaluate(model, tasks, valid_dataset, criterion, epoch)\n",
    "\n",
    "    epoch_len = len(str(n_epochs))\n",
    "    print_msg = (f'[{epoch:>{epoch_len}}/{n_epochs:>{epoch_len}}]     ' +\n",
    "                    f'train_loss: {train_loss:.5f} ' +\n",
    "                    f'valid_loss: {valid_loss:.5f}')\n",
    "    print(\"\\n\", print_msg, \"\\n\")\n",
    "\n",
    "\n",
    "## Predict on Test set\n",
    "\n",
    "def predict_spans(dataset, save_path, tasks):\n",
    "    model.eval()\n",
    "    predicts = {}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        all_tasks = []\n",
    "        if \"me\" in tasks: \n",
    "            all_tasks.append('me')\n",
    "        if \"mp\" in tasks: \n",
    "            all_tasks.append('mp')\n",
    "        if \"qual\" in tasks: \n",
    "            all_tasks.append('qual')\n",
    "        \n",
    "        for task_type in all_tasks:\n",
    "            this_labels_added = []\n",
    "            for batch in dataset.batched_dataset:\n",
    "                (texts, token_type_ids, att_masks, token_offsets, quant_data) = batch\n",
    "                preds = model(texts, att_masks)\n",
    "\n",
    "                \n",
    "                preds = preds[task_type]\n",
    "                for sent_preds, sent_att_masks, sent_token_offsets, sent_quant_data in zip(preds, att_masks, token_offsets, quant_data):\n",
    "\n",
    "                    this_sentence_positives = []\n",
    "                    curr_positive_idx = -1\n",
    "                    for i, (token_preds, token_masks) in enumerate(zip(sent_preds, sent_att_masks)):\n",
    "                        if token_masks.item() != 0:\n",
    "                            if token_preds.argmax().item() == 1:\n",
    "                                if curr_positive_idx == -1:\n",
    "                                    curr_positive_idx = i\n",
    "                            else:\n",
    "                                if curr_positive_idx != -1:\n",
    "                                    this_sentence_positives.append([curr_positive_idx, i-1])\n",
    "                                    curr_positive_idx = -1\n",
    "                                    break\n",
    "                        else:\n",
    "                            if curr_positive_idx != -1:\n",
    "                                this_sentence_positives.append([curr_positive_idx, i-1])\n",
    "                                curr_positive_idx = -1\n",
    "                                break\n",
    "\n",
    "                    # Here convert indices to offsets\n",
    "                    # assert len(this_sentence_positives) == 1\n",
    "                    if len(this_sentence_positives) != 0:\n",
    "                        span_offsets = this_sentence_positives[0]\n",
    "                        this_label_offs = (sent_token_offsets[span_offsets[0]][0], sent_token_offsets[span_offsets[1]][1])\n",
    "                        this_labels_added.append([sent_quant_data, (task_type, this_label_offs )])\n",
    "\n",
    "            json.dump(this_labels_added, open(save_path + '_' + task_type + \".json\", 'w+'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "241dca5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "248 248\n",
      "Loaded and processed data\n",
      "Loaded tokenizer\n",
      "Flattened doc - 846 \n",
      "Some examples: [['Data were drawn from the Whitehall II study with baseline examination in 0000; follow-up screenings in 0000, 0000, and 0000; and additional disease ascertainment from hospital data and registry linkage on 0000 participants (mean age 00.0 years, 00% women) without depressive symptoms at baseline.', ['S0006322312001096-1136', 1, 'Quantity', 73, 78, 1, '1991;', \"{'mods': ['IsCount']}\", 0, 296], [73, 78]], ['Data were drawn from the Whitehall II study with baseline examination in 0000; follow-up screenings in 0000, 0000, and 0000; and additional disease ascertainment from hospital data and registry linkage on 0000 participants (mean age 00.0 years, 00% women) without depressive symptoms at baseline.', ['S0006322312001096-1136', 2, 'Quantity', 103, 114, 2, '1997, 2003,', \"{'mods': ['IsCount']}\", 0, 296], [103, 114]]]\n",
      "CLS, SEP, PAD, <E>, </E> tokens are: 0 2 1 50265 50266\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [41]\u001b[0m, in \u001b[0;36m<cell line: 6>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m     os\u001b[38;5;241m.\u001b[39msystem(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrm -rf \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m folder_name)\n\u001b[0;32m      4\u001b[0m os\u001b[38;5;241m.\u001b[39mmkdir(folder_name)\n\u001b[1;32m----> 6\u001b[0m predict_train_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mMETestDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_doc_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_label_folder\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/train_labels.json\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m predict_spans(predict_train_dataset, folder_name \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/train_spans\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      9\u001b[0m predict_trial_dataset \u001b[38;5;241m=\u001b[39m METestDataset(trial_doc_path, test_label_folder \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/trial_labels.json\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Input \u001b[1;32mIn [30]\u001b[0m, in \u001b[0;36mMETestDataset.__init__\u001b[1;34m(self, text_path, quant_label_path)\u001b[0m\n\u001b[0;32m     23\u001b[0m new_special_tokens_dict \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124madditional_special_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<E>\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m</E>\u001b[39m\u001b[38;5;124m\"\u001b[39m]}\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbert_tok\u001b[38;5;241m.\u001b[39madd_special_tokens(new_special_tokens_dict)\n\u001b[1;32m---> 26\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatched_dataset \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mall_data\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[1;32mIn [30]\u001b[0m, in \u001b[0;36mMETestDataset.batch_dataset\u001b[1;34m(self, sentence_mapped_data)\u001b[0m\n\u001b[0;32m     98\u001b[0m     batch_raw_text\u001b[38;5;241m.\u001b[39mappend(single_raw_text)\n\u001b[0;32m     99\u001b[0m     batch_raw_labels\u001b[38;5;241m.\u001b[39mappend(single_raw_anns)\n\u001b[1;32m--> 101\u001b[0m batched_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbert_tok\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_encode_plus\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_raw_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    102\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    105\u001b[0m \u001b[38;5;66;03m# Create sequence labels using token offsets\u001b[39;00m\n\u001b[0;32m    106\u001b[0m no_special_batch_tokens \u001b[38;5;241m=\u001b[39m batched_dict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[1;32m~\\.conda\\envs\\measeval\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2503\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.batch_encode_plus\u001b[1;34m(self, batch_text_or_text_pairs, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[0;32m   2493\u001b[0m \u001b[38;5;66;03m# Backward compatibility for 'truncation_strategy', 'pad_to_max_length'\u001b[39;00m\n\u001b[0;32m   2494\u001b[0m padding_strategy, truncation_strategy, max_length, kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_padding_truncation_strategies(\n\u001b[0;32m   2495\u001b[0m     padding\u001b[38;5;241m=\u001b[39mpadding,\n\u001b[0;32m   2496\u001b[0m     truncation\u001b[38;5;241m=\u001b[39mtruncation,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2500\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   2501\u001b[0m )\n\u001b[1;32m-> 2503\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_batch_encode_plus\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2504\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_text_or_text_pairs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_text_or_text_pairs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2505\u001b[0m \u001b[43m    \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2506\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpadding_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2507\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtruncation_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtruncation_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2508\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2509\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstride\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2510\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_split_into_words\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_split_into_words\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2511\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2512\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2513\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2514\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2515\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2516\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2517\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2518\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2519\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2520\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2521\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\.conda\\envs\\measeval\\lib\\site-packages\\transformers\\models\\gpt2\\tokenization_gpt2_fast.py:159\u001b[0m, in \u001b[0;36mGPT2TokenizerFast._batch_encode_plus\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    153\u001b[0m is_split_into_words \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis_split_into_words\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    154\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd_prefix_space \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_split_into_words, (\n\u001b[0;32m    155\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou need to instantiate \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m with add_prefix_space=True \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    156\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mto use it with pretokenized inputs.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    157\u001b[0m )\n\u001b[1;32m--> 159\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_batch_encode_plus\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\.conda\\envs\\measeval\\lib\\site-packages\\transformers\\tokenization_utils_fast.py:408\u001b[0m, in \u001b[0;36mPreTrainedTokenizerFast._batch_encode_plus\u001b[1;34m(self, batch_text_or_text_pairs, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose)\u001b[0m\n\u001b[0;32m    401\u001b[0m \u001b[38;5;66;03m# Convert the output to have dict[list] from list[dict] and remove the additional overflows dimension\u001b[39;00m\n\u001b[0;32m    402\u001b[0m \u001b[38;5;66;03m# From (variable) shape (batch, overflows, sequence length) to ~ (batch * overflows, sequence length)\u001b[39;00m\n\u001b[0;32m    403\u001b[0m \u001b[38;5;66;03m# (we say ~ because the number of overflow varies with the example in the batch)\u001b[39;00m\n\u001b[0;32m    404\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m    405\u001b[0m \u001b[38;5;66;03m# To match each overflowing sample with the original sample in the batch\u001b[39;00m\n\u001b[0;32m    406\u001b[0m \u001b[38;5;66;03m# we add an overflow_to_sample_mapping array (see below)\u001b[39;00m\n\u001b[0;32m    407\u001b[0m sanitized_tokens \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m--> 408\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m \u001b[43mtokens_and_encodings\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[0;32m    409\u001b[0m     stack \u001b[38;5;241m=\u001b[39m [e \u001b[38;5;28;01mfor\u001b[39;00m item, _ \u001b[38;5;129;01min\u001b[39;00m tokens_and_encodings \u001b[38;5;28;01mfor\u001b[39;00m e \u001b[38;5;129;01min\u001b[39;00m item[key]]\n\u001b[0;32m    410\u001b[0m     sanitized_tokens[key] \u001b[38;5;241m=\u001b[39m stack\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "folder_name = output_folder + bert_type.replace('/', '_') + \"_final_preds_\" + datetime.today().strftime('%Y-%m-%d_%H_%M_%S')\n",
    "if os.path.isdir(folder_name):\n",
    "    os.system(\"rm -rf \" + folder_name)\n",
    "os.mkdir(folder_name)\n",
    "\n",
    "predict_train_dataset = METestDataset(train_doc_path, test_label_folder + \"/train_labels.json\")\n",
    "predict_spans(predict_train_dataset, folder_name + \"/train_spans\")\n",
    "\n",
    "predict_trial_dataset = METestDataset(trial_doc_path, test_label_folder + \"/trial_labels.json\")\n",
    "predict_spans(predict_trial_dataset, folder_name + \"/trial_spans\")\n",
    "\n",
    "predict_test_dataset = METestDataset(eval_doc_path, test_label_folder + \"/test_labels.json\")\n",
    "predict_spans(predict_test_dataset, folder_name + \"/test_spans\")\n",
    "\n",
    "# don't really save this off...\n",
    "# torch.save(model.state_dict(), folder_name+\"/model.pt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "measeval-kernel",
   "language": "python",
   "name": "measeval-kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
