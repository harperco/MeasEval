{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making a few baselines available\n",
    "\n",
    "Okay, so this is mostly working, but there are some issues\n",
    "\n",
    "**Note -- all the paths in the next cell, as well as any place where submission output is writte to \"subdir\", those paths need to be updated. The subdir paths should be empty directories where .tsv files will be output.**\n",
    "\n",
    "Pipeline here is:\n",
    "* Train each spaCy entity model separately\n",
    "* Predict entities from each model and collect them \n",
    "* Work out extra Quantity components.\n",
    "    * [TODO] Modifiers -- Will probably just do these as a series regex\n",
    "    * Units -- Doing this one in a straight brute force matching thing -- take last longest mathing string of any unit found in the training data. \n",
    "* Relationships and alignment.\n",
    "    * This is the really hard bit. \n",
    "    * Initially I tried to align based on relationships using the dependency parse trick in the example here, but it didn't quite work the way I wanted: https://spacy.io/usage/examples#entity-relations\n",
    "    * Below, there are two simple versions. \n",
    "        * First is incredibly naive, and just takes each predicted span in the order they are found in the text.\n",
    "        * Second is slightly more complex, matching each span to it's nearest neighbor in the text and knocking them out to prevent reuse.\n",
    "        * [TODO] Third possibility will be to only rely on SpaCy predictions for the Quantities, then use nearest noun phrase chunks to approximate the other related components.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/sam/MeasEval/baselines\n",
      "/home/sam/MeasEval/baselines/../data/trial/tsv\n"
     ]
    }
   ],
   "source": [
    "# A few imports and set up our paths\n",
    "import itertools\n",
    "import spacy\n",
    "import random\n",
    "import os\n",
    "from spacy.util import minibatch, compounding\n",
    "from spacy.training.example import Example\n",
    "nlp = spacy.load(\"en_core_web_trf\")\n",
    "\n",
    "currentdir = os.getcwd() # ~/MeasEval/baselines\n",
    "print(currentdir)\n",
    "filename = os.path.join(currentdir, '../data/trial/tsv')\n",
    "print(filename)\n",
    "\n",
    "trainpaths = [os.path.join(currentdir, \"../data/trial/tsv/\"),\n",
    "             os.path.join(currentdir, \"../data/train/tsv/\")]\n",
    "\n",
    "evalpath = os.path.join(currentdir, \"../data/eval/text/\")\n",
    "\n",
    "textpaths = [os.path.join(currentdir, \"../data/trial/txt/\"),\n",
    "            os.path.join(currentdir, \"../data/train/text/\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set shorthands for annotation spans\n",
    "typemap = {\"Quantity\": \"QUANT\",\n",
    "           \"MeasuredEntity\": \"ME\", \n",
    "           \"MeasuredProperty\": \"MP\", \n",
    "           \"Qualifier\": \"QUAL\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect all the ids and all the text files in both the train and trial directories\n",
    "# Set our train test split for doing initial model development.\n",
    "docIds = []\n",
    "textset = {}\n",
    "for fileset in textpaths:\n",
    "    for fn in os.listdir(fileset):\n",
    "        with open(fileset+fn) as textfile:\n",
    "            text = textfile.read() #.splitlines()\n",
    "            #print(fn[:-4])\n",
    "            textset[fn[:-4]] = text\n",
    "            docIds.append(fn[:-4])\n",
    "\n",
    "random.seed(42)\n",
    "random.shuffle(docIds)\n",
    "\n",
    "trainIds = docIds[:220]\n",
    "testIds = docIds[220:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build training data from TSVs in expected format for spacy NER models...\n",
    "# We have to train each model separately, because spacy doesn't let us have \n",
    "# Multiple entities that overlap, and we have this a lot (Especially in our Qualifiers)\n",
    "# Unfortunately, we even have a fair bit of overlap within annotation types, \n",
    "# and end up needing to throw away a bunch of training data.\n",
    "\n",
    "# Note that we have data split for train / test, and we also have full training data.\n",
    "\n",
    "trainents = {\"QUANT\": [], \"ME\": [], \"MP\": [], \"QUAL\": []}\n",
    "traindata = {\"QUANT\": [], \"ME\": [], \"MP\": [], \"QUAL\": []}\n",
    "testents = {\"QUANT\": [], \"ME\": [], \"MP\": [], \"QUAL\": []}\n",
    "testdata = {\"QUANT\": [], \"ME\": [], \"MP\": [], \"QUAL\": []}\n",
    "\n",
    "alltrainents = {\"QUANT\": [], \"ME\": [], \"MP\": [], \"QUAL\": []}\n",
    "alltraindata = {\"QUANT\": [], \"ME\": [], \"MP\": [], \"QUAL\": []}\n",
    "\n",
    "for fileset in trainpaths:\n",
    "    for fn in os.listdir(fileset):\n",
    "        entities = {\"QUANT\": [], \"ME\": [], \"MP\": [], \"QUAL\": []}\n",
    "        with open(fileset+fn) as annotfile:\n",
    "            text = textset[fn[:-4]]\n",
    "            next(annotfile)\n",
    "            annots = annotfile.read().splitlines()\n",
    "            for a in annots:\n",
    "                annot = a.split(\"\\t\")\n",
    "                atype = typemap[annot[2]]\n",
    "                start = int(annot[3])\n",
    "                stop = int(annot[4])\n",
    "                # This is where we toss out the overlaps:\n",
    "                overlap = False\n",
    "                for ent in entities[atype]:\n",
    "                    if ((start >= ent[0] and start <= ent[1]) or (stop >= ent[0] and stop <= ent[1]) or\n",
    "                        (ent[0] >= start and ent[0] <= stop) or (ent[1] >= start and ent[1] <= stop)):\n",
    "                        #print(str(start)+\"-\"+str(stop)+\" overlaps \" + str(ent))\n",
    "                        overlap = True\n",
    "                if overlap == False:    \n",
    "                    entities[atype].append((start, stop, atype))\n",
    "            if fn[:-4] in trainIds:\n",
    "                traindata[\"QUANT\"].append((text, {\"entities\": entities[\"QUANT\"]}))\n",
    "                traindata[\"ME\"].append((text, {\"entities\": entities[\"ME\"]}))\n",
    "                traindata[\"MP\"].append((text, {\"entities\": entities[\"MP\"]}))\n",
    "                traindata[\"QUAL\"].append((text, {\"entities\": entities[\"QUAL\"]}))\n",
    "                trainents[\"QUANT\"].extend(entities[\"QUANT\"])\n",
    "                trainents[\"ME\"].extend(entities[\"ME\"])\n",
    "                trainents[\"MP\"].extend(entities[\"MP\"])\n",
    "                trainents[\"QUAL\"].extend(entities[\"QUAL\"])\n",
    "            else:\n",
    "                testdata[\"QUANT\"].append((text, {\"entities\": entities[\"QUANT\"]}))\n",
    "                testdata[\"ME\"].append((text, {\"entities\": entities[\"ME\"]}))\n",
    "                testdata[\"MP\"].append((text, {\"entities\": entities[\"MP\"]}))\n",
    "                testdata[\"QUAL\"].append((text, {\"entities\": entities[\"QUAL\"]}))\n",
    "                testents[\"QUANT\"].extend(entities[\"QUANT\"])\n",
    "                testents[\"ME\"].extend(entities[\"ME\"])\n",
    "                testents[\"MP\"].extend(entities[\"MP\"])\n",
    "                testents[\"QUAL\"].extend(entities[\"QUAL\"])\n",
    "            alltraindata[\"QUANT\"].append((text, {\"entities\": entities[\"QUANT\"]}))\n",
    "            alltraindata[\"ME\"].append((text, {\"entities\": entities[\"ME\"]}))\n",
    "            alltraindata[\"MP\"].append((text, {\"entities\": entities[\"MP\"]}))\n",
    "            alltraindata[\"QUAL\"].append((text, {\"entities\": entities[\"QUAL\"]}))\n",
    "            alltrainents[\"QUANT\"].extend(entities[\"QUANT\"])\n",
    "            alltrainents[\"ME\"].extend(entities[\"ME\"])\n",
    "            alltrainents[\"MP\"].extend(entities[\"MP\"])\n",
    "            alltrainents[\"QUAL\"].extend(entities[\"QUAL\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:\n",
      "QUANT: 825\n",
      "ME: 652\n",
      "MP: 454\n",
      "QUAL: 183\n",
      "Total: 2114\n",
      "\n",
      "Test:\n",
      "QUANT: 339\n",
      "ME: 259\n",
      "MP: 197\n",
      "QUAL: 95\n",
      "Total: 890\n",
      "\n",
      "All training:\n",
      "QUANT: 1164\n",
      "ME: 911\n",
      "MP: 651\n",
      "QUAL: 278\n",
      "Total: 3004\n"
     ]
    }
   ],
   "source": [
    "# We don't throw out _that_ many, see counts below.\n",
    "print(\"Training:\")\n",
    "entcount = 0\n",
    "for t in [\"QUANT\", \"ME\", \"MP\", \"QUAL\"]:\n",
    "    print(t + \": \" + str(len(trainents[t])))\n",
    "    entcount+=len(trainents[t])\n",
    "print(\"Total: \" + str(entcount))\n",
    "entcount = 0\n",
    "\n",
    "print(\"\\nTest:\")\n",
    "for t in [\"QUANT\", \"ME\", \"MP\", \"QUAL\"]:\n",
    "    print(t + \": \" + str(len(testents[t])))\n",
    "    entcount+=len(testents[t])\n",
    "print(\"Total: \" + str(entcount))\n",
    "entcount = 0\n",
    "\n",
    "print(\"\\nAll training:\")\n",
    "for t in [\"QUANT\", \"ME\", \"MP\", \"QUAL\"]:\n",
    "    print(t + \": \" + str(len(alltrainents[t])))\n",
    "    entcount+=len(alltrainents[t])\n",
    "print(\"Total: \" + str(entcount))\n",
    "# Before filtering overlaps:\n",
    "# QUANT: 1164\n",
    "# ME: 1148\n",
    "# MP: 742\n",
    "# QUAL: 309\n",
    "\n",
    "# Only filtered the one direction:\n",
    "# QUANT: 1164\n",
    "# ME: 914\n",
    "# MP: 651\n",
    "# QUAL: 278\n",
    "\n",
    "# From the full set\n",
    "# QUANT: 1164\n",
    "# ME: 911\n",
    "# MP: 651\n",
    "# QUAL: 278"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6907216494845361\n",
      "0.30927835051546393\n"
     ]
    }
   ],
   "source": [
    "#check to make sure we're close to a 70/30 split. :)\n",
    "print(804/(804+360))\n",
    "print(360/(804+360))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training for QUANT\n",
      "['ner']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sam/measeval/lib/python3.7/site-packages/spacy/training/iob_utils.py:144: UserWarning: [W030] Some entities could not be aligned in the text \"High latitude temperatures in Saturn’s upper atmos...\" with entities \"[(92, 104, 'QUANT'), (464, 474, 'QUANT'), (529, 54...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  entities=ent_str[:50] + \"...\" if len(ent_str) > 50 else ent_str,\n",
      "/home/sam/measeval/lib/python3.7/site-packages/spacy/training/iob_utils.py:144: UserWarning: [W030] Some entities could not be aligned in the text \"θ=tan−1(w/U) against mean wind speed (gate midpoin...\" with entities \"[(52, 57, 'QUANT'), (81, 91, 'QUANT')]\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  entities=ent_str[:50] + \"...\" if len(ent_str) > 50 else ent_str,\n",
      "/home/sam/measeval/lib/python3.7/site-packages/spacy/training/iob_utils.py:144: UserWarning: [W030] Some entities could not be aligned in the text \"The final scenario (Fig. 3d) was solved with the n...\" with entities \"[(130, 134, 'QUANT'), (178, 183, 'QUANT')]\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  entities=ent_str[:50] + \"...\" if len(ent_str) > 50 else ent_str,\n",
      "/home/sam/measeval/lib/python3.7/site-packages/spacy/training/iob_utils.py:144: UserWarning: [W030] Some entities could not be aligned in the text \"We use data taken by the Low Energy Magnetospheric...\" with entities \"[(313, 357, 'QUANT'), (605, 608, 'QUANT'), (713, 7...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  entities=ent_str[:50] + \"...\" if len(ent_str) > 50 else ent_str,\n",
      "/home/sam/measeval/lib/python3.7/site-packages/spacy/training/iob_utils.py:144: UserWarning: [W030] Some entities could not be aligned in the text \"Polythiophene (PTh) was purchased from Sigma-Aldri...\" with entities \"[(194, 199, 'QUANT'), (407, 413, 'QUANT'), (453, 4...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  entities=ent_str[:50] + \"...\" if len(ent_str) > 50 else ent_str,\n",
      "/home/sam/measeval/lib/python3.7/site-packages/spacy/training/iob_utils.py:144: UserWarning: [W030] Some entities could not be aligned in the text \"SQUID magnetometry measurements revealed that both...\" with entities \"[(159, 178, 'QUANT')]\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  entities=ent_str[:50] + \"...\" if len(ent_str) > 50 else ent_str,\n",
      "/home/sam/measeval/lib/python3.7/site-packages/spacy/training/iob_utils.py:144: UserWarning: [W030] Some entities could not be aligned in the text \"Prior to VESTAL analyses, a 5–55 Hz bandpass filte...\" with entities \"[(28, 35, 'QUANT'), (99, 108, 'QUANT'), (298, 304,...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  entities=ent_str[:50] + \"...\" if len(ent_str) > 50 else ent_str,\n",
      "/home/sam/measeval/lib/python3.7/site-packages/spacy/training/iob_utils.py:144: UserWarning: [W030] Some entities could not be aligned in the text \"Density profiles O and H based on the C2 model (Pa...\" with entities \"[(149, 155, 'QUANT')]\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  entities=ent_str[:50] + \"...\" if len(ent_str) > 50 else ent_str,\n",
      "/home/sam/measeval/lib/python3.7/site-packages/spacy/training/iob_utils.py:144: UserWarning: [W030] Some entities could not be aligned in the text \"The PETM was a period of geologically-rapid global...\" with entities \"[(100, 107, 'QUANT'), (177, 183, 'QUANT'), (382, 3...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  entities=ent_str[:50] + \"...\" if len(ent_str) > 50 else ent_str,\n",
      "/home/sam/measeval/lib/python3.7/site-packages/spacy/training/iob_utils.py:144: UserWarning: [W030] Some entities could not be aligned in the text \"Based on culture experiments (Sutton et al., 2013)...\" with entities \"[(347, 351, 'QUANT'), (604, 607, 'QUANT'), (848, 8...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  entities=ent_str[:50] + \"...\" if len(ent_str) > 50 else ent_str,\n",
      "/home/sam/measeval/lib/python3.7/site-packages/spacy/training/iob_utils.py:144: UserWarning: [W030] Some entities could not be aligned in the text \"The performance and morphology of vacuum co-deposi...\" with entities \"[(430, 437, 'QUANT'), (464, 469, 'QUANT'), (573, 5...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  entities=ent_str[:50] + \"...\" if len(ent_str) > 50 else ent_str,\n",
      "/home/sam/measeval/lib/python3.7/site-packages/spacy/training/iob_utils.py:144: UserWarning: [W030] Some entities could not be aligned in the text \"The situation for the 45° case is more interesting...\" with entities \"[(22, 25, 'QUANT'), (148, 154, 'QUANT'), (528, 534...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  entities=ent_str[:50] + \"...\" if len(ent_str) > 50 else ent_str,\n",
      "/home/sam/measeval/lib/python3.7/site-packages/spacy/training/iob_utils.py:144: UserWarning: [W030] Some entities could not be aligned in the text \"Comparison of the performance of ITO/AALD ZnO/P3HT...\" with entities \"[(81, 87, 'QUANT'), (152, 155, 'QUANT'), (303, 311...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  entities=ent_str[:50] + \"...\" if len(ent_str) > 50 else ent_str,\n",
      "/home/sam/measeval/lib/python3.7/site-packages/spacy/training/iob_utils.py:144: UserWarning: [W030] Some entities could not be aligned in the text \"Cleaned sponge and diatom opal was dissolved via w...\" with entities \"[(123, 128, 'QUANT'), (137, 143, 'QUANT'), (148, 1...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  entities=ent_str[:50] + \"...\" if len(ent_str) > 50 else ent_str,\n",
      "/home/sam/measeval/lib/python3.7/site-packages/spacy/training/iob_utils.py:144: UserWarning: [W030] Some entities could not be aligned in the text \"•SOC stocks decreased by 12.4% in Costa Rica and 0...\" with entities \"[(25, 30, 'QUANT'), (49, 54, 'QUANT'), (131, 140, ...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  entities=ent_str[:50] + \"...\" if len(ent_str) > 50 else ent_str,\n",
      "/home/sam/measeval/lib/python3.7/site-packages/spacy/training/iob_utils.py:144: UserWarning: [W030] Some entities could not be aligned in the text \"In Paper I we noted that the stellar XUV flux, or ...\" with entities \"[(110, 114, 'QUANT'), (182, 205, 'QUANT'), (812, 8...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  entities=ent_str[:50] + \"...\" if len(ent_str) > 50 else ent_str,\n",
      "/home/sam/measeval/lib/python3.7/site-packages/spacy/training/iob_utils.py:144: UserWarning: [W030] Some entities could not be aligned in the text \"Two stock solutions of each standard compound were...\" with entities \"[(0, 3, 'QUANT'), (110, 118, 'QUANT'), (123, 129, ...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  entities=ent_str[:50] + \"...\" if len(ent_str) > 50 else ent_str,\n",
      "/home/sam/measeval/lib/python3.7/site-packages/spacy/training/iob_utils.py:144: UserWarning: [W030] Some entities could not be aligned in the text \"The substellar tide is included in the C3 model. W...\" with entities \"[(515, 522, 'QUANT'), (571, 582, 'QUANT')]\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  entities=ent_str[:50] + \"...\" if len(ent_str) > 50 else ent_str,\n",
      "/home/sam/measeval/lib/python3.7/site-packages/spacy/training/iob_utils.py:144: UserWarning: [W030] Some entities could not be aligned in the text \"All analyses were carried out at the NERC Isotope ...\" with entities \"[(146, 149, 'QUANT'), (324, 329, 'QUANT'), (397, 4...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  entities=ent_str[:50] + \"...\" if len(ent_str) > 50 else ent_str,\n",
      "/home/sam/measeval/lib/python3.7/site-packages/spacy/training/iob_utils.py:144: UserWarning: [W030] Some entities could not be aligned in the text \"It is well known that the seismic velocity in sand...\" with entities \"[(117, 134, 'QUANT'), (383, 386, 'QUANT'), (459, 4...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  entities=ent_str[:50] + \"...\" if len(ent_str) > 50 else ent_str,\n",
      "/home/sam/measeval/lib/python3.7/site-packages/spacy/training/iob_utils.py:144: UserWarning: [W030] Some entities could not be aligned in the text \"The boundary conditions were as follows. On the to...\" with entities \"[(438, 444, 'QUANT'), (481, 487, 'QUANT'), (566, 5...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  entities=ent_str[:50] + \"...\" if len(ent_str) > 50 else ent_str,\n",
      "/home/sam/measeval/lib/python3.7/site-packages/spacy/training/iob_utils.py:144: UserWarning: [W030] Some entities could not be aligned in the text \"A net heating efficiency of 50% is similar to the ...\" with entities \"[(28, 31, 'QUANT'), (263, 271, 'QUANT'), (308, 318...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  entities=ent_str[:50] + \"...\" if len(ent_str) > 50 else ent_str,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Losses {'ner': 2353.83859668039}\n",
      "Losses {'ner': 965.0986722209691}\n",
      "Losses {'ner': 787.2303246247621}\n",
      "Losses {'ner': 620.7044063112965}\n",
      "Losses {'ner': 556.0968800558812}\n",
      "Losses {'ner': 462.9948807736022}\n",
      "Losses {'ner': 436.63124581333625}\n",
      "Losses {'ner': 386.21123878800404}\n",
      "Losses {'ner': 364.1915256809784}\n",
      "Losses {'ner': 332.32168960198794}\n",
      "Losses {'ner': 303.46224976986264}\n",
      "Losses {'ner': 310.79711211387513}\n",
      "Losses {'ner': 302.4420422295791}\n",
      "Losses {'ner': 253.5317691886503}\n",
      "Losses {'ner': 244.4961642824048}\n",
      "Losses {'ner': 233.83045522314163}\n",
      "Losses {'ner': 218.3238722513621}\n",
      "Losses {'ner': 184.7773706759468}\n",
      "Losses {'ner': 155.43416707189772}\n",
      "Losses {'ner': 147.85358727399316}\n",
      "Starting training for ME\n",
      "['ner']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sam/measeval/lib/python3.7/site-packages/spacy/training/iob_utils.py:144: UserWarning: [W030] Some entities could not be aligned in the text \"The situation for the 45° case is more interesting...\" with entities \"[(26, 30, 'ME'), (146, 147, 'ME')]\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  entities=ent_str[:50] + \"...\" if len(ent_str) > 50 else ent_str,\n",
      "/home/sam/measeval/lib/python3.7/site-packages/spacy/training/iob_utils.py:144: UserWarning: [W030] Some entities could not be aligned in the text \"In agreement with K10, we showed that the H Lyman ...\" with entities \"[(624, 637, 'ME'), (871, 893, 'ME')]\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  entities=ent_str[:50] + \"...\" if len(ent_str) > 50 else ent_str,\n",
      "/home/sam/measeval/lib/python3.7/site-packages/spacy/training/iob_utils.py:144: UserWarning: [W030] Some entities could not be aligned in the text \"Tinnitus is the perception of sounds in the head o...\" with entities \"[(492, 510, 'ME'), (538, 551, 'ME'), (555, 561, 'M...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  entities=ent_str[:50] + \"...\" if len(ent_str) > 50 else ent_str,\n",
      "/home/sam/measeval/lib/python3.7/site-packages/spacy/training/iob_utils.py:144: UserWarning: [W030] Some entities could not be aligned in the text \"Vertical hydraulic gradients governing the rates o...\" with entities \"[(268, 273, 'ME')]\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  entities=ent_str[:50] + \"...\" if len(ent_str) > 50 else ent_str,\n",
      "/home/sam/measeval/lib/python3.7/site-packages/spacy/training/iob_utils.py:144: UserWarning: [W030] Some entities could not be aligned in the text \"•SOC stocks decreased by 12.4% in Costa Rica and 0...\" with entities \"[(1, 11, 'ME'), (144, 148, 'ME')]\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  entities=ent_str[:50] + \"...\" if len(ent_str) > 50 else ent_str,\n",
      "/home/sam/measeval/lib/python3.7/site-packages/spacy/training/iob_utils.py:144: UserWarning: [W030] Some entities could not be aligned in the text \"θ=tan−1(w/U) against mean wind speed (gate midpoin...\" with entities \"[(38, 51, 'ME'), (67, 71, 'ME')]\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  entities=ent_str[:50] + \"...\" if len(ent_str) > 50 else ent_str,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Losses {'ner': 3037.626688493432}\n",
      "Losses {'ner': 1291.547163418788}\n",
      "Losses {'ner': 1174.6931893097906}\n",
      "Losses {'ner': 1083.2613052745953}\n",
      "Losses {'ner': 1038.1497538565945}\n",
      "Losses {'ner': 997.8600898500052}\n",
      "Losses {'ner': 967.3046063582567}\n",
      "Losses {'ner': 838.282702610474}\n",
      "Losses {'ner': 816.4900078333457}\n",
      "Losses {'ner': 777.9750131459452}\n",
      "Losses {'ner': 756.7456264762973}\n",
      "Losses {'ner': 693.8314789807062}\n",
      "Losses {'ner': 645.5114351167521}\n",
      "Losses {'ner': 605.7121372113093}\n",
      "Losses {'ner': 613.138713437131}\n",
      "Losses {'ner': 548.203874762193}\n",
      "Losses {'ner': 498.05326385174135}\n",
      "Losses {'ner': 505.7753012987052}\n",
      "Losses {'ner': 451.11173903830945}\n",
      "Losses {'ner': 481.0974740427447}\n",
      "Starting training for MP\n",
      "['ner']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sam/measeval/lib/python3.7/site-packages/spacy/training/iob_utils.py:144: UserWarning: [W030] Some entities could not be aligned in the text \"Based on culture experiments (Sutton et al., 2013)...\" with entities \"[(309, 312, 'MP'), (608, 636, 'MP'), (1131, 1133, ...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  entities=ent_str[:50] + \"...\" if len(ent_str) > 50 else ent_str,\n",
      "/home/sam/measeval/lib/python3.7/site-packages/spacy/training/iob_utils.py:144: UserWarning: [W030] Some entities could not be aligned in the text \"Cleaned sponge and diatom opal was dissolved via w...\" with entities \"[(129, 133, 'MP'), (35, 71, 'MP'), (217, 226, 'MP'...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  entities=ent_str[:50] + \"...\" if len(ent_str) > 50 else ent_str,\n",
      "/home/sam/measeval/lib/python3.7/site-packages/spacy/training/iob_utils.py:144: UserWarning: [W030] Some entities could not be aligned in the text \"The Vocontian Basin was part of the western gulf i...\" with entities \"[(798, 803, 'MP'), (1384, 1385, 'MP')]\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  entities=ent_str[:50] + \"...\" if len(ent_str) > 50 else ent_str,\n",
      "/home/sam/measeval/lib/python3.7/site-packages/spacy/training/iob_utils.py:144: UserWarning: [W030] Some entities could not be aligned in the text \"Polythiophene (PTh) was purchased from Sigma-Aldri...\" with entities \"[(200, 204, 'MP'), (528, 534, 'MP'), (553, 564, 'M...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  entities=ent_str[:50] + \"...\" if len(ent_str) > 50 else ent_str,\n",
      "/home/sam/measeval/lib/python3.7/site-packages/spacy/training/iob_utils.py:144: UserWarning: [W030] Some entities could not be aligned in the text \"(D) PGRN secreted into the medium over 24 hr of mi...\" with entities \"[(116, 132, 'MP')]\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  entities=ent_str[:50] + \"...\" if len(ent_str) > 50 else ent_str,\n",
      "/home/sam/measeval/lib/python3.7/site-packages/spacy/training/iob_utils.py:144: UserWarning: [W030] Some entities could not be aligned in the text \"The performance and morphology of vacuum co-deposi...\" with entities \"[(441, 444, 'MP'), (451, 460, 'MP'), (570, 573, 'M...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  entities=ent_str[:50] + \"...\" if len(ent_str) > 50 else ent_str,\n",
      "/home/sam/measeval/lib/python3.7/site-packages/spacy/training/iob_utils.py:144: UserWarning: [W030] Some entities could not be aligned in the text \"The detections of atomic hydrogen, heavy atoms and...\" with entities \"[(897, 907, 'MP'), (1368, 1388, 'MP'), (1706, 1742...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  entities=ent_str[:50] + \"...\" if len(ent_str) > 50 else ent_str,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Losses {'ner': 2131.8105848216255}\n",
      "Losses {'ner': 838.9195560468838}\n",
      "Losses {'ner': 807.3163520366593}\n",
      "Losses {'ner': 734.8008153773321}\n",
      "Losses {'ner': 675.7293453314019}\n",
      "Losses {'ner': 663.8636317439183}\n",
      "Losses {'ner': 632.779466454026}\n",
      "Losses {'ner': 560.3579238891494}\n",
      "Losses {'ner': 525.6006643623749}\n",
      "Losses {'ner': 477.1927512656069}\n",
      "Losses {'ner': 448.45341926675763}\n",
      "Losses {'ner': 453.34026057820046}\n",
      "Losses {'ner': 406.2225215355868}\n",
      "Losses {'ner': 379.1704250391542}\n",
      "Losses {'ner': 376.8746445396598}\n",
      "Losses {'ner': 359.7230199168193}\n",
      "Losses {'ner': 349.12865306930235}\n",
      "Losses {'ner': 314.5790188688067}\n",
      "Losses {'ner': 268.6019577980444}\n",
      "Losses {'ner': 275.32160081985046}\n",
      "Starting training for QUAL\n",
      "['ner']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sam/measeval/lib/python3.7/site-packages/spacy/training/iob_utils.py:144: UserWarning: [W030] Some entities could not be aligned in the text \"We used standard operating protocols to measure ri...\" with entities \"[(690, 722, 'QUAL'), (1224, 1269, 'QUAL')]\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  entities=ent_str[:50] + \"...\" if len(ent_str) > 50 else ent_str,\n",
      "/home/sam/measeval/lib/python3.7/site-packages/spacy/training/iob_utils.py:144: UserWarning: [W030] Some entities could not be aligned in the text \"A negative carbon isotope excursion of 5‰ has been...\" with entities \"[(137, 168, 'QUAL'), (487, 501, 'QUAL')]\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  entities=ent_str[:50] + \"...\" if len(ent_str) > 50 else ent_str,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Losses {'ner': 1441.369805399856}\n",
      "Losses {'ner': 366.77103113417195}\n",
      "Losses {'ner': 523.6944648895253}\n",
      "Losses {'ner': 486.85651112510504}\n",
      "Losses {'ner': 493.1048182342237}\n",
      "Losses {'ner': 448.95828431538143}\n",
      "Losses {'ner': 349.8587100118874}\n",
      "Losses {'ner': 466.7165930486663}\n",
      "Losses {'ner': 313.8626232803198}\n",
      "Losses {'ner': 401.5693766288063}\n",
      "Losses {'ner': 341.60816283615253}\n",
      "Losses {'ner': 321.93961220125266}\n",
      "Losses {'ner': 314.0979040717744}\n",
      "Losses {'ner': 332.41748693584645}\n",
      "Losses {'ner': 298.05146923752403}\n",
      "Losses {'ner': 352.7963637511399}\n",
      "Losses {'ner': 233.12233183847698}\n",
      "Losses {'ner': 230.23549416291408}\n",
      "Losses {'ner': 237.8053403315015}\n",
      "Losses {'ner': 238.123433067292}\n"
     ]
    }
   ],
   "source": [
    "# Simplest possible model training. I'm sure there's tons I could do to optimize here.\n",
    "# Note that we lose a few more training instances here due to tokenizer mismatch issues.\n",
    "# Only effects Qualifiers and MeasuredProperties...\n",
    "models = {}\n",
    "for entType in [\"QUANT\", \"ME\", \"MP\", \"QUAL\"]:\n",
    "    print(\"Starting training for \" + entType)\n",
    "    models[entType] = spacy.blank(\"en\")\n",
    "    ner = models[entType].create_pipe(\"ner\")\n",
    "    models[entType].add_pipe(\"ner\")\n",
    "    print(models[entType].pipe_names)\n",
    "    ner.add_label(entType)\n",
    "    optimizer = models[entType].begin_training()\n",
    "\n",
    "    sizes = compounding(1.0, 4.0, 1.001)\n",
    "    # batch up the examples using spaCy's minibatch\n",
    "    for itn in range(20):\n",
    "        random.shuffle(traindata[entType])\n",
    "        batches = minibatch(traindata[entType], size=sizes)\n",
    "        losses = {}\n",
    "        for batch in batches:\n",
    "            for texts, annotations in batch:\n",
    "                doc = nlp.make_doc(texts)\n",
    "                example = Example.from_dict(doc, annotations)\n",
    "                models[entType].update([example], sgd=optimizer, drop=0.35, losses=losses)\n",
    "            # texts, annotations = zip(*batch)\n",
    "            # docs = nlp.make_doc(texts)\n",
    "            # examples = Example.from_dict(docs, annotations)\n",
    "            # models[entType].update([examples], sgd=optimizer, drop=0.35, losses=losses)\n",
    "        print(\"Losses\", losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And build our entity predictions for each of the four models...\n",
    "ents = {}\n",
    "counts = { \"total\": 0, \"QUANT\": 0, \"ME\": 0, \"MP\": 0, \"QUAL\": 0}\n",
    "for docId in testIds:\n",
    "    text = textset[docId]\n",
    "    #for docid,text in evaltextset.items():\n",
    "    counts[\"total\"] += 1\n",
    "    ents[docId] = {}\n",
    "\n",
    "    for entType in [\"QUANT\", \"ME\", \"MP\", \"QUAL\"]:\n",
    "        ents[docId][entType] = ()\n",
    "        doc = models[entType](text)\n",
    "        ents[docId][entType] = doc.ents\n",
    "        if len(list(ents[docId][entType])) > 0:\n",
    "            counts[entType]+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect a set of unique units for use in populating the unit data...\n",
    "import json\n",
    "units = []\n",
    "\n",
    "for fileset in trainpaths:\n",
    "    for fn in os.listdir(fileset):\n",
    "        # Let's make sure to limit the units to just the smaller train set\n",
    "        if fn[:-4] in trainIds:\n",
    "            with open(fileset+fn) as annotfile:\n",
    "                text = textset[fn[:-4]]\n",
    "                next(annotfile)\n",
    "                annots = annotfile.read().splitlines()\n",
    "                for a in annots:\n",
    "                    annot = a.split(\"\\t\")\n",
    "                    atype = typemap[annot[2]]\n",
    "                    if atype == \"QUANT\" and annot[7] != \"\":\n",
    "                        jsondata = json.loads(annot[7])\n",
    "                        if \"unit\" in jsondata:\n",
    "                            units.append(jsondata[\"unit\"])\n",
    "uniqunits = list(set(units))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "634\n",
      "128\n"
     ]
    }
   ],
   "source": [
    "print(len(units))\n",
    "print(len(uniqunits))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p ~/semeval/scratch/subs/baseline-simpler-split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simplest version, let's just check the lengths of everything\n",
    "# Then pop them off in the order they exist.\n",
    "header = \"docId\\tannotSet\\tannotType\\tstartOffset\\tendOffset\\tannotId\\ttext\\tother\"\n",
    "#subdir = \"/Users/harperco/projects/semeval/scratch/subs/baseline-simpler-split/\"\n",
    "subdir = os.path.join(currentdir,\"../../semeval/scratch/subs/baseline-simpler-split/\")\n",
    "count = 0\n",
    "for docId, allents in ents.items():\n",
    "    #if docId == \"S0378112713005288-1800\":\n",
    "    #print(allents)\n",
    "    annotSet = 1\n",
    "    #print(str(len(allents['QUANT']))+\"|\"+str(len(allents['ME']))+\"|\"\n",
    "    #      +str(len(allents['MP']))+\"|\"+str(len(allents['QUAL'])))\n",
    "    sub = open(subdir+docId + \".tsv\", \"w\")\n",
    "    sub.write(header+\"\\n\")\n",
    "    for quant in allents['QUANT']:\n",
    "        unitmatches = []\n",
    "        for unit in uniqunits: \n",
    "            if unit in quant.text:\n",
    "                unitmatches.append(unit)\n",
    "        if len(unitmatches) > 0: \n",
    "            unit = max(unitmatches, key=len)\n",
    "        strings = []\n",
    "        meId = 0\n",
    "        annotId = 1\n",
    "        quantString = (docId + \"\\t\" + str(annotSet) + \"\\tQuantity\\t\" + str(quant.start_char) + \"\\t\" +\n",
    "                        str(quant.end_char) + \"\\t\" + str(annotId) + \"\\t\" + quant.text+\"\\t{\\\"unit\\\": \\\"\" + unit +  \"\\\"}\")\n",
    "        strings.append(quantString)\n",
    "        annotId+=1\n",
    "        if (len(allents['ME']) > annotSet-1 and len(allents['MP']) > annotSet-1):\n",
    "            mp = allents['MP'][annotSet-1]\n",
    "            me = allents['ME'][annotSet-1]\n",
    "            mpString = (docId + \"\\t\" + str(annotSet) + \"\\tMeasuredProperty\\t\" + str(mp.start_char) + \"\\t\" + \n",
    "                    str(mp.end_char) + \"\\t\" + str(annotId) + \"\\t\" + mp.text + \"\\t{\\\"HasQuantity\\\": \\\"\" + \n",
    "                    str(annotId-1) + \"\\\"}\" )\n",
    "            strings.append(mpString)\n",
    "            annotId+=1\n",
    "\n",
    "            #print(me.text)\n",
    "            meString = (docId + \"\\t\" + str(annotSet) + \"\\tMeasuredEntity\\t\" + str(me.start_char) + \"\\t\" + \n",
    "                        str(me.end_char) + \"\\t\" + str(annotId) + \"\\t\" + me.text + \"\\t{\\\"HasProperty\\\": \\\"\" + \n",
    "                        str(annotId-1) + \"\\\"}\" )\n",
    "            strings.append(meString)\n",
    "            meId = annotId\n",
    "            annotId+=1\n",
    "        elif (len(allents['ME']) > annotSet-1):\n",
    "            me = allents['ME'][annotSet-1]\n",
    "            meString = (docId + \"\\t\" + str(annotSet) + \"\\tMeasuredEntity\\t\" + str(me.start_char) + \"\\t\" + \n",
    "                        str(me.end_char) + \"\\t\" + str(annotId) + \"\\t\" + me.text + \"\\t{\\\"HasProperty\\\": \\\"\" + \n",
    "                        str(annotId-1) + \"\\\"}\" )\n",
    "            strings.append(meString)\n",
    "            meId = annotId\n",
    "            annotId+=1     \n",
    "        if (len(allents['QUAL']) > annotSet-1 and meId != 0):\n",
    "            qual = allents['QUAL'][annotSet-1]\n",
    "            qualString = (docId + \"\\t\" + str(annotSet) + \"\\tQualifier\\t\" + str(qual.start_char) + \"\\t\" + \n",
    "                        str(qual.end_char) + \"\\t\" + str(annotId) + \"\\t\" + qual.text + \"\\t{\\\"Qualifies\\\": \\\"\" + \n",
    "                        str(meId) + \"\\\"}\" )\n",
    "            strings.append(qualString)\n",
    "            meId = annotId\n",
    "            annotId+=1                           \n",
    "\n",
    "        #print(\"ENT: \" + me.text)\n",
    "        #print(\"PROP: \" + mp.text)\n",
    "        for s in strings:\n",
    "            #print(s)\n",
    "            sub.write(s+\"\\n\")\n",
    "        annotSet+=1\n",
    "    sub.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'tuple'>\n"
     ]
    }
   ],
   "source": [
    "for docId, allents in ents.items():\n",
    "    if docId == \"S0016236113008041-3012\":\n",
    "        print(type(allents['QUANT']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "90 kW"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ents['S0016236113008041-3012']['QUANT'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Shelling out to measeval-eval.py inline.\n",
    "\n",
    "Note, we have added another new flag to the evaluation script: -l or limit.\n",
    "\n",
    "This was the default up until the evaluation period opened. It limits the gold data files loaded to only files that are included in the submission. This is so that you can set an arbitrary train/test split (as we've done above) and not record the training portion in the gold data used for evaluation.\n",
    "\n",
    "Also note that the \"gold/scratch\" directory used for eval below is a combined copy of _all_ .tsv files in both the data/train/tsv and data/test/tsv directories in the MeasEval Github repo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validating Vlad(source=LocalFile('/home/sam/semeval/scratch/subs/baseline-simpler-split/S1570870512000637-1206.tsv'))\n",
      "\u001b[0;32mPassed! :)\u001b[0m\n",
      "\n",
      "Validating Vlad(source=LocalFile('/home/sam/semeval/scratch/subs/baseline-simpler-split/S0006322312001096-1202.tsv'))\n",
      "\u001b[0;32mPassed! :)\u001b[0m\n",
      "\n",
      "Validating Vlad(source=LocalFile('/home/sam/semeval/scratch/subs/baseline-simpler-split/S2211124713006475-1205.tsv'))\n",
      "\u001b[0;32mPassed! :)\u001b[0m\n",
      "\n",
      "Validating Vlad(source=LocalFile('/home/sam/semeval/scratch/subs/baseline-simpler-split/S0921818113002245-859.tsv'))\n",
      "\u001b[0;32mPassed! :)\u001b[0m\n",
      "\n",
      "Validating Vlad(source=LocalFile('/home/sam/semeval/scratch/subs/baseline-simpler-split/S0022459611006116-547.tsv'))\n",
      "\u001b[0;32mPassed! :)\u001b[0m\n",
      "\n",
      "Validating Vlad(source=LocalFile('/home/sam/semeval/scratch/subs/baseline-simpler-split/S0167880913001229-1033.tsv'))\n",
      "\u001b[0;32mPassed! :)\u001b[0m\n",
      "\n",
      "Validating Vlad(source=LocalFile('/home/sam/semeval/scratch/subs/baseline-simpler-split/S0012821X12004384-1232.tsv'))\n",
      "\u001b[0;32mPassed! :)\u001b[0m\n",
      "\n",
      "Validating Vlad(source=LocalFile('/home/sam/semeval/scratch/subs/baseline-simpler-split/S0021979713004438-1415.tsv'))\n",
      "\u001b[0;32mPassed! :)\u001b[0m\n",
      "\n",
      "Validating Vlad(source=LocalFile('/home/sam/semeval/scratch/subs/baseline-simpler-split/S0019103512002801-1927.tsv'))\n",
      "\u001b[0;32mPassed! :)\u001b[0m\n",
      "\n",
      "Validating Vlad(source=LocalFile('/home/sam/semeval/scratch/subs/baseline-simpler-split/S2213671113000908-810.tsv'))\n",
      "\u001b[0;32mPassed! :)\u001b[0m\n",
      "\n",
      "Validating Vlad(source=LocalFile('/home/sam/semeval/scratch/subs/baseline-simpler-split/S0012821X13002185-835.tsv'))\n",
      "\u001b[0;32mPassed! :)\u001b[0m\n",
      "\n",
      "Validating Vlad(source=LocalFile('/home/sam/semeval/scratch/subs/baseline-simpler-split/S0021979713004438-1401.tsv'))\n",
      "\u001b[0;32mPassed! :)\u001b[0m\n",
      "\n",
      "Validating Vlad(source=LocalFile('/home/sam/semeval/scratch/subs/baseline-simpler-split/S0012821X13002185-1200.tsv'))\n",
      "\u001b[0;32mPassed! :)\u001b[0m\n",
      "\n",
      "Validating Vlad(source=LocalFile('/home/sam/semeval/scratch/subs/baseline-simpler-split/S2213671113000738-647.tsv'))\n",
      "\u001b[0;32mPassed! :)\u001b[0m\n",
      "\n",
      "Validating Vlad(source=LocalFile('/home/sam/semeval/scratch/subs/baseline-simpler-split/S0925443913001385-1646.tsv'))\n",
      "\u001b[0;32mPassed! :)\u001b[0m\n",
      "\n",
      "Validating Vlad(source=LocalFile('/home/sam/semeval/scratch/subs/baseline-simpler-split/S0019103512002801-1608.tsv'))\n",
      "\u001b[0;32mPassed! :)\u001b[0m\n",
      "\n",
      "Validating Vlad(source=LocalFile('/home/sam/semeval/scratch/subs/baseline-simpler-split/S0925443913001385-1429.tsv'))\n",
      "\u001b[0;32mPassed! :)\u001b[0m\n",
      "\n",
      "Validating Vlad(source=LocalFile('/home/sam/semeval/scratch/subs/baseline-simpler-split/S0006322312001096-1260.tsv'))\n",
      "\u001b[0;32mPassed! :)\u001b[0m\n",
      "\n",
      "Validating Vlad(source=LocalFile('/home/sam/semeval/scratch/subs/baseline-simpler-split/S175058361300203X-1556.tsv'))\n",
      "\u001b[0;32mPassed! :)\u001b[0m\n",
      "\n",
      "Validating Vlad(source=LocalFile('/home/sam/semeval/scratch/subs/baseline-simpler-split/S1386142513006823-2084.tsv'))\n",
      "\u001b[0;32mPassed! :)\u001b[0m\n",
      "\n",
      "Validating Vlad(source=LocalFile('/home/sam/semeval/scratch/subs/baseline-simpler-split/S0032063313003218-6651.tsv'))\n",
      "\u001b[0;32mPassed! :)\u001b[0m\n",
      "\n",
      "Validating Vlad(source=LocalFile('/home/sam/semeval/scratch/subs/baseline-simpler-split/S0019103512004009-4007.tsv'))\n",
      "\u001b[0;32mPassed! :)\u001b[0m\n",
      "\n",
      "Validating Vlad(source=LocalFile('/home/sam/semeval/scratch/subs/baseline-simpler-split/S0016236113008041-3161.tsv'))\n",
      "\u001b[0;32mPassed! :)\u001b[0m\n",
      "\n",
      "Validating Vlad(source=LocalFile('/home/sam/semeval/scratch/subs/baseline-simpler-split/S0019103513005058-4158.tsv'))\n",
      "\u001b[0;32mPassed! :)\u001b[0m\n",
      "\n",
      "Validating Vlad(source=LocalFile('/home/sam/semeval/scratch/subs/baseline-simpler-split/S0019103512002801-1824.tsv'))\n",
      "\u001b[0;32mPassed! :)\u001b[0m\n",
      "\n",
      "Validating Vlad(source=LocalFile('/home/sam/semeval/scratch/subs/baseline-simpler-split/S0167819113001051-1247.tsv'))\n",
      "\u001b[0;32mPassed! :)\u001b[0m\n",
      "\n",
      "Validating Vlad(source=LocalFile('/home/sam/semeval/scratch/subs/baseline-simpler-split/S0167577X13006393-644.tsv'))\n",
      "\u001b[0;32mPassed! :)\u001b[0m\n",
      "\n",
      "Validating Vlad(source=LocalFile('/home/sam/semeval/scratch/subs/baseline-simpler-split/S0022000014000026-17824.tsv'))\n",
      "\u001b[0;32mPassed! :)\u001b[0m\n",
      "\n",
      "Validating Vlad(source=LocalFile('/home/sam/semeval/scratch/subs/baseline-simpler-split/S0165587612003680-953.tsv'))\n",
      "\u001b[0;32mPassed! :)\u001b[0m\n",
      "\n",
      "Validating Vlad(source=LocalFile('/home/sam/semeval/scratch/subs/baseline-simpler-split/S0019103512002801-1342.tsv'))\n",
      "\u001b[0;32mPassed! :)\u001b[0m\n",
      "\n",
      "Validating Vlad(source=LocalFile('/home/sam/semeval/scratch/subs/baseline-simpler-split/S0378112713005288-2062.tsv'))\n",
      "\u001b[0;32mPassed! :)\u001b[0m\n",
      "\n",
      "Validating Vlad(source=LocalFile('/home/sam/semeval/scratch/subs/baseline-simpler-split/S0006322312001096-1271.tsv'))\n",
      "\u001b[0;32mPassed! :)\u001b[0m\n",
      "\n",
      "Validating Vlad(source=LocalFile('/home/sam/semeval/scratch/subs/baseline-simpler-split/S0006322312001096-1136.tsv'))\n",
      "\u001b[0;32mPassed! :)\u001b[0m\n",
      "\n",
      "Validating Vlad(source=LocalFile('/home/sam/semeval/scratch/subs/baseline-simpler-split/S0012821X13002185-994.tsv'))\n",
      "\u001b[0;32mPassed! :)\u001b[0m\n",
      "\n",
      "Validating Vlad(source=LocalFile('/home/sam/semeval/scratch/subs/baseline-simpler-split/S1388248113001951-339.tsv'))\n",
      "\u001b[0;32mPassed! :)\u001b[0m\n",
      "\n",
      "Validating Vlad(source=LocalFile('/home/sam/semeval/scratch/subs/baseline-simpler-split/S2211124712002884-903.tsv'))\n",
      "\u001b[0;32mPassed! :)\u001b[0m\n",
      "\n",
      "Validating Vlad(source=LocalFile('/home/sam/semeval/scratch/subs/baseline-simpler-split/S0378383912000130-1048.tsv'))\n",
      "\u001b[0;32mPassed! :)\u001b[0m\n",
      "\n",
      "Validating Vlad(source=LocalFile('/home/sam/semeval/scratch/subs/baseline-simpler-split/S0378383912000130-3745.tsv'))\n",
      "\u001b[0;32mPassed! :)\u001b[0m\n",
      "\n",
      "Validating Vlad(source=LocalFile('/home/sam/semeval/scratch/subs/baseline-simpler-split/S0032063312003054-2483.tsv'))\n",
      "\u001b[0;32mPassed! :)\u001b[0m\n",
      "\n",
      "Validating Vlad(source=LocalFile('/home/sam/semeval/scratch/subs/baseline-simpler-split/S0016236113008041-3186.tsv'))\n",
      "\u001b[0;32mPassed! :)\u001b[0m\n",
      "\n",
      "Validating Vlad(source=LocalFile('/home/sam/semeval/scratch/subs/baseline-simpler-split/S0019103512004009-4492.tsv'))\n",
      "\u001b[0;32mPassed! :)\u001b[0m\n",
      "\n",
      "Validating Vlad(source=LocalFile('/home/sam/semeval/scratch/subs/baseline-simpler-split/S0012821X13007309-1482.tsv'))\n",
      "\u001b[0;32mPassed! :)\u001b[0m\n",
      "\n",
      "Validating Vlad(source=LocalFile('/home/sam/semeval/scratch/subs/baseline-simpler-split/S2213671113000738-684.tsv'))\n",
      "\u001b[0;32mPassed! :)\u001b[0m\n",
      "\n",
      "Validating Vlad(source=LocalFile('/home/sam/semeval/scratch/subs/baseline-simpler-split/S1359645413009816-2973.tsv'))\n",
      "\u001b[0;32mPassed! :)\u001b[0m\n",
      "\n",
      "Validating Vlad(source=LocalFile('/home/sam/semeval/scratch/subs/baseline-simpler-split/S0019103513005058-4210.tsv'))\n",
      "\u001b[0;32mPassed! :)\u001b[0m\n",
      "\n",
      "Validating Vlad(source=LocalFile('/home/sam/semeval/scratch/subs/baseline-simpler-split/S0019103513005058-3154.tsv'))\n",
      "\u001b[0;32mPassed! :)\u001b[0m\n",
      "\n",
      "Validating Vlad(source=LocalFile('/home/sam/semeval/scratch/subs/baseline-simpler-split/S016412121300188X-5066.tsv'))\n",
      "\u001b[0;32mPassed! :)\u001b[0m\n",
      "\n",
      "Validating Vlad(source=LocalFile('/home/sam/semeval/scratch/subs/baseline-simpler-split/S0016236113008041-890.tsv'))\n",
      "\u001b[0;32mPassed! :)\u001b[0m\n",
      "\n",
      "Validating Vlad(source=LocalFile('/home/sam/semeval/scratch/subs/baseline-simpler-split/S0168945213001805-3964.tsv'))\n",
      "\u001b[0;32mPassed! :)\u001b[0m\n",
      "\n",
      "Validating Vlad(source=LocalFile('/home/sam/semeval/scratch/subs/baseline-simpler-split/S016412121300188X-4545.tsv'))\n",
      "\u001b[0;32mPassed! :)\u001b[0m\n",
      "\n",
      "Validating Vlad(source=LocalFile('/home/sam/semeval/scratch/subs/baseline-simpler-split/S016412121300188X-4069.tsv'))\n",
      "\u001b[0;32mPassed! :)\u001b[0m\n",
      "\n",
      "Validating Vlad(source=LocalFile('/home/sam/semeval/scratch/subs/baseline-simpler-split/S2213671113000738-738.tsv'))\n",
      "\u001b[0;32mPassed! :)\u001b[0m\n",
      "\n",
      "Validating Vlad(source=LocalFile('/home/sam/semeval/scratch/subs/baseline-simpler-split/S0167610513002729-1062.tsv'))\n",
      "\u001b[0;32mPassed! :)\u001b[0m\n",
      "\n",
      "Validating Vlad(source=LocalFile('/home/sam/semeval/scratch/subs/baseline-simpler-split/S1084804513001987-7409.tsv'))\n",
      "\u001b[0;32mPassed! :)\u001b[0m\n",
      "\n",
      "Validating Vlad(source=LocalFile('/home/sam/semeval/scratch/subs/baseline-simpler-split/S0019103512003533-3306.tsv'))\n",
      "\u001b[0;32mPassed! :)\u001b[0m\n",
      "\n",
      "Validating Vlad(source=LocalFile('/home/sam/semeval/scratch/subs/baseline-simpler-split/S0012821X13002185-1061.tsv'))\n",
      "\u001b[0;32mPassed! :)\u001b[0m\n",
      "\n",
      "Validating Vlad(source=LocalFile('/home/sam/semeval/scratch/subs/baseline-simpler-split/S0019103512003995-2681.tsv'))\n",
      "\u001b[0;32mPassed! :)\u001b[0m\n",
      "\n",
      "Validating Vlad(source=LocalFile('/home/sam/semeval/scratch/subs/baseline-simpler-split/S0012821X12004384-1594.tsv'))\n",
      "\u001b[0;32mPassed! :)\u001b[0m\n",
      "\n",
      "Validating Vlad(source=LocalFile('/home/sam/semeval/scratch/subs/baseline-simpler-split/S0925443913001385-1638.tsv'))\n",
      "\u001b[0;32mPassed! :)\u001b[0m\n",
      "\n",
      "Validating Vlad(source=LocalFile('/home/sam/semeval/scratch/subs/baseline-simpler-split/S1873506113001116-1369.tsv'))\n",
      "\u001b[0;32mPassed! :)\u001b[0m\n",
      "\n",
      "Validating Vlad(source=LocalFile('/home/sam/semeval/scratch/subs/baseline-simpler-split/S0019103512004009-5019.tsv'))\n",
      "\u001b[0;32mPassed! :)\u001b[0m\n",
      "\n",
      "Validating Vlad(source=LocalFile('/home/sam/semeval/scratch/subs/baseline-simpler-split/S0006322312001096-1248.tsv'))\n",
      "\u001b[0;32mPassed! :)\u001b[0m\n",
      "\n",
      "Validating Vlad(source=LocalFile('/home/sam/semeval/scratch/subs/baseline-simpler-split/S0167577X14001256-517.tsv'))\n",
      "\u001b[0;32mPassed! :)\u001b[0m\n",
      "\n",
      "Validating Vlad(source=LocalFile('/home/sam/semeval/scratch/subs/baseline-simpler-split/S0012821X12004384-1415.tsv'))\n",
      "\u001b[0;32mPassed! :)\u001b[0m\n",
      "\n",
      "Validating Vlad(source=LocalFile('/home/sam/semeval/scratch/subs/baseline-simpler-split/S1550413113004920-1550.tsv'))\n",
      "\u001b[0;32mPassed! :)\u001b[0m\n",
      "\n",
      "Validating Vlad(source=LocalFile('/home/sam/semeval/scratch/subs/baseline-simpler-split/S2211124713006475-1195.tsv'))\n",
      "\u001b[0;32mPassed! :)\u001b[0m\n",
      "\n",
      "Validating Vlad(source=LocalFile('/home/sam/semeval/scratch/subs/baseline-simpler-split/S0019103512003533-5598.tsv'))\n",
      "\u001b[0;32mPassed! :)\u001b[0m\n",
      "\n",
      "Validating Vlad(source=LocalFile('/home/sam/semeval/scratch/subs/baseline-simpler-split/S0016236113008041-3012.tsv'))\n",
      "\u001b[0;32mPassed! :)\u001b[0m\n",
      "\n",
      "Validating Vlad(source=LocalFile('/home/sam/semeval/scratch/subs/baseline-simpler-split/S0167577X13006393-787.tsv'))\n",
      "\u001b[0;32mPassed! :)\u001b[0m\n",
      "\n",
      "Validating Vlad(source=LocalFile('/home/sam/semeval/scratch/subs/baseline-simpler-split/S0167819113001051-1550.tsv'))\n",
      "\u001b[0;32mPassed! :)\u001b[0m\n",
      "\n",
      "Validating Vlad(source=LocalFile('/home/sam/semeval/scratch/subs/baseline-simpler-split/S0022000014000026-7850.tsv'))\n",
      "\u001b[0;32mPassed! :)\u001b[0m\n",
      "\n",
      "Validating Vlad(source=LocalFile('/home/sam/semeval/scratch/subs/baseline-simpler-split/S0019103512004009-3488.tsv'))\n",
      "\u001b[0;32mPassed! :)\u001b[0m\n",
      "\n",
      "Validating Vlad(source=LocalFile('/home/sam/semeval/scratch/subs/baseline-simpler-split/S2213671113001306-1286.tsv'))\n",
      "\u001b[0;32mPassed! :)\u001b[0m\n",
      "\n",
      "Validating Vlad(source=LocalFile('/home/sam/semeval/scratch/subs/baseline-simpler-split/S1367912013002277-1213.tsv'))\n",
      "\u001b[0;32mPassed! :)\u001b[0m\n",
      "\n",
      "Validating Vlad(source=LocalFile('/home/sam/semeval/scratch/subs/baseline-simpler-split/S0019103512001388-1070.tsv'))\n",
      "\u001b[0;32mPassed! :)\u001b[0m\n",
      "\n",
      "Validating Vlad(source=LocalFile('/home/sam/semeval/scratch/subs/baseline-simpler-split/S0012821X13007309-1989.tsv'))\n",
      "\u001b[0;32mPassed! :)\u001b[0m\n",
      "\n",
      "Validating Vlad(source=LocalFile('/home/sam/semeval/scratch/subs/baseline-simpler-split/S0019103512003995-1910.tsv'))\n",
      "\u001b[0;32mPassed! :)\u001b[0m\n",
      "\n",
      "Validating Vlad(source=LocalFile('/home/sam/semeval/scratch/subs/baseline-simpler-split/S0038071713001971-1427.tsv'))\n",
      "\u001b[0;32mPassed! :)\u001b[0m\n",
      "\n",
      "Validating Vlad(source=LocalFile('/home/sam/semeval/scratch/subs/baseline-simpler-split/S0012821X12004384-1265.tsv'))\n",
      "\u001b[0;32mPassed! :)\u001b[0m\n",
      "\n",
      "Validating Vlad(source=LocalFile('/home/sam/semeval/scratch/subs/baseline-simpler-split/S0032386113005454-2055.tsv'))\n",
      "\u001b[0;32mPassed! :)\u001b[0m\n",
      "\n",
      "Validating Vlad(source=LocalFile('/home/sam/semeval/scratch/subs/baseline-simpler-split/S0927775713009606-1074.tsv'))\n",
      "\u001b[0;32mPassed! :)\u001b[0m\n",
      "\n",
      "Validating Vlad(source=LocalFile('/home/sam/semeval/scratch/subs/baseline-simpler-split/S2213671113000908-979.tsv'))\n",
      "\u001b[0;32mPassed! :)\u001b[0m\n",
      "\n",
      "Validating Vlad(source=LocalFile('/home/sam/semeval/scratch/subs/baseline-simpler-split/S0019103512002801-1716.tsv'))\n",
      "\u001b[0;32mPassed! :)\u001b[0m\n",
      "\n",
      "Validating Vlad(source=LocalFile('/home/sam/semeval/scratch/subs/baseline-simpler-split/S016412121300188X-5038.tsv'))\n",
      "\u001b[0;32mPassed! :)\u001b[0m\n",
      "\n",
      "Validating Vlad(source=LocalFile('/home/sam/semeval/scratch/subs/baseline-simpler-split/S0019103513005058-4349.tsv'))\n",
      "\u001b[0;32mPassed! :)\u001b[0m\n",
      "\n",
      "Validating Vlad(source=LocalFile('/home/sam/semeval/scratch/subs/baseline-simpler-split/S0019103512003995-1767.tsv'))\n",
      "\u001b[0;32mPassed! :)\u001b[0m\n",
      "\n",
      "Validating Vlad(source=LocalFile('/home/sam/semeval/scratch/subs/baseline-simpler-split/S0012821X12004384-1302.tsv'))\n",
      "\u001b[0;32mPassed! :)\u001b[0m\n",
      "\n",
      "Validating Vlad(source=LocalFile('/home/sam/semeval/scratch/subs/baseline-simpler-split/S016412121300188X-4640.tsv'))\n",
      "\u001b[0;32mPassed! :)\u001b[0m\n",
      "\n",
      "Validating Vlad(source=LocalFile('/home/sam/semeval/scratch/subs/baseline-simpler-split/S0019103512004009-5271.tsv'))\n",
      "\u001b[0;32mPassed! :)\u001b[0m\n",
      "\n",
      "Validating Vlad(source=LocalFile('/home/sam/semeval/scratch/subs/baseline-simpler-split/S0960148113002048-3775.tsv'))\n",
      "\u001b[0;32mPassed! :)\u001b[0m\n",
      "\n",
      "Validating Vlad(source=LocalFile('/home/sam/semeval/scratch/subs/baseline-simpler-split/S0019103513005058-3094.tsv'))\n",
      "\u001b[0;32mPassed! :)\u001b[0m\n",
      "\n",
      "Validating Vlad(source=LocalFile('/home/sam/semeval/scratch/subs/baseline-simpler-split/S0167610512002292-3187.tsv'))\n",
      "\u001b[0;32mPassed! :)\u001b[0m\n",
      "\n",
      "Validating Vlad(source=LocalFile('/home/sam/semeval/scratch/subs/baseline-simpler-split/S0927775713009606-1216.tsv'))\n",
      "\u001b[0;32mPassed! :)\u001b[0m\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/sam/MeasEval/eval/measeval-eval.py\", line 214, in <module>\n",
      "    for gfn in os.listdir(args.indir+args.gold):\n",
      "FileNotFoundError: [Errno 2] No such file or directory: '/home/sam/semeval/scratch/gold/'\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "!python ~/MeasEval/eval/measeval-eval.py -i $HOME/semeval/ -g \"scratch/gold/\" -s \"scratch/subs/baseline-simpler-split/\" -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/Users/harperco/projects/semeval/scratch/subs/baseline-split/S0019103513005058-3094.tsv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_77195/3989277475.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0msortedstrings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m     \u001b[0msub\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubdir\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mdocId\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\".tsv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"w\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mstring\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mallstrings\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/Users/harperco/projects/semeval/scratch/subs/baseline-split/S0019103513005058-3094.tsv'"
     ]
    }
   ],
   "source": [
    "# This last, fairly unweildy chunk of code is:\n",
    "# * collecting everything, \n",
    "# * Building the TSV strings\n",
    "# * Attempting to identify a unit\n",
    "# * matching and populating annotSet based on knockout logic, \n",
    "# * resorting, and populating TSV files.\n",
    "\n",
    "# Configure header string and submission directory (latter needs to exist.)\n",
    "header = \"docId\\tannotSet\\tannotType\\tstartOffset\\tendOffset\\tannotId\\ttext\\tother\"\n",
    "subdir = \"/Users/harperco/projects/semeval/scratch/subs/baseline-split/\"\n",
    "\n",
    "for docId, allents in ents.items():\n",
    "    # First we collect our Quantities\n",
    "    # We want to get the strin version, the full set, and the \"knockout\" list.\n",
    "    quantstrings = []\n",
    "    quants = []\n",
    "    knockout = []\n",
    "    annotSet = 1\n",
    "    for quant in allents['QUANT']:\n",
    "        # Match units in the Quant, then find the longest unit \n",
    "        unitmatches = []\n",
    "        for unit in uniqunits: \n",
    "            if unit in quant.text:\n",
    "                unitmatches.append(unit)\n",
    "        if len(unitmatches) > 0: \n",
    "            unit = max(unitmatches, key=len)\n",
    "        # Build the quantity string, and also the dictionary for quant and knockout.\n",
    "        quantstrings.append(docId + \"\\t\" + str(annotSet) + \"\\tQuantity\\t\" + str(quant.start_char) + \"\\t\" +\n",
    "                          str(quant.end_char) + \"\\t1\\t\" + quant.text+\"\\t{\\\"unit\\\": \\\"\" + unit +  \"\\\"}\")\n",
    "        quants.append({\"annotSet\": annotSet, \"annotId\": 1, \"start\": quant.start_char, \"end\": quant.end_char, \n",
    "                       \"text\": quant.text, \"type\": \"Quantity\"}) \n",
    "        knockout.append({\"annotSet\": annotSet, \"annotId\": 1, \"start\": quant.start_char, \"end\": quant.end_char, \n",
    "                       \"text\": quant.text, \"type\": \"Quantity\"}) \n",
    "        annotSet+=1\n",
    "    \n",
    "    # So now we want to do the ents, as we need this queued up to do more matching with the MPs\n",
    "    mestrings = []\n",
    "    mestring = \"\"\n",
    "    mes = []\n",
    "    knockoutmes = []\n",
    "    #annotSet = 1\n",
    "    for me in allents['ME']:\n",
    "        knockoutmes.append({\"start\": me.start_char, \"end\": me.end_char, \"text\": me.text, \"type\": \"MeasuredEntity\"}) \n",
    "\n",
    "    # Now we work through our measured properties.\n",
    "    mpstrings = []\n",
    "    mpstring = \"\"\n",
    "    mps = []\n",
    "    knockoutmps = []\n",
    "    for mp in allents['MP']:\n",
    "        if len(knockout) > 0 and len(knockoutmes) > 0:\n",
    "            start = mp.start_char\n",
    "            end = mp.end_char\n",
    "            nearest = {\"dist\": 100000000, \"set\": 0, \"id\": 0, \"index\": 100000000}\n",
    "            index = 0\n",
    "            for q in knockout:\n",
    "                dists = [abs(start-q[\"start\"]), abs(end-q[\"start\"]), abs(start-q[\"end\"]), abs(end-q[\"end\"])]\n",
    "                mindist = min(dists)\n",
    "                if mindist < nearest[\"dist\"]:\n",
    "                    nearest[\"dist\"] = mindist\n",
    "                    nearest[\"set\"] = q[\"annotSet\"]\n",
    "                    nearest[\"id\"] = q[\"annotId\"]\n",
    "                    nearest[\"index\"] = index\n",
    "                index+=1\n",
    "            knockout.pop(nearest[\"index\"])\n",
    "\n",
    "            mpString = (docId + \"\\t\" + str(nearest[\"set\"]) + \"\\tMeasuredProperty\\t\" + str(mp.start_char) + \"\\t\" + \n",
    "                        str(mp.end_char) + \"\\t\" + str(nearest[\"id\"]+1) + \"\\t\" + mp.text + \"\\t{\\\"HasQuantity\\\": \\\"\" + \n",
    "                        str(nearest[\"id\"]) + \"\\\"}\" )\n",
    "            mpstrings.append(mpString)\n",
    "            mps.append({\"annotSet\": nearest[\"set\"], \"annotId\": nearest[\"id\"]+1, \"start\": mp.start_char, \n",
    "                        \"end\": mp.end_char, \"text\": mp.text, \"type\": \"MeasuredProperty\"})\n",
    "            knockoutmps.append({\"annotSet\": nearest[\"set\"], \"annotId\": nearest[\"id\"]+1, \"start\": mp.start_char, \n",
    "                        \"end\": mp.end_char, \"text\": mp.text, \"type\": \"MeasuredProperty\"})\n",
    "\n",
    "            nearestme = {\"dist\": 100000000, \"index\": 100000000}\n",
    "            index = 0\n",
    "            if len(knockoutmes) > 0:\n",
    "                for me in knockoutmes:\n",
    "                    dists = [abs(start-me[\"start\"]), abs(end-me[\"start\"]), abs(start-me[\"end\"]), abs(end-me[\"end\"])]\n",
    "                    mindist = min(dists)\n",
    "                    if mindist < nearestme[\"dist\"]:\n",
    "                        nearestme[\"dist\"] = mindist\n",
    "                        nearestme[\"index\"] = index\n",
    "                    index+=1\n",
    "                meString = (docId + \"\\t\" + str(nearest[\"set\"]) + \"\\tMeasuredEntity\\t\" + str(me[\"start\"]) + \"\\t\" + \n",
    "                            str(me[\"end\"]) + \"\\t\" + str(nearest[\"id\"]+2) + \"\\t\" + me[\"text\"] + \"\\t{\\\"HasProperty\\\": \\\"\" + \n",
    "                            str(nearest[\"id\"]+1) + \"\\\"}\" )   \n",
    "                mestrings.append(meString)\n",
    "\n",
    "                knockoutmes.pop(nearestme[\"index\"])\n",
    "\n",
    "\n",
    "    # Now we do any leftover MEs, which should go straight to a Quantity:\n",
    "\n",
    "    for me in knockoutmes:\n",
    "        start = me[\"start\"]\n",
    "        end = me[\"end\"]\n",
    "        nearest = {\"dist\": 100000000, \"set\": 0, \"id\": 0, \"index\": 100000000, \"type\": \"\"}\n",
    "        index = 0                \n",
    "        for q in knockout:\n",
    "            dists = [abs(start-q[\"start\"]), abs(end-q[\"start\"]), abs(start-q[\"end\"]), abs(end-q[\"end\"])]\n",
    "            mindist = min(dists)\n",
    "            if mindist < nearest[\"dist\"]:\n",
    "                nearest[\"dist\"] = mindist\n",
    "                nearest[\"set\"] = q[\"annotSet\"]\n",
    "                nearest[\"id\"] = q[\"annotId\"]\n",
    "                nearest[\"index\"] = index\n",
    "                nearest[\"type\"] = q[\"type\"]\n",
    "            index+=1\n",
    "        if len(knockout) > 0:\n",
    "            knockout.pop(nearest[\"index\"])\n",
    "            meString = (docId + \"\\t\" + str(nearest[\"set\"]) + \"\\tMeasuredEntity\\t\" + str(me[\"start\"]) + \"\\t\" + \n",
    "                        str(me[\"end\"]) + \"\\t\" + str(nearest[\"id\"]+1) + \"\\t\" + me[\"text\"] + \"\\t{\\\"HasQuantity\\\": \\\"\" + \n",
    "                        str(nearest[\"id\"]) + \"\\\"}\" )   \n",
    "            mestrings.append(meString)\n",
    "            mes.append({\"annotSet\": nearest[\"set\"], \"annotId\": nearest[\"id\"]+1, \"start\": me[\"start\"], \n",
    "                        \"end\": me[\"end\"], \"text\": me[\"text\"], \"type\": \"MeasuredEntity\"})\n",
    "            \n",
    "    #Finally, let's process our Qualifiers:\n",
    "    kitchensink = [x for x in itertools.chain(quants, mps, mes)]\n",
    "    qualstrings = []\n",
    "    for qual in allents['QUAL']:\n",
    "        start = qual.start_char\n",
    "        end = qual.end_char\n",
    "        nearest = {\"dist\": 100000000, \"set\": 0, \"id\": 0, \"index\": 100000000}\n",
    "        index = 0\n",
    "        for q in kitchensink:\n",
    "            dists = [abs(start-q[\"start\"]), abs(end-q[\"start\"]), abs(start-q[\"end\"]), abs(end-q[\"end\"])]\n",
    "            mindist = min(dists)\n",
    "            if mindist < nearest[\"dist\"]:\n",
    "                nearest[\"dist\"] = mindist\n",
    "                nearest[\"set\"] = q[\"annotSet\"]\n",
    "                nearest[\"id\"] = q[\"annotId\"]\n",
    "                nearest[\"index\"] = index\n",
    "            index+=1\n",
    "        kitchensink.pop(nearest[\"index\"])\n",
    "\n",
    "        qualString = (docId + \"\\t\" + str(nearest[\"set\"]) + \"\\tQualifier\\t\" + str(qual.start_char) + \"\\t\" + \n",
    "                    str(qual.end_char) + \"\\t\" + str(nearest[\"id\"]+1) + \"\\t\" + qual.text + \"\\t{\\\"Qualifies\\\": \\\"\" + \n",
    "                    str(nearest[\"id\"]) + \"\\\"}\" )\n",
    "        qualstrings.append(qualString)\n",
    "\n",
    "    # Finally, we collect everythign:\n",
    "\n",
    "    import itertools\n",
    "    allstrings = [x for x in itertools.chain(quantstrings, mpstrings, mestrings, qualstrings)]\n",
    "    sortedstrings = {}\n",
    "\n",
    "    sub = open(subdir+docId + \".tsv\", \"w\")\n",
    "\n",
    "    for string in allstrings:\n",
    "        annotSet = string.split(\"\\t\")[1]\n",
    "        annotId = string.split(\"\\t\")[5]\n",
    "        if annotSet not in sortedstrings:\n",
    "            sortedstrings[annotSet] = {}\n",
    "        sortedstrings[annotSet][annotId] = string   \n",
    "    sub.write(header+\"\\n\")\n",
    "    for aset, val in sortedstrings.items():\n",
    "        for aid, string in val.items():\n",
    "            sub.write(string+\"\\n\")\n",
    "    sub.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Validating Vlad(source=LocalFile('/Users/harperco/projects/semeval/scratch/subs/baseline-split/S0019103511004994-1382.tsv'))\r\n",
      "\u001b[0;32mPassed! :)\u001b[0m\r\n",
      "\r\n",
      "Validating Vlad(source=LocalFile('/Users/harperco/projects/semeval/scratch/subs/baseline-split/S2213671113001306-1286.tsv'))\r\n",
      "\u001b[0;32mPassed! :)\u001b[0m\r\n",
      "\r\n",
      "Validating Vlad(source=LocalFile('/Users/harperco/projects/semeval/scratch/subs/baseline-split/S0960148113002048-3775.tsv'))\r\n",
      "\u001b[0;32mPassed! :)\u001b[0m\r\n",
      "\r\n",
      "Validating Vlad(source=LocalFile('/Users/harperco/projects/semeval/scratch/subs/baseline-split/S0025322712001600-2406.tsv'))\r\n",
      "\u001b[0;32mPassed! :)\u001b[0m\r\n",
      "\r\n",
      "Validating Vlad(source=LocalFile('/Users/harperco/projects/semeval/scratch/subs/baseline-split/S2213671113000921-994.tsv'))\r\n",
      "\u001b[0;32mPassed! :)\u001b[0m\r\n",
      "\r\n",
      "Validating Vlad(source=LocalFile('/Users/harperco/projects/semeval/scratch/subs/baseline-split/S0165587612003680-998.tsv'))\r\n",
      "\u001b[0;32mPassed! :)\u001b[0m\r\n",
      "\r\n",
      "Validating Vlad(source=LocalFile('/Users/harperco/projects/semeval/scratch/subs/baseline-split/S0016236113008041-2924.tsv'))\r\n",
      "\u001b[0;32mPassed! :)\u001b[0m\r\n",
      "\r\n",
      "Validating Vlad(source=LocalFile('/Users/harperco/projects/semeval/scratch/subs/baseline-split/S0950705113001895-23699.tsv'))\r\n",
      "\u001b[0;32mPassed! :)\u001b[0m\r\n",
      "\r\n",
      "Validating Vlad(source=LocalFile('/Users/harperco/projects/semeval/scratch/subs/baseline-split/S0006322312001096-1248.tsv'))\r\n",
      "\u001b[0;32mPassed! :)\u001b[0m\r\n",
      "\r\n",
      "Validating Vlad(source=LocalFile('/Users/harperco/projects/semeval/scratch/subs/baseline-split/S016412121300188X-4069.tsv'))\r\n",
      "\u001b[0;32mPassed! :)\u001b[0m\r\n",
      "\r\n",
      "Validating Vlad(source=LocalFile('/Users/harperco/projects/semeval/scratch/subs/baseline-split/S0019103512004009-2930.tsv'))\r\n",
      "\u001b[0;32mPassed! :)\u001b[0m\r\n",
      "\r\n",
      "Validating Vlad(source=LocalFile('/Users/harperco/projects/semeval/scratch/subs/baseline-split/S1386142513006823-2084.tsv'))\r\n",
      "\u001b[0;32mPassed! :)\u001b[0m\r\n",
      "\r\n",
      "Validating Vlad(source=LocalFile('/Users/harperco/projects/semeval/scratch/subs/baseline-split/S0960148113004989-3327.tsv'))\r\n",
      "\u001b[0;32mPassed! :)\u001b[0m\r\n",
      "\r\n",
      "Validating Vlad(source=LocalFile('/Users/harperco/projects/semeval/scratch/subs/baseline-split/S2213671113000738-435.tsv'))\r\n",
      "\u001b[0;32mPassed! :)\u001b[0m\r\n",
      "\r\n",
      "Validating Vlad(source=LocalFile('/Users/harperco/projects/semeval/scratch/subs/baseline-split/S016412121300188X-5038.tsv'))\r\n",
      "\u001b[0;32mPassed! :)\u001b[0m\r\n",
      "\r\n",
      "Validating Vlad(source=LocalFile('/Users/harperco/projects/semeval/scratch/subs/baseline-split/S0032063312003054-1990.tsv'))\r\n",
      "\u001b[0;32mPassed! :)\u001b[0m\r\n",
      "\r\n",
      "Validating Vlad(source=LocalFile('/Users/harperco/projects/semeval/scratch/subs/baseline-split/S0012821X12004384-1415.tsv'))\r\n",
      "\u001b[0;32mPassed! :)\u001b[0m\r\n",
      "\r\n",
      "Validating Vlad(source=LocalFile('/Users/harperco/projects/semeval/scratch/subs/baseline-split/S0921818113002245-1571.tsv'))\r\n",
      "\u001b[0;32mPassed! :)\u001b[0m\r\n",
      "\r\n",
      "Validating Vlad(source=LocalFile('/Users/harperco/projects/semeval/scratch/subs/baseline-split/S0167880913001229-1304.tsv'))\r\n",
      "\u001b[0;32mPassed! :)\u001b[0m\r\n",
      "\r\n",
      "Validating Vlad(source=LocalFile('/Users/harperco/projects/semeval/scratch/subs/baseline-split/S0019103512004009-3962.tsv'))\r\n",
      "\u001b[0;32mPassed! :)\u001b[0m\r\n",
      "\r\n",
      "Validating Vlad(source=LocalFile('/Users/harperco/projects/semeval/scratch/subs/baseline-split/S016412121300188X-4640.tsv'))\r\n",
      "\u001b[0;32mPassed! :)\u001b[0m\r\n",
      "\r\n",
      "Validating Vlad(source=LocalFile('/Users/harperco/projects/semeval/scratch/subs/baseline-split/S1084804513001987-7409.tsv'))\r\n",
      "\u001b[0;32mPassed! :)\u001b[0m\r\n",
      "\r\n",
      "Validating Vlad(source=LocalFile('/Users/harperco/projects/semeval/scratch/subs/baseline-split/S0012821X13002185-835.tsv'))\r\n",
      "\u001b[0;32mPassed! :)\u001b[0m\r\n",
      "\r\n",
      "Validating Vlad(source=LocalFile('/Users/harperco/projects/semeval/scratch/subs/baseline-split/S0032386113005454-2865.tsv'))\r\n",
      "\u001b[0;32mPassed! :)\u001b[0m\r\n",
      "\r\n",
      "Validating Vlad(source=LocalFile('/Users/harperco/projects/semeval/scratch/subs/baseline-split/S0012821X12004384-1302.tsv'))\r\n",
      "\u001b[0;32mPassed! :)\u001b[0m\r\n",
      "\r\n",
      "Validating Vlad(source=LocalFile('/Users/harperco/projects/semeval/scratch/subs/baseline-split/S0006322312001096-1177.tsv'))\r\n",
      "\u001b[0;32mPassed! :)\u001b[0m\r\n",
      "\r\n",
      "Validating Vlad(source=LocalFile('/Users/harperco/projects/semeval/scratch/subs/baseline-split/S0019103513005058-3154.tsv'))\r\n",
      "\u001b[0;32mPassed! :)\u001b[0m\r\n",
      "\r\n",
      "Validating Vlad(source=LocalFile('/Users/harperco/projects/semeval/scratch/subs/baseline-split/S2213671113000738-647.tsv'))\r\n",
      "\u001b[0;32mPassed! :)\u001b[0m\r\n",
      "\r\n",
      "Validating Vlad(source=LocalFile('/Users/harperco/projects/semeval/scratch/subs/baseline-split/S0021979713004438-1415.tsv'))\r\n",
      "\u001b[0;32mPassed! :)\u001b[0m\r\n",
      "\r\n",
      "Validating Vlad(source=LocalFile('/Users/harperco/projects/semeval/scratch/subs/baseline-split/S2213671113000738-684.tsv'))\r\n",
      "\u001b[0;32mPassed! :)\u001b[0m\r\n",
      "\r\n",
      "Validating Vlad(source=LocalFile('/Users/harperco/projects/semeval/scratch/subs/baseline-split/S0019103512001388-3081.tsv'))\r\n",
      "\u001b[0;32mPassed! :)\u001b[0m\r\n",
      "\r\n",
      "Validating Vlad(source=LocalFile('/Users/harperco/projects/semeval/scratch/subs/baseline-split/S2213671113000738-445.tsv'))\r\n",
      "\u001b[0;32mPassed! :)\u001b[0m\r\n",
      "\r\n",
      "Validating Vlad(source=LocalFile('/Users/harperco/projects/semeval/scratch/subs/baseline-split/S2213671113000908-810.tsv'))\r\n",
      "\u001b[0;32mPassed! :)\u001b[0m\r\n",
      "\r\n",
      "Validating Vlad(source=LocalFile('/Users/harperco/projects/semeval/scratch/subs/baseline-split/S030881461301604X-1002.tsv'))\r\n",
      "\u001b[0;32mPassed! :)\u001b[0m\r\n",
      "\r\n",
      "Validating Vlad(source=LocalFile('/Users/harperco/projects/semeval/scratch/subs/baseline-split/S0019103512003533-5031.tsv'))\r\n",
      "\u001b[0;32mPassed! :)\u001b[0m\r\n",
      "\r\n",
      "Validating Vlad(source=LocalFile('/Users/harperco/projects/semeval/scratch/subs/baseline-split/S0022000014000026-18167.tsv'))\r\n",
      "\u001b[0;32mPassed! :)\u001b[0m\r\n",
      "\r\n",
      "Validating Vlad(source=LocalFile('/Users/harperco/projects/semeval/scratch/subs/baseline-split/S0012821X13002185-994.tsv'))\r\n",
      "\u001b[0;32mPassed! :)\u001b[0m\r\n",
      "\r\n",
      "Validating Vlad(source=LocalFile('/Users/harperco/projects/semeval/scratch/subs/baseline-split/S0032386113005454-2308.tsv'))\r\n",
      "\u001b[0;32mPassed! :)\u001b[0m\r\n",
      "\r\n",
      "Validating Vlad(source=LocalFile('/Users/harperco/projects/semeval/scratch/subs/baseline-split/S0019103512003995-2737.tsv'))\r\n",
      "\u001b[0;32mPassed! :)\u001b[0m\r\n",
      "\r\n",
      "Validating Vlad(source=LocalFile('/Users/harperco/projects/semeval/scratch/subs/baseline-split/S0168945213001805-4536.tsv'))\r\n",
      "\u001b[0;32mPassed! :)\u001b[0m\r\n",
      "\r\n",
      "Validating Vlad(source=LocalFile('/Users/harperco/projects/semeval/scratch/subs/baseline-split/S0019103512002801-1927.tsv'))\r\n",
      "\u001b[0;32mPassed! :)\u001b[0m\r\n",
      "\r\n",
      "Validating Vlad(source=LocalFile('/Users/harperco/projects/semeval/scratch/subs/baseline-split/S1873506114000075-1242.tsv'))\r\n",
      "\u001b[0;32mPassed! :)\u001b[0m\r\n",
      "\r\n",
      "Validating Vlad(source=LocalFile('/Users/harperco/projects/semeval/scratch/subs/baseline-split/S016412121300188X-4436.tsv'))\r\n",
      "\u001b[0;32mPassed! :)\u001b[0m\r\n",
      "\r\n",
      "Validating Vlad(source=LocalFile('/Users/harperco/projects/semeval/scratch/subs/baseline-split/S0022399913003358-943.tsv'))\r\n",
      "\u001b[0;32mPassed! :)\u001b[0m\r\n",
      "\r\n",
      "Validating Vlad(source=LocalFile('/Users/harperco/projects/semeval/scratch/subs/baseline-split/S0927024813003036-2011.tsv'))\r\n",
      "\u001b[0;32mPassed! :)\u001b[0m\r\n",
      "\r\n",
      "Validating Vlad(source=LocalFile('/Users/harperco/projects/semeval/scratch/subs/baseline-split/S175058361300203X-1280.tsv'))\r\n",
      "\u001b[0;32mPassed! :)\u001b[0m\r\n",
      "\r\n",
      "Validating Vlad(source=LocalFile('/Users/harperco/projects/semeval/scratch/subs/baseline-split/S0019103512004009-2821.tsv'))\r\n",
      "\u001b[0;32mPassed! :)\u001b[0m\r\n",
      "\r\n",
      "Validating Vlad(source=LocalFile('/Users/harperco/projects/semeval/scratch/subs/baseline-split/S0168945213001805-4454.tsv'))\r\n",
      "\u001b[0;32mPassed! :)\u001b[0m\r\n",
      "\r\n",
      "Validating Vlad(source=LocalFile('/Users/harperco/projects/semeval/scratch/subs/baseline-split/S0378383912000130-3732.tsv'))\r\n",
      "\u001b[0;32mPassed! :)\u001b[0m\r\n",
      "\r\n",
      "Validating Vlad(source=LocalFile('/Users/harperco/projects/semeval/scratch/subs/baseline-split/S2213671113001306-1398.tsv'))\r\n",
      "\u001b[0;32mPassed! :)\u001b[0m\r\n",
      "\r\n",
      "Validating Vlad(source=LocalFile('/Users/harperco/projects/semeval/scratch/subs/baseline-split/S0019103512002801-1849.tsv'))\r\n",
      "\u001b[0;32mPassed! :)\u001b[0m\r\n",
      "\r\n",
      "Validating Vlad(source=LocalFile('/Users/harperco/projects/semeval/scratch/subs/baseline-split/S0378383911001669-1058.tsv'))\r\n",
      "\u001b[0;32mPassed! :)\u001b[0m\r\n",
      "\r\n",
      "Validating Vlad(source=LocalFile('/Users/harperco/projects/semeval/scratch/subs/baseline-split/S0019103512003533-3299.tsv'))\r\n",
      "\u001b[0;32mPassed! :)\u001b[0m\r\n",
      "\r\n",
      "Validating Vlad(source=LocalFile('/Users/harperco/projects/semeval/scratch/subs/baseline-split/S2213671113000738-738.tsv'))\r\n",
      "\u001b[0;32mPassed! :)\u001b[0m\r\n",
      "\r\n",
      "Validating Vlad(source=LocalFile('/Users/harperco/projects/semeval/scratch/subs/baseline-split/S0006322312001096-1197.tsv'))\r\n",
      "\u001b[0;32mPassed! :)\u001b[0m\r\n",
      "\r\n",
      "Validating Vlad(source=LocalFile('/Users/harperco/projects/semeval/scratch/subs/baseline-split/S0925443913001385-1429.tsv'))\r\n",
      "\u001b[0;32mPassed! :)\u001b[0m\r\n",
      "\r\n",
      "Validating Vlad(source=LocalFile('/Users/harperco/projects/semeval/scratch/subs/baseline-split/S0019103512003533-3306.tsv'))\r\n",
      "\u001b[0;32mPassed! :)\u001b[0m\r\n",
      "\r\n",
      "Validating Vlad(source=LocalFile('/Users/harperco/projects/semeval/scratch/subs/baseline-split/S0019103512004009-3488.tsv'))\r\n",
      "\u001b[0;32mPassed! :)\u001b[0m\r\n",
      "\r\n",
      "Validating Vlad(source=LocalFile('/Users/harperco/projects/semeval/scratch/subs/baseline-split/S0167819113001051-1550.tsv'))\r\n",
      "\u001b[0;32mPassed! :)\u001b[0m\r\n",
      "\r\n",
      "Validating Vlad(source=LocalFile('/Users/harperco/projects/semeval/scratch/subs/baseline-split/S0167880913001229-1033.tsv'))\r\n",
      "\u001b[0;32mPassed! :)\u001b[0m\r\n",
      "\r\n",
      "Validating Vlad(source=LocalFile('/Users/harperco/projects/semeval/scratch/subs/baseline-split/S0921818113002245-859.tsv'))\r\n",
      "\u001b[0;32mPassed! :)\u001b[0m\r\n",
      "\r\n",
      "Validating Vlad(source=LocalFile('/Users/harperco/projects/semeval/scratch/subs/baseline-split/S0019103512003533-4685.tsv'))\r\n",
      "\u001b[0;32mPassed! :)\u001b[0m\r\n",
      "\r\n",
      "Validating Vlad(source=LocalFile('/Users/harperco/projects/semeval/scratch/subs/baseline-split/S0019103512003995-3420.tsv'))\r\n",
      "\u001b[0;32mPassed! :)\u001b[0m\r\n",
      "\r\n",
      "Validating Vlad(source=LocalFile('/Users/harperco/projects/semeval/scratch/subs/baseline-split/S0022459611006116-1160.tsv'))\r\n",
      "\u001b[0;32mPassed! :)\u001b[0m\r\n",
      "\r\n",
      "Validating Vlad(source=LocalFile('/Users/harperco/projects/semeval/scratch/subs/baseline-split/S1389128612002496-6119.tsv'))\r\n",
      "\u001b[0;32mPassed! :)\u001b[0m\r\n",
      "\r\n",
      "Validating Vlad(source=LocalFile('/Users/harperco/projects/semeval/scratch/subs/baseline-split/S0006322312001096-1230.tsv'))\r\n",
      "\u001b[0;32mPassed! :)\u001b[0m\r\n",
      "\r\n",
      "Validating Vlad(source=LocalFile('/Users/harperco/projects/semeval/scratch/subs/baseline-split/S0378383912000130-1054.tsv'))\r\n",
      "\u001b[0;32mPassed! :)\u001b[0m\r\n",
      "\r\n",
      "Validating Vlad(source=LocalFile('/Users/harperco/projects/semeval/scratch/subs/baseline-split/S0019103512003533-5598.tsv'))\r\n",
      "\u001b[0;32mPassed! :)\u001b[0m\r\n",
      "\r\n",
      "Validating Vlad(source=LocalFile('/Users/harperco/projects/semeval/scratch/subs/baseline-split/S0019103512004009-4492.tsv'))\r\n",
      "\u001b[0;32mPassed! :)\u001b[0m\r\n",
      "\r\n",
      "Validating Vlad(source=LocalFile('/Users/harperco/projects/semeval/scratch/subs/baseline-split/S0925443913001385-1638.tsv'))\r\n",
      "\u001b[0;32mPassed! :)\u001b[0m\r\n",
      "\r\n",
      "Validating Vlad(source=LocalFile('/Users/harperco/projects/semeval/scratch/subs/baseline-split/S1367912013002277-1213.tsv'))\r\n",
      "\u001b[0;32mPassed! :)\u001b[0m\r\n",
      "\r\n",
      "Validating Vlad(source=LocalFile('/Users/harperco/projects/semeval/scratch/subs/baseline-split/S0012821X12004384-1284.tsv'))\r\n",
      "\u001b[0;32mPassed! :)\u001b[0m\r\n",
      "\r\n",
      "Validating Vlad(source=LocalFile('/Users/harperco/projects/semeval/scratch/subs/baseline-split/S0012821X13007309-1605.tsv'))\r\n",
      "\u001b[0;32mPassed! :)\u001b[0m\r\n",
      "\r\n",
      "Validating Vlad(source=LocalFile('/Users/harperco/projects/semeval/scratch/subs/baseline-split/S0165587612003680-953.tsv'))\r\n",
      "\u001b[0;32mPassed! :)\u001b[0m\r\n",
      "\r\n",
      "Validating Vlad(source=LocalFile('/Users/harperco/projects/semeval/scratch/subs/baseline-split/S0021979713004438-1907.tsv'))\r\n",
      "\u001b[0;32mPassed! :)\u001b[0m\r\n",
      "\r\n",
      "Validating Vlad(source=LocalFile('/Users/harperco/projects/semeval/scratch/subs/baseline-split/S0012821X13002185-1231.tsv'))\r\n",
      "\u001b[0;32mPassed! :)\u001b[0m\r\n",
      "\r\n",
      "Validating Vlad(source=LocalFile('/Users/harperco/projects/semeval/scratch/subs/baseline-split/S016412121300188X-4937.tsv'))\r\n",
      "\u001b[0;32mPassed! :)\u001b[0m\r\n",
      "\r\n",
      "Validating Vlad(source=LocalFile('/Users/harperco/projects/semeval/scratch/subs/baseline-split/S0925443913001385-839.tsv'))\r\n",
      "\u001b[0;32mPassed! :)\u001b[0m\r\n",
      "\r\n",
      "Validating Vlad(source=LocalFile('/Users/harperco/projects/semeval/scratch/subs/baseline-split/S0019103512003533-4971.tsv'))\r\n",
      "\u001b[0;32mPassed! :)\u001b[0m\r\n",
      "\r\n",
      "Validating Vlad(source=LocalFile('/Users/harperco/projects/semeval/scratch/subs/baseline-split/S0012821X12004384-1221.tsv'))\r\n",
      "\u001b[0;32mPassed! :)\u001b[0m\r\n",
      "\r\n",
      "Validating Vlad(source=LocalFile('/Users/harperco/projects/semeval/scratch/subs/baseline-split/S0019103512003533-5072.tsv'))\r\n",
      "\u001b[0;32mPassed! :)\u001b[0m\r\n",
      "\r\n",
      "Validating Vlad(source=LocalFile('/Users/harperco/projects/semeval/scratch/subs/baseline-split/S0019103512004009-3825.tsv'))\r\n",
      "\u001b[0;32mPassed! :)\u001b[0m\r\n",
      "\r\n",
      "Validating Vlad(source=LocalFile('/Users/harperco/projects/semeval/scratch/subs/baseline-split/S0016236113008041-3171.tsv'))\r\n",
      "\u001b[0;32mPassed! :)\u001b[0m\r\n",
      "\r\n",
      "Validating Vlad(source=LocalFile('/Users/harperco/projects/semeval/scratch/subs/baseline-split/S0016236113008041-3159.tsv'))\r\n",
      "\u001b[0;32mPassed! :)\u001b[0m\r\n",
      "\r\n",
      "Validating Vlad(source=LocalFile('/Users/harperco/projects/semeval/scratch/subs/baseline-split/S0927775713009606-1216.tsv'))\r\n",
      "\u001b[0;32mPassed! :)\u001b[0m\r\n",
      "\r\n",
      "Validating Vlad(source=LocalFile('/Users/harperco/projects/semeval/scratch/subs/baseline-split/S1359645413009816-2973.tsv'))\r\n",
      "\u001b[0;32mPassed! :)\u001b[0m\r\n",
      "\r\n",
      "Validating Vlad(source=LocalFile('/Users/harperco/projects/semeval/scratch/subs/baseline-split/S2213158213000582-1340.tsv'))\r\n",
      "\u001b[0;32mPassed! :)\u001b[0m\r\n",
      "\r\n",
      "Validating Vlad(source=LocalFile('/Users/harperco/projects/semeval/scratch/subs/baseline-split/S0927024813002420-1202.tsv'))\r\n",
      "\u001b[0;32mPassed! :)\u001b[0m\r\n",
      "\r\n",
      "Validating Vlad(source=LocalFile('/Users/harperco/projects/semeval/scratch/subs/baseline-split/S0019103512003995-2760.tsv'))\r\n",
      "\u001b[0;32mPassed! :)\u001b[0m\r\n",
      "\r\n",
      "Validating Vlad(source=LocalFile('/Users/harperco/projects/semeval/scratch/subs/baseline-split/S0927024813001955-679.tsv'))\r\n",
      "\u001b[0;32mPassed! :)\u001b[0m\r\n",
      "\r\n",
      "Validating Vlad(source=LocalFile('/Users/harperco/projects/semeval/scratch/subs/baseline-split/S0921818113002245-1752.tsv'))\r\n",
      "\u001b[0;32mPassed! :)\u001b[0m\r\n",
      "\r\n",
      "Validating Vlad(source=LocalFile('/Users/harperco/projects/semeval/scratch/subs/baseline-split/S0927775713009606-1361.tsv'))\r\n",
      "\u001b[0;32mPassed! :)\u001b[0m\r\n",
      "\r\n",
      "Validating Vlad(source=LocalFile('/Users/harperco/projects/semeval/scratch/subs/baseline-split/S175058361300203X-1638.tsv'))\r\n",
      "\u001b[0;32mPassed! :)\u001b[0m\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submission directory contains: 93\n",
      "Gold directory contains: 88\n",
      "Gold count of Quantity: 360\n",
      "Gold count of MeasuredProperty: 221\n",
      "Gold count of MeasuredEntity: 352\n",
      "Gold count of Qualifier: 95\n",
      "\n",
      "Submission count of Quantity: 363\n",
      "Submission count of MeasuredProperty: 57\n",
      "Submission count of MeasuredEntity: 89\n",
      "Submission count of Qualifier: 30\n",
      "\n",
      "Working in mode overall\n",
      "True positives (matching rows): 616\n",
      "False positives (submission only): 461\n",
      "False negatives (gold only): 1553\n",
      "\n",
      "Precision: 0.5719591457753017\n",
      "Recall: 0.28400184416781926\n",
      "F-measure: 0.3795440542205792\n",
      "\n",
      "Overall Score Exact Match: 0.19467680608365018\n",
      "Overall Score F1 (Overlap): 0.23017754630369258\n"
     ]
    }
   ],
   "source": [
    "!python /Users/harperco/projects/semeval/MeasEval/eval/measeval-eval.py -i \"/Users/harperco/projects/semeval/\" -g \"scratch/gold/\" -s \"scratch/subs/baseline-split/\" -l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Noteably, we see from the two cells above that the more involved matching of spans based on proximity doesn't add muchj more than .01 to the overall F1 score.\n",
    "\n",
    "### Now we'll repeat the training above, but using the full set of training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training for QUANT\n",
      "['ner']\n",
      "Losses {'ner': 3953.4779805369817}\n",
      "Losses {'ner': 1610.3809841818131}\n",
      "Losses {'ner': 1447.6390614423142}\n",
      "Losses {'ner': 1590.713278219099}\n",
      "Losses {'ner': 1035.438445837068}\n",
      "Losses {'ner': 842.9937791002653}\n",
      "Losses {'ner': 807.0279002391468}\n",
      "Losses {'ner': 725.251484618387}\n",
      "Losses {'ner': 736.8128775222854}\n",
      "Losses {'ner': 510.5135251911736}\n",
      "Losses {'ner': 403.54704855692324}\n",
      "Losses {'ner': 440.63163169393533}\n",
      "Losses {'ner': 534.9749610727363}\n",
      "Losses {'ner': 403.5207589780159}\n",
      "Losses {'ner': 438.44466070558275}\n",
      "Losses {'ner': 375.71735788605736}\n",
      "Losses {'ner': 337.82190201392393}\n",
      "Losses {'ner': 318.26199420943146}\n",
      "Losses {'ner': 313.2439186123273}\n",
      "Losses {'ner': 279.8252344196454}\n",
      "Starting training for ME\n",
      "['ner']\n",
      "Losses {'ner': 3567.991322077707}\n",
      "Losses {'ner': 3004.681139232435}\n",
      "Losses {'ner': 2878.56946327679}\n",
      "Losses {'ner': 3680.0836548520483}\n",
      "Losses {'ner': 3098.901827742639}\n",
      "Losses {'ner': 4310.777471259318}\n",
      "Losses {'ner': 4702.907995848022}\n",
      "Losses {'ner': 2392.7066890111587}\n",
      "Losses {'ner': 3355.2577918454663}\n",
      "Losses {'ner': 3825.7345735955564}\n",
      "Losses {'ner': 3644.053779903215}\n",
      "Losses {'ner': 2636.672667558041}\n",
      "Losses {'ner': 2614.094888106836}\n",
      "Losses {'ner': 2071.9383536638416}\n",
      "Losses {'ner': 2909.711295977985}\n",
      "Losses {'ner': 2352.905331086876}\n",
      "Losses {'ner': 2389.8679093906685}\n",
      "Losses {'ner': 2422.180709001954}\n",
      "Losses {'ner': 2434.3578217090258}\n",
      "Losses {'ner': 2497.2758859205196}\n",
      "Starting training for MP\n",
      "['ner']\n",
      "Losses {'ner': 3254.635304490764}\n",
      "Losses {'ner': 2663.715997780763}\n",
      "Losses {'ner': 2348.8328890465664}\n",
      "Losses {'ner': 1795.326909099107}\n",
      "Losses {'ner': 3236.256570879201}\n",
      "Losses {'ner': 4285.845626492926}\n",
      "Losses {'ner': 2458.6593580160707}\n",
      "Losses {'ner': 1925.0942993211756}\n",
      "Losses {'ner': 4721.216462012586}\n",
      "Losses {'ner': 1945.3910962461212}\n",
      "Losses {'ner': 1549.3754366298188}\n",
      "Losses {'ner': 944.1052869211973}\n",
      "Losses {'ner': 1624.6338786767762}\n",
      "Losses {'ner': 2686.8238938720065}\n",
      "Losses {'ner': 1489.7405206391245}\n",
      "Losses {'ner': 1301.5361006838375}\n",
      "Losses {'ner': 1123.3966580144938}\n",
      "Losses {'ner': 1284.0428901308992}\n",
      "Losses {'ner': 882.258380596823}\n",
      "Losses {'ner': 1515.0388639574971}\n",
      "Starting training for QUAL\n",
      "['ner']\n",
      "Losses {'ner': 2823.0551365175406}\n",
      "Losses {'ner': 1576.8008423001136}\n",
      "Losses {'ner': 2925.4940435581243}\n",
      "Losses {'ner': 3604.447463058135}\n",
      "Losses {'ner': 2857.847622477497}\n",
      "Losses {'ner': 2886.416783317536}\n",
      "Losses {'ner': 3050.597004963253}\n",
      "Losses {'ner': 2389.6494506696154}\n",
      "Losses {'ner': 1638.3461003475861}\n",
      "Losses {'ner': 2741.8141291871343}\n",
      "Losses {'ner': 1994.0035973859292}\n",
      "Losses {'ner': 2436.4625460208695}\n",
      "Losses {'ner': 1688.4322575375732}\n",
      "Losses {'ner': 2163.3145777616187}\n",
      "Losses {'ner': 2092.949604945921}\n",
      "Losses {'ner': 2691.152707099121}\n",
      "Losses {'ner': 2019.2140030419773}\n",
      "Losses {'ner': 2135.5793789885665}\n",
      "Losses {'ner': 2588.827753628153}\n",
      "Losses {'ner': 2176.0787715522033}\n"
     ]
    }
   ],
   "source": [
    "# Now we'll repeat the same set of things for the full set of training data:\n",
    "\n",
    "models = {}\n",
    "for entType in [\"QUANT\", \"ME\", \"MP\", \"QUAL\"]:\n",
    "    print(\"Starting training for \" + entType)\n",
    "    models[entType] = spacy.blank(\"en\")\n",
    "    ner = models[entType].create_pipe(\"ner\")\n",
    "    models[entType].add_pipe(ner)\n",
    "    print(models[entType].pipe_names)\n",
    "    ner.add_label(entType)\n",
    "    optimizer = models[entType].begin_training()\n",
    "\n",
    "    sizes = compounding(1.0, 4.0, 1.001)\n",
    "    # batch up the examples using spaCy's minibatch\n",
    "    for itn in range(20):\n",
    "        random.shuffle(alltraindata[entType])\n",
    "        batches = minibatch(alltraindata[entType], size=sizes)\n",
    "        losses = {}\n",
    "        for batch in batches:\n",
    "            texts, annotations = zip(*batch)\n",
    "            models[entType].update(texts, annotations, sgd=optimizer, drop=0.35, losses=losses)\n",
    "        print(\"Losses\", losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the eval text data together\n",
    "evalpath = \"/Users/harperco/projects/semeval/measeval-publish-stage/eval/text/\"\n",
    "\n",
    "evaltextset = {}\n",
    "for fn in os.listdir(evalpath):\n",
    "    with open(evalpath+fn) as textfile:\n",
    "        text = textfile.read() #.splitlines()\n",
    "        #print(fn[:-4])\n",
    "        evaltextset[fn[:-4]] = text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And build our entity predictions for each of the four models...\n",
    "ents = {}\n",
    "counts = { \"total\": 0, \"QUANT\": 0, \"ME\": 0, \"MP\": 0, \"QUAL\": 0}\n",
    "for docid,text in evaltextset.items():\n",
    "    counts[\"total\"] += 1\n",
    "    ents[docid] = {}\n",
    "\n",
    "    for entType in [\"QUANT\", \"ME\", \"MP\", \"QUAL\"]:\n",
    "        ents[docid][entType] = ()\n",
    "        doc = models[entType](text)\n",
    "        ents[docid][entType] = doc.ents\n",
    "        if len(list(ents[docid][entType])) > 0:\n",
    "            counts[entType]+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect a set of unique units for use in populating the unit data...\n",
    "import json\n",
    "units = []\n",
    "\n",
    "for fileset in trainpaths:\n",
    "    for fn in os.listdir(fileset):\n",
    "        # This time we run the unit collection for all the training data\n",
    "        # if fn[:-4] in trainIds:\n",
    "            with open(fileset+fn) as annotfile:\n",
    "                text = textset[fn[:-4]]\n",
    "                next(annotfile)\n",
    "                annots = annotfile.read().splitlines()\n",
    "                for a in annots:\n",
    "                    annot = a.split(\"\\t\")\n",
    "                    atype = typemap[annot[2]]\n",
    "                    if atype == \"QUANT\" and annot[7] != \"\":\n",
    "                        jsondata = json.loads(annot[7])\n",
    "                        if \"unit\" in jsondata:\n",
    "                            units.append(jsondata[\"unit\"])\n",
    "uniqunits = list(set(units))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simpler version, let's just check the lenths of everything\n",
    "# Then pop them off in the order they exist.\n",
    "header = \"docId\\tannotSet\\tannotType\\tstartOffset\\tendOffset\\tannotId\\ttext\\tother\"\n",
    "subdir = \"/Users/harperco/projects/semeval/scratch/subs/baseline-simpler-2/\"\n",
    "count = 0\n",
    "for docId, allents in ents.items():\n",
    "    #if docId == \"S0378112713005288-1800\":\n",
    "    #print(allents)\n",
    "    annotSet = 1\n",
    "    #print(str(len(allents['QUANT']))+\"|\"+str(len(allents['ME']))+\"|\"\n",
    "    #      +str(len(allents['MP']))+\"|\"+str(len(allents['QUAL'])))\n",
    "    sub = open(subdir+docId + \".tsv\", \"w\")\n",
    "    sub.write(header+\"\\n\")\n",
    "    for quant in allents['QUANT']:\n",
    "        unitmatches = []\n",
    "        for unit in uniqunits: \n",
    "            if unit in quant.text:\n",
    "                unitmatches.append(unit)\n",
    "        if len(unitmatches) > 0: \n",
    "            unit = max(unitmatches, key=len)\n",
    "        strings = []\n",
    "        meId = 0\n",
    "        annotId = 1\n",
    "        quantString = (docId + \"\\t\" + str(annotSet) + \"\\tQuantity\\t\" + str(quant.start_char) + \"\\t\" +\n",
    "                        str(quant.end_char) + \"\\t\" + str(annotId) + \"\\t\" + quant.text+\"\\t{\\\"unit\\\": \\\"\" + unit +  \"\\\"}\")\n",
    "        strings.append(quantString)\n",
    "        annotId+=1\n",
    "        if (len(allents['ME']) > annotSet-1 and len(allents['MP']) > annotSet-1):\n",
    "            mp = allents['MP'][annotSet-1]\n",
    "            me = allents['ME'][annotSet-1]\n",
    "            mpString = (docId + \"\\t\" + str(annotSet) + \"\\tMeasuredProperty\\t\" + str(mp.start_char) + \"\\t\" + \n",
    "                    str(mp.end_char) + \"\\t\" + str(annotId) + \"\\t\" + mp.text + \"\\t{\\\"HasQuantity\\\": \\\"\" + \n",
    "                    str(annotId-1) + \"\\\"}\" )\n",
    "            strings.append(mpString)\n",
    "            annotId+=1\n",
    "\n",
    "            #print(me.text)\n",
    "            meString = (docId + \"\\t\" + str(annotSet) + \"\\tMeasuredEntity\\t\" + str(me.start_char) + \"\\t\" + \n",
    "                        str(me.end_char) + \"\\t\" + str(annotId) + \"\\t\" + me.text + \"\\t{\\\"HasProperty\\\": \\\"\" + \n",
    "                        str(annotId-1) + \"\\\"}\" )\n",
    "            strings.append(meString)\n",
    "            meId = annotId\n",
    "            annotId+=1\n",
    "        elif (len(allents['ME']) > annotSet-1):\n",
    "            me = allents['ME'][annotSet-1]\n",
    "            meString = (docId + \"\\t\" + str(annotSet) + \"\\tMeasuredEntity\\t\" + str(me.start_char) + \"\\t\" + \n",
    "                        str(me.end_char) + \"\\t\" + str(annotId) + \"\\t\" + me.text + \"\\t{\\\"HasProperty\\\": \\\"\" + \n",
    "                        str(annotId-1) + \"\\\"}\" )\n",
    "            strings.append(meString)\n",
    "            meId = annotId\n",
    "            annotId+=1     \n",
    "        if (len(allents['QUAL']) > annotSet-1 and meId != 0):\n",
    "            qual = allents['QUAL'][annotSet-1]\n",
    "            qualString = (docId + \"\\t\" + str(annotSet) + \"\\tQualifier\\t\" + str(qual.start_char) + \"\\t\" + \n",
    "                        str(qual.end_char) + \"\\t\" + str(annotId) + \"\\t\" + qual.text + \"\\t{\\\"Qualifies\\\": \\\"\" + \n",
    "                        str(meId) + \"\\\"}\" )\n",
    "            strings.append(qualString)\n",
    "            meId = annotId\n",
    "            annotId+=1                           \n",
    "\n",
    "        #print(\"ENT: \" + me.text)\n",
    "        #print(\"PROP: \" + mp.text)\n",
    "        for s in strings:\n",
    "            #print(s)\n",
    "            sub.write(s+\"\\n\")\n",
    "        annotSet+=1\n",
    "    sub.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This last, fairly unweildy chunk of code is:\n",
    "# * collecting everything, \n",
    "# * Building the TSV strings\n",
    "# * Attempting to identify a unit\n",
    "# * matching and populating annotSet based on knockout logic, \n",
    "# * resorting, and populating TSV files.\n",
    "\n",
    "# Configure header string and submission directory (latter needs to exist.)\n",
    "header = \"docId\\tannotSet\\tannotType\\tstartOffset\\tendOffset\\tannotId\\ttext\\tother\"\n",
    "subdir = \"/Users/harperco/projects/semeval/scratch/subs/baseline-2/\"\n",
    "\n",
    "for docId, allents in ents.items():\n",
    "    #print(allents)\n",
    "    # First we collect our Quantities\n",
    "    # We want to get the strin version, the full set, and the \"knockout\" list.\n",
    "    quantstrings = []\n",
    "    quants = []\n",
    "    knockout = []\n",
    "    annotSet = 1\n",
    "    for quant in allents['QUANT']:\n",
    "        # Match units in the Quant, then find the longest unit \n",
    "        unitmatches = []\n",
    "        for unit in uniqunits: \n",
    "            if unit in quant.text:\n",
    "                unitmatches.append(unit)\n",
    "        if len(unitmatches) > 0: \n",
    "            unit = max(unitmatches, key=len)\n",
    "        # Build the quantity string, and also the dictionary for quant and knockout.\n",
    "        quantstrings.append(docId + \"\\t\" + str(annotSet) + \"\\tQuantity\\t\" + str(quant.start_char) + \"\\t\" +\n",
    "                          str(quant.end_char) + \"\\t1\\t\" + quant.text+\"\\t{\\\"unit\\\": \\\"\" + unit +  \"\\\"}\")\n",
    "        quants.append({\"annotSet\": annotSet, \"annotId\": 1, \"start\": quant.start_char, \"end\": quant.end_char, \n",
    "                       \"text\": quant.text, \"type\": \"Quantity\"}) \n",
    "        knockout.append({\"annotSet\": annotSet, \"annotId\": 1, \"start\": quant.start_char, \"end\": quant.end_char, \n",
    "                       \"text\": quant.text, \"type\": \"Quantity\"}) \n",
    "        annotSet+=1\n",
    "    \n",
    "    # So now we want to do the ents, as we need this queued up to do more matching with the MPs\n",
    "    mestrings = []\n",
    "    mestring = \"\"\n",
    "    mes = []\n",
    "    knockoutmes = []\n",
    "    #annotSet = 1\n",
    "    for me in allents['ME']:\n",
    "        knockoutmes.append({\"start\": me.start_char, \"end\": me.end_char, \"text\": me.text, \"type\": \"MeasuredEntity\"}) \n",
    "\n",
    "    # Now we work through our measured properties.\n",
    "    mpstrings = []\n",
    "    mpstring = \"\"\n",
    "    mps = []\n",
    "    knockoutmps = []\n",
    "    for mp in allents['MP']:\n",
    "        if len(knockout) > 0 and len(knockoutmes) > 0:\n",
    "            start = mp.start_char\n",
    "            end = mp.end_char\n",
    "            nearest = {\"dist\": 100000000, \"set\": 0, \"id\": 0, \"index\": 100000000}\n",
    "            index = 0\n",
    "            for q in knockout:\n",
    "                dists = [abs(start-q[\"start\"]), abs(end-q[\"start\"]), abs(start-q[\"end\"]), abs(end-q[\"end\"])]\n",
    "                mindist = min(dists)\n",
    "                if mindist < nearest[\"dist\"]:\n",
    "                    nearest[\"dist\"] = mindist\n",
    "                    nearest[\"set\"] = q[\"annotSet\"]\n",
    "                    nearest[\"id\"] = q[\"annotId\"]\n",
    "                    nearest[\"index\"] = index\n",
    "                index+=1\n",
    "            knockout.pop(nearest[\"index\"])\n",
    "\n",
    "            mpString = (docId + \"\\t\" + str(nearest[\"set\"]) + \"\\tMeasuredProperty\\t\" + str(mp.start_char) + \"\\t\" + \n",
    "                        str(mp.end_char) + \"\\t\" + str(nearest[\"id\"]+1) + \"\\t\" + mp.text + \"\\t{\\\"HasQuantity\\\": \\\"\" + \n",
    "                        str(nearest[\"id\"]) + \"\\\"}\" )\n",
    "            mpstrings.append(mpString)\n",
    "            mps.append({\"annotSet\": nearest[\"set\"], \"annotId\": nearest[\"id\"]+1, \"start\": mp.start_char, \n",
    "                        \"end\": mp.end_char, \"text\": mp.text, \"type\": \"MeasuredProperty\"})\n",
    "            knockoutmps.append({\"annotSet\": nearest[\"set\"], \"annotId\": nearest[\"id\"]+1, \"start\": mp.start_char, \n",
    "                        \"end\": mp.end_char, \"text\": mp.text, \"type\": \"MeasuredProperty\"})\n",
    "\n",
    "            nearestme = {\"dist\": 100000000, \"index\": 100000000}\n",
    "            index = 0\n",
    "            if len(knockoutmes) > 0:\n",
    "                for me in knockoutmes:\n",
    "                    dists = [abs(start-me[\"start\"]), abs(end-me[\"start\"]), abs(start-me[\"end\"]), abs(end-me[\"end\"])]\n",
    "                    mindist = min(dists)\n",
    "                    if mindist < nearestme[\"dist\"]:\n",
    "                        nearestme[\"dist\"] = mindist\n",
    "                        nearestme[\"index\"] = index\n",
    "                    index+=1\n",
    "                meString = (docId + \"\\t\" + str(nearest[\"set\"]) + \"\\tMeasuredEntity\\t\" + str(me[\"start\"]) + \"\\t\" + \n",
    "                            str(me[\"end\"]) + \"\\t\" + str(nearest[\"id\"]+2) + \"\\t\" + me[\"text\"] + \"\\t{\\\"HasProperty\\\": \\\"\" + \n",
    "                            str(nearest[\"id\"]+1) + \"\\\"}\" )   \n",
    "                mestrings.append(meString)\n",
    "\n",
    "                knockoutmes.pop(nearestme[\"index\"])\n",
    "\n",
    "\n",
    "    # Now we do any leftover MEs, which should go straight to a Quantity:\n",
    "\n",
    "    for me in knockoutmes:\n",
    "        start = me[\"start\"]\n",
    "        end = me[\"end\"]\n",
    "        nearest = {\"dist\": 100000000, \"set\": 0, \"id\": 0, \"index\": 100000000, \"type\": \"\"}\n",
    "        index = 0                \n",
    "        for q in knockout:\n",
    "            dists = [abs(start-q[\"start\"]), abs(end-q[\"start\"]), abs(start-q[\"end\"]), abs(end-q[\"end\"])]\n",
    "            mindist = min(dists)\n",
    "            if mindist < nearest[\"dist\"]:\n",
    "                nearest[\"dist\"] = mindist\n",
    "                nearest[\"set\"] = q[\"annotSet\"]\n",
    "                nearest[\"id\"] = q[\"annotId\"]\n",
    "                nearest[\"index\"] = index\n",
    "                nearest[\"type\"] = q[\"type\"]\n",
    "            index+=1\n",
    "        if len(knockout) > 0:\n",
    "            knockout.pop(nearest[\"index\"])\n",
    "            meString = (docId + \"\\t\" + str(nearest[\"set\"]) + \"\\tMeasuredEntity\\t\" + str(me[\"start\"]) + \"\\t\" + \n",
    "                        str(me[\"end\"]) + \"\\t\" + str(nearest[\"id\"]+1) + \"\\t\" + me[\"text\"] + \"\\t{\\\"HasQuantity\\\": \\\"\" + \n",
    "                        str(nearest[\"id\"]) + \"\\\"}\" )   \n",
    "            mestrings.append(meString)\n",
    "            mes.append({\"annotSet\": nearest[\"set\"], \"annotId\": nearest[\"id\"]+1, \"start\": me[\"start\"], \n",
    "                        \"end\": me[\"end\"], \"text\": me[\"text\"], \"type\": \"MeasuredEntity\"})\n",
    "            \n",
    "    #Finally, let's process our Qualifiers:\n",
    "    kitchensink = [x for x in itertools.chain(quants, mps, mes)]\n",
    "    qualstrings = []\n",
    "    for qual in allents['QUAL']:\n",
    "        start = qual.start_char\n",
    "        end = qual.end_char\n",
    "        nearest = {\"dist\": 100000000, \"set\": 0, \"id\": 0, \"index\": 100000000}\n",
    "        index = 0\n",
    "        if len(kitchensink) > 0:\n",
    "            for q in kitchensink:\n",
    "                dists = [abs(start-q[\"start\"]), abs(end-q[\"start\"]), abs(start-q[\"end\"]), abs(end-q[\"end\"])]\n",
    "                mindist = min(dists)\n",
    "                if mindist < nearest[\"dist\"]:\n",
    "                    nearest[\"dist\"] = mindist\n",
    "                    nearest[\"set\"] = q[\"annotSet\"]\n",
    "                    nearest[\"id\"] = q[\"annotId\"]\n",
    "                    nearest[\"index\"] = index\n",
    "                index+=1\n",
    "            kitchensink.pop(nearest[\"index\"])\n",
    "\n",
    "            qualString = (docId + \"\\t\" + str(nearest[\"set\"]) + \"\\tQualifier\\t\" + str(qual.start_char) + \"\\t\" + \n",
    "                        str(qual.end_char) + \"\\t\" + str(nearest[\"id\"]+1) + \"\\t\" + qual.text + \"\\t{\\\"Qualifies\\\": \\\"\" + \n",
    "                        str(nearest[\"id\"]) + \"\\\"}\" )\n",
    "            qualstrings.append(qualString)\n",
    "\n",
    "    # Finally, we collect everythign:\n",
    "\n",
    "    import itertools\n",
    "    allstrings = [x for x in itertools.chain(quantstrings, mpstrings, mestrings, qualstrings)]\n",
    "    sortedstrings = {}\n",
    "\n",
    "    sub = open(subdir+docId + \".tsv\", \"w\")\n",
    "\n",
    "    for string in allstrings:\n",
    "        annotSet = string.split(\"\\t\")[1]\n",
    "        annotId = string.split(\"\\t\")[5]\n",
    "        if annotSet not in sortedstrings:\n",
    "            sortedstrings[annotSet] = {}\n",
    "        sortedstrings[annotSet][annotId] = string   \n",
    "    sub.write(header+\"\\n\")\n",
    "    for aset, val in sortedstrings.items():\n",
    "        for aid, string in val.items():\n",
    "            sub.write(string+\"\\n\")\n",
    "    sub.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "Performance of the 2nd of these two models is currently our strongest baseline, achieveing the following scores on the evaluation data:\n",
    "\n",
    "* Overall Score Exact Match: 0.21156036446469248 \n",
    "* Overall Score F1 (Overlap): 0.23945662847323318 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 ('measeval')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "4efefcdbf29d6bcfb9e799af7a697c736cdfcc0b08f7ff81ab8415b94290da1e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
