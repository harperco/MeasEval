{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "80a582ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\matts\\.conda\\envs\\measeval\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Params not loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\matts\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "try:\n",
    "    from params import params\n",
    "except:\n",
    "    print(\"Params not loaded\")\n",
    "    class Params:\n",
    "        def __init__(self):\n",
    "            self.bert_type = \"bert-base-cased\"\n",
    "            self.device = \"cuda\"\n",
    "            self.batch_size = 16\n",
    "            self.task = \"QUANT\"\n",
    "    params = Params()\n",
    "\n",
    "import random\n",
    "import os\n",
    "import json\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import re\n",
    "\n",
    "typemap = {\"Quantity\": \"QUANT\",\n",
    "           \"MeasuredEntity\": \"ME\", \n",
    "           \"MeasuredProperty\": \"MP\", \n",
    "           \"Qualifier\": \"QUAL\"\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ca3c43fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_path = \"./data/train/text/\"\n",
    "label_path = \"./data/train/tsv/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d345cda6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_doc_ids(text_path):\n",
    "    # Get text and associated IDs from each text\n",
    "    textset = {}\n",
    "    for fn in os.listdir(text_path):\n",
    "        with open(text_path+fn, encoding='utf-8', errors='replace') as textfile:\n",
    "            text = textfile.read()\n",
    "            textset[fn[:-4]] = text\n",
    "\n",
    "    return textset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "51699701",
   "metadata": {},
   "outputs": [],
   "source": [
    "textset = get_doc_ids(text_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bcf575d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Carbon isotopic results of total organic matter (δ13CTOC) and amorphous organic matter (δ13CAOM) against core 22/10a-4 lithology and Apectodinium spp. (%). Blue=bulk rock δ13CTOC; black=δ13CAOM; solid red symbols=bulk rock δ13CTOC from samples with <30% wood/plant tissue (determined from palynological residue of the sample); open red symbols=bulk rock δ13CTOC from samples with >30% wood/plant tissue. The first appearance of Apectodinium augustum identifies the PETM in the North Sea (Bujak and Brinkhuis, 1998), and the first negative shift in δ13C identifies the approximate position of the CIE onset and the Paleocene–Eocene boundary. Values shaded at 2614.7 and 2619.6 m are considered possible outliers based on statistical analysis of the palynological residues (see Section 4.1). Lithologic column shows position of sand intervals (yellow), claystone intervals (brown; predominantly laminated claystone, dark brown), and ash layers (pink). (For interpretation of the references to color in this figure caption, the reader is referred to the web version of this article.)\n"
     ]
    }
   ],
   "source": [
    "print(textset['S0012821X12004384-952'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "62d513eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S0022000014000026-7850\n",
      "233 15 248\n"
     ]
    }
   ],
   "source": [
    "# Load all annotations\n",
    "files_with_label = [file_name[:-4] for file_name in os.listdir(label_path)]\n",
    "all_files_with_or_without_label = [file_name[:-4] for file_name in os.listdir(text_path)]\n",
    "files_without_label = list(set(all_files_with_or_without_label).difference(set(files_with_label)))\n",
    "print(files_without_label[0])\n",
    "print(len(files_with_label), len(files_without_label), len(all_files_with_or_without_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a914a9c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(files_with_label, files_without_label):\n",
    "\n",
    "    alldata = []\n",
    "\n",
    "    # Load annotations from files with label\n",
    "    for fn_no_ext in files_with_label:\n",
    "        fn = fn_no_ext + \".tsv\"\n",
    "        entities = {\"QUANT\": [], \"ME\": [], \"MP\": [], \"QUAL\": []}\n",
    "        with open(label_path+fn, encoding='utf-8', errors='replace') as annotfile:\n",
    "            text = textset[fn[:-4]]\n",
    "            next(annotfile)\n",
    "            annots = annotfile.read().splitlines()\n",
    "            for a in annots:\n",
    "                annot = a.split(\"\\t\")\n",
    "                atype = typemap[annot[2]]\n",
    "                start = int(annot[3])\n",
    "                stop = int(annot[4])\n",
    "                # This is where we toss out the overlaps:\n",
    "                overlap = False\n",
    "                for ent in entities[atype]:\n",
    "                    if ((start >= ent[0] and start <= ent[1]) or (stop >= ent[0] and stop <= ent[1]) or\n",
    "                        (ent[0] >= start and ent[0] <= stop) or (ent[1] >= start and ent[1] <= stop)):\n",
    "                        overlap = True\n",
    "                if overlap == False:    \n",
    "                    entities[atype].append((start, stop, atype))\n",
    "            alldata.append((text,\n",
    "                            {\"QUANT\": entities[\"QUANT\"],\n",
    "                             \"ME\": entities[\"ME\"],\n",
    "                             \"MP\": entities[\"MP\"],\n",
    "                             \"QUAL\": entities[\"QUAL\"]\n",
    "                            },\n",
    "                            (fn,)\n",
    "                        ))\n",
    "\n",
    "    # Load annotations from files without label\n",
    "    for fn in files_without_label:\n",
    "        text = textset[fn]\n",
    "        alldata.append((text,\n",
    "                        {\"QUANT\": [],\n",
    "                         \"ME\": [],\n",
    "                         \"MP\": [],\n",
    "                         \"QUAL\": []\n",
    "                    },\n",
    "                    (fn,)\n",
    "                ))\n",
    "    return alldata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b39e3d1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Data were drawn from the Whitehall II study with baseline examination in 1991; follow-up screenings in 1997, 2003, and 2008; and additional disease ascertainment from hospital data and registry linkage on 5318 participants (mean age 54.8 years, 31% women) without depressive symptoms at baseline. Vascular risk was assessed with the Framingham Cardiovascular, Coronary Heart Disease, and Stroke Risk Scores. New depressive symptoms at each follow-up screening were identified by General Health Questionnaire caseness, a Center for Epidemiologic Studies Depression Scale score ≥16, and use of antidepressant medication.',\n",
       " {'QUANT': [(73, 77, 'QUANT'),\n",
       "   (103, 123, 'QUANT'),\n",
       "   (205, 222, 'QUANT'),\n",
       "   (233, 243, 'QUANT'),\n",
       "   (245, 248, 'QUANT'),\n",
       "   (576, 579, 'QUANT')],\n",
       "  'ME': [(49, 69, 'ME'),\n",
       "   (79, 99, 'ME'),\n",
       "   (25, 43, 'ME'),\n",
       "   (205, 222, 'ME'),\n",
       "   (520, 569, 'ME')],\n",
       "  'MP': [(224, 232, 'MP'), (249, 254, 'MP'), (570, 575, 'MP')],\n",
       "  'QUAL': []},\n",
       " ('S0006322312001096-1136.tsv',))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data = load_dataset(files_with_label, files_without_label)\n",
    "all_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b83bc492",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_split_data(all_data):\n",
    "    # Sentence Tokenize the dataset\n",
    "    processed_data = []\n",
    "\n",
    "    cnt_toks = {\"figs.\": 0, \"fig.\": 0, \"et al.\": 0,\n",
    "                \"ref.\": 0, \"eq.\": 0, \"e.g.\": 0,\n",
    "                \"i.e.\": 0, \"nos.\": 0, \"no.\": 0,\n",
    "                \"spp.\": 0\n",
    "                }\n",
    "    regex_end_checker = [\".*[a-zA-Z]figs\\.$\", \n",
    "                        \".*[a-zA-Z]fig\\.$\",\n",
    "                        \".*[a-zA-Z]et al\\.$\",\n",
    "                        \".*[a-zA-Z]ref\\.$\",\n",
    "                        \".*[a-zA-Z]eq\\.$\",\n",
    "                        \".*[a-zA-Z]e\\.g\\.$\",\n",
    "                        \".*[a-zA-Z]i\\.e\\.$\",\n",
    "                        \".*[a-zA-Z]nos\\.$\",\n",
    "                        \".*[a-zA-Z]no\\.$\",\n",
    "                        \".*[a-zA-Z]spp\\.$\",\n",
    "                        # figs., fig., et al., Ref., Eq., e.g., i.e., Nos., No., spp.\n",
    "                    ]\n",
    "\n",
    "    assert len(cnt_toks) == len(regex_end_checker)\n",
    "\n",
    "    # list of sentences\n",
    "    # for every tokenized sentence obtained\n",
    "        # check if ends with \"fig or Figs or et al.\"\n",
    "        # keep track of the index where the this sentence starts and end,\n",
    "\n",
    "    all_tokenized_data = []\n",
    "    for doc in all_data:\n",
    "        flag = False\n",
    "        sentences = sent_tokenize(doc[0])\n",
    "\n",
    "        fixed_sentence_tokens = []\n",
    "        curr_len = 0\n",
    "        for s in sentences:\n",
    "            if flag == True:\n",
    "                assert s[0] != ' '\n",
    "                white_length = doc[0][curr_len:].find(s[0])\n",
    "\n",
    "                prev_len = len(fixed_sentence_tokens[-1])\n",
    "                fixed_sentence_tokens[-1] = fixed_sentence_tokens[-1] + (\" \"*white_length) + s\n",
    "\n",
    "                assert fixed_sentence_tokens[-1][prev_len+white_length] == doc[0][curr_len+white_length], (fixed_sentence_tokens[-1], doc[0], curr_len, tmp_this_sent_len)\n",
    "                tmp_this_sent_len = white_length + len(s)\n",
    "                assert fixed_sentence_tokens[-1][-1] == doc[0][curr_len+tmp_this_sent_len-1], (fixed_sentence_tokens[-1], doc[0], curr_len, tmp_this_sent_len)\n",
    "                curr_len += tmp_this_sent_len\n",
    "            else:\n",
    "                if len(fixed_sentence_tokens) != 0:\n",
    "                    assert s[0] != ' '\n",
    "                    white_length = doc[0][curr_len:].find(s[0])\n",
    "                    fixed_sentence_tokens.append( (\" \"*white_length) + s )\n",
    "                else:\n",
    "                    fixed_sentence_tokens.append(s)\n",
    "                assert fixed_sentence_tokens[-1][0] == doc[0][curr_len], (fixed_sentence_tokens, doc[0], curr_len, tmp_this_sent_len)\n",
    "                tmp_this_sent_len = len(fixed_sentence_tokens[-1])\n",
    "                assert fixed_sentence_tokens[-1][-1] == doc[0][curr_len+tmp_this_sent_len-1], (fixed_sentence_tokens[-1], doc[0], curr_len, tmp_this_sent_len)\n",
    "                curr_len += tmp_this_sent_len\n",
    "\n",
    "            lower_cased_s = fixed_sentence_tokens[-1].lower()\n",
    "            flag = False\n",
    "            for i, k in enumerate(cnt_toks):\n",
    "                this_regex_pattern = regex_end_checker[i]\n",
    "                if lower_cased_s.endswith(k) and re.match(this_regex_pattern, lower_cased_s) == None:\n",
    "                    cnt_toks[k] += 1\n",
    "                    flag = True\n",
    "                    break\n",
    "\n",
    "        all_tokenized_data.append(fixed_sentence_tokens)      \n",
    "    print(\"Fixed sentence splitting:\", cnt_toks)\n",
    "    return all_tokenized_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a55eac67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fixed sentence splitting: {'figs.': 4, 'fig.': 92, 'et al.': 31, 'ref.': 3, 'eq.': 3, 'e.g.': 8, 'i.e.': 3, 'nos.': 2, 'no.': 3, 'spp.': 2}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Data were drawn from the Whitehall II study with baseline examination in 1991; follow-up screenings in 1997, 2003, and 2008; and additional disease ascertainment from hospital data and registry linkage on 5318 participants (mean age 54.8 years, 31% women) without depressive symptoms at baseline.',\n",
       " ' Vascular risk was assessed with the Framingham Cardiovascular, Coronary Heart Disease, and Stroke Risk Scores.',\n",
       " ' New depressive symptoms at each follow-up screening were identified by General Health Questionnaire caseness, a Center for Epidemiologic Studies Depression Scale score ≥16, and use of antidepressant medication.']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_data = tokenize_split_data(all_data)\n",
    "tokenized_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fba888de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def annotation_map(all_tokenized_data, all_data):\n",
    "    \"\"\"\n",
    "        Inputs:\n",
    "            all_tokenized_data: list of [list of sentences]\n",
    "            all_data: list of Tuple[doc, annotations, (doc_id,)]\n",
    "\n",
    "        Outputs:\n",
    "            all_annotated_split_data: \n",
    "                    [\n",
    "                        Dict{'doc_id': doc_id,\n",
    "                            'sentences': [list of string]\n",
    "                            'offsets': [list of int]\n",
    "                            'annotations': [list of Dict{\"QUANT\": [],\n",
    "                                                        \"ME\": [],\n",
    "                                                        \"MP\": [],\n",
    "                                                        \"QUAL\": []\n",
    "                                                    }\n",
    "                                            ],\n",
    "\n",
    "                            ... Repeat for the second doc\n",
    "            ]\n",
    "    \"\"\"\n",
    "\n",
    "    # in the next loop\n",
    "        # Replace all numbers by zero.\n",
    "        # check if any of the annotations are used for this falls in between the start and end. Make sure no overlap\n",
    "        # add offset as well.\n",
    "\n",
    "    normalize = lambda x: re.sub(r'\\d', '0', x)\n",
    "    all_annotated_split_data = []\n",
    "\n",
    "    for doc, sent_splits in zip(all_data, all_tokenized_data):\n",
    "\n",
    "        this_offsets = []\n",
    "\n",
    "        prev_end = 0\n",
    "        for s in sent_splits:\n",
    "            this_offsets.append([prev_end, prev_end+len(s)])\n",
    "            prev_end += len(s)\n",
    "\n",
    "        this_annotations = []\n",
    "        for s, offset in zip(sent_splits, this_offsets):\n",
    "            this_sent_ann = {}\n",
    "            for k,v in doc[1].items():\n",
    "                this_key_annotation_sentence = []\n",
    "\n",
    "                for ann in v:\n",
    "                    if offset[0] <= ann[0] and ann[1] < offset[1]:\n",
    "                        this_key_annotation_sentence.append((ann[0]-offset[0], ann[1]-offset[0]))\n",
    "\n",
    "                this_sent_ann[k] = this_key_annotation_sentence\n",
    "\n",
    "            this_annotations.append(this_sent_ann)\n",
    "\n",
    "        all_annotated_split_data.append({'doc_id': doc[-1][0],\n",
    "                    'sentences': [normalize(ss) for ss in sent_splits],\n",
    "                    'offsets': this_offsets,\n",
    "                    'annotations': this_annotations\n",
    "                })\n",
    "        # print(all_annotated_split_data[-1])\n",
    "        assert len(all_annotated_split_data[-1]['offsets']) == len(all_annotated_split_data[-1]['sentences'])\n",
    "        assert len(all_annotated_split_data[-1]['offsets']) == len(all_annotated_split_data[-1]['annotations'])\n",
    "\n",
    "    return all_annotated_split_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "36b05468",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_annotated_split_data = annotation_map(tokenized_data, all_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "da32cd7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'doc_id': 'S0019103512004009-2930',\n",
       " 'sentences': ['The differences between previous models arise from different assumptions regarding heating rates and boundary conditions.',\n",
       "  ' In addition to modeling the density profiles of the detected heavy species, we have improved these aspects of the calculations in our work.',\n",
       "  ' For instance, the lower boundary conditions are constrained by results from a detailed photochemical model of the lower atmosphere (Lavvas et al., in preparation).',\n",
       "  ' With regard to the upper boundary conditions, we demonstrate that for HD000000b the extrapolated ‘outflow’ boundary conditions (e.g., Tian et al., 0000) are consistent with recent results from kinetic theory (Volkov et al., 0000a,b) as long as the upper boundary is at a sufficiently high altitude – although uncertainties regarding the interaction of the atmosphere with the stellar wind may limit the validity of both boundary conditions.',\n",
       "  ' We highlight the effect of heating efficiency and stellar flux on the density and temperature profiles, and constrain the likely heating rates by using photoelectron heating efficiencies based on the results of Cecchi-Pestellini et al. (0000) and our own estimates (Section 0.0).',\n",
       "  ' As a result we provide a robust qualitative description of the density profiles, and constrain the mean temperature and velocity profile in the thermosphere.',\n",
       "  ' A second paper by Koskinen et al. (0000) (Paper II) compares our results directly with the observations.'],\n",
       " 'offsets': [[0, 121],\n",
       "  [121, 261],\n",
       "  [261, 425],\n",
       "  [425, 866],\n",
       "  [866, 1146],\n",
       "  [1146, 1304],\n",
       "  [1304, 1409]],\n",
       " 'annotations': [{'QUANT': [], 'ME': [], 'MP': [], 'QUAL': []},\n",
       "  {'QUANT': [], 'ME': [], 'MP': [], 'QUAL': []},\n",
       "  {'QUANT': [], 'ME': [], 'MP': [], 'QUAL': []},\n",
       "  {'QUANT': [], 'ME': [], 'MP': [], 'QUAL': []},\n",
       "  {'QUANT': [], 'ME': [], 'MP': [], 'QUAL': []},\n",
       "  {'QUANT': [], 'ME': [], 'MP': [], 'QUAL': []},\n",
       "  {'QUANT': [], 'ME': [], 'MP': [], 'QUAL': []}]}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_annotated_split_data[246]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5ddd34a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_tok = AutoTokenizer.from_pretrained(\"roberta-base\", use_fast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "088d16b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_dataset(sentence_wise_data, shuffle=False):\n",
    "    \"\"\"\n",
    "        Inputs:\n",
    "            all_annotated_split_data: \n",
    "                    [\n",
    "                        Dict{'doc_id': doc_id,\n",
    "                            'sentences': [list of string]\n",
    "                            'offsets': [list of int]\n",
    "                            'annotations': [list of Dict{\"QUANT\": [],\n",
    "                                                        \"ME\": [],\n",
    "                                                        \"MP\": [],\n",
    "                                                        \"QUAL\": []\n",
    "                                                    }\n",
    "                                            ],\n",
    "                            }\n",
    "                            ... Similar dict for the second doc and so on.\n",
    "            ]\n",
    "        Outputs:\n",
    "            Batched data: [doc_ixs, sent_offsets, sentences, labels, pad_masks]\n",
    "    \"\"\"\n",
    "    # First flatten and shuffle\n",
    "    # Then Batch\n",
    "    flattened = [[doc['doc_id'], doc['sentences'][i], doc['offsets'][i], doc['annotations'][i]]\n",
    "                 for doc in sentence_wise_data for i in range(len(doc['sentences']))]\n",
    "\n",
    "    print(f'Flattened {len(sentence_wise_data)} docs into {len(flattened)} data points',\n",
    "            \"\\nSome examples:\", flattened[:2])\n",
    "    if shuffle:\n",
    "        random.shuffle(flattened)\n",
    "\n",
    "    cls_token_idx = bert_tok.convert_tokens_to_ids(bert_tok.tokenize('[CLS]'))[0]\n",
    "    sep_token_idx = bert_tok.convert_tokens_to_ids(bert_tok.tokenize('[SEP]'))[0]\n",
    "    pad_token_idx = bert_tok.convert_tokens_to_ids(bert_tok.tokenize('[PAD]'))[0]\n",
    "\n",
    "\n",
    "    dataset = []\n",
    "    idx = 0\n",
    "    num_data = len(flattened)\n",
    "    while idx < num_data:\n",
    "        batch_doc_ids = []\n",
    "        batch_sent_offsets = []\n",
    "        batch_raw_text = []\n",
    "        batch_raw_labels = []\n",
    "\n",
    "        for single_docid, single_sentence, single_offset, single_annotations in \\\n",
    "                    flattened[idx:min(idx + batch_size, num_data)]:\n",
    "\n",
    "            batch_doc_ids.append(single_docid)\n",
    "            batch_sent_offsets.append(single_offset)\n",
    "            batch_raw_text.append(single_sentence)\n",
    "            batch_raw_labels.append(single_annotations)\n",
    "\n",
    "\n",
    "        batched_dict = bert_tok.batch_encode_plus(batch_raw_text,\n",
    "                                                    return_offsets_mapping=True,\n",
    "                                                    padding=True)\n",
    "\n",
    "\n",
    "        batch_tokens = torch.LongTensor(batched_dict['input_ids']).to(device)\n",
    "        batch_maxlen = batch_tokens.shape[-1]\n",
    "        pad_masks = torch.LongTensor(batched_dict['attention_mask']).to(device)\n",
    "        batch_offset_mapping = batched_dict['offset_mapping']\n",
    "\n",
    "        # Create sequence labels using token offsets\n",
    "        batch_labels = []\n",
    "        for single_token_offs, single_anns in zip(batch_offset_mapping, batch_raw_labels):\n",
    "            anns = single_anns[task]\n",
    "            single_labels = []\n",
    "            i = 0\n",
    "            for off in single_token_offs:\n",
    "                if off == (0,0):\n",
    "                    single_labels.append(0)\n",
    "                elif type(anns) == list and i < len(anns):\n",
    "                    if off[1] < anns[i][0]:\n",
    "                        single_labels.append(0)\n",
    "                    elif off[0] > anns[i][1]:\n",
    "                        i += 1\n",
    "                        single_labels.append(0)\n",
    "                    else:\n",
    "                        single_labels.append(1)\n",
    "                else:\n",
    "                    single_labels.append(0)\n",
    "            batch_labels.append(single_labels)\n",
    "\n",
    "        batch_labels = torch.LongTensor(batch_labels).to(device)\n",
    "\n",
    "        b = batch_size if (idx + batch_size) < num_data else (num_data - idx)\n",
    "        assert batch_tokens.size() == torch.Size([b, batch_maxlen])\n",
    "        assert batch_labels.size() == torch.Size([b, batch_maxlen])\n",
    "        assert pad_masks.size() == torch.Size([b, batch_maxlen])\n",
    "\n",
    "        dataset.append((batch_tokens, batch_labels, pad_masks, batch_doc_ids, batch_sent_offsets, batch_offset_mapping))\n",
    "        idx += batch_size\n",
    "\n",
    "    print(\"num_batches=\", len(dataset), \" | num_data=\", num_data)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8265ccbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "device = 'cuda'\n",
    "task = 'QUANT'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6c6cd1ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flattened 248 docs into 1265 data points \n",
      "Some examples: [['S0006322312001096-1136.tsv', 'Data were drawn from the Whitehall II study with baseline examination in 0000; follow-up screenings in 0000, 0000, and 0000; and additional disease ascertainment from hospital data and registry linkage on 0000 participants (mean age 00.0 years, 00% women) without depressive symptoms at baseline.', [0, 296], {'QUANT': [(73, 77), (103, 123), (205, 222), (233, 243), (245, 248)], 'ME': [(49, 69), (79, 99), (25, 43), (205, 222)], 'MP': [(224, 232), (249, 254)], 'QUAL': []}], ['S0006322312001096-1136.tsv', ' Vascular risk was assessed with the Framingham Cardiovascular, Coronary Heart Disease, and Stroke Risk Scores.', [296, 407], {'QUANT': [], 'ME': [], 'MP': [], 'QUAL': []}]]\n",
      "num_batches= 20  | num_data= 1265\n"
     ]
    }
   ],
   "source": [
    "batched_dataset = batch_dataset(all_annotated_split_data, shuffle=True if \"train\" in text_path else False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38262170",
   "metadata": {},
   "source": [
    "## End Prep; Begin Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "f82c21f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "import torch, torch.nn as nn\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ce1461ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x22024642410>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch_seed = 434\n",
    "\n",
    "torch.manual_seed(torch_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "3527365d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Train one epoch ####\n",
    "def train(model, batched_dataset, criterion):\n",
    "    model.train() # Put the model to train\n",
    "\n",
    "    # Lets keep track of the losses at each update\n",
    "    train_losses = []\n",
    "    num_batch = 0\n",
    "\n",
    "    for batch in batched_dataset:\n",
    "        # Unpack the batch\n",
    "        (texts, labels, att_masks, doc_ids, offsets, token_offsets) = batch\n",
    "\n",
    "        # Make predictions on the model\n",
    "        preds = model(texts, att_masks)\n",
    "\n",
    "        # Take into account padded while calculating loss\n",
    "        loss_unreduced = criterion(preds.permute(0,2,1), labels)\n",
    "        loss = (loss_unreduced * att_masks).sum() / (att_masks).sum()\n",
    "\n",
    "        # Update model weights\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if num_batch % 10 == 0:\n",
    "            print(\"Train loss at {}:\".format(num_batch), loss.item())\n",
    "\n",
    "        num_batch += 1\n",
    "        # Append losses\n",
    "        train_losses.append(loss.item())\n",
    "\n",
    "    return np.average(train_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "ceaf04bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, batched_dataset, criterion):\n",
    "    target_names=[\"NOT_\" + params.task, params.task]\n",
    "\n",
    "    # Put the model to eval mode\n",
    "    model.eval()\n",
    "\n",
    "    # Keep track on predictions\n",
    "    valid_losses = []\n",
    "    predicts = []\n",
    "    gnd_truths = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in batched_dataset:\n",
    "            # Unpack the batch\n",
    "            (texts, labels, att_masks, doc_ids, offsets, token_offsets) = batch\n",
    "            # Make predictions on the model\n",
    "            preds = model(texts, att_masks)\n",
    "\n",
    "            # Take into account padded while calculating loss\n",
    "            loss_unreduced = criterion(preds.permute(0,2,1), labels)\n",
    "            loss = (loss_unreduced * att_masks).sum() / (att_masks).sum()\n",
    "\n",
    "            # Get argmax of non-padded tokens\n",
    "            for sent_preds, sent_labels, sent_att_masks in zip(preds, labels, att_masks):\n",
    "                for token_preds, token_labels, token_masks in zip(sent_preds, sent_labels, sent_att_masks):\n",
    "                    if token_masks.item() != 0:\n",
    "                        predicts.append(token_preds.argmax().item())\n",
    "                        gnd_truths.append((token_labels.item()))\n",
    "            valid_losses.append(loss.item())\n",
    "\n",
    "            assert len(predicts) == len(gnd_truths)\n",
    "\n",
    "    # Create confusion matrix and evaluate on the predictions\n",
    "    confuse_mat = confusion_matrix(gnd_truths, predicts)\n",
    "    if dummy_run:\n",
    "        classify_report = None\n",
    "    else:\n",
    "        classify_report = classification_report(gnd_truths, predicts,\n",
    "                                        target_names=target_names,\n",
    "                                        output_dict=True)\n",
    "\n",
    "    mean_valid_loss = np.average(valid_losses)\n",
    "    print(\"Valid_loss\", mean_valid_loss)\n",
    "    print(confuse_mat)\n",
    "\n",
    "    if not dummy_run:\n",
    "        for labl in target_names:\n",
    "            print(labl,\"F1-score:\", classify_report[labl][\"f1-score\"])\n",
    "        print(\"Accu:\", classify_report[\"accuracy\"])\n",
    "        print(\"F1-Weighted\", classify_report[\"weighted avg\"][\"f1-score\"])\n",
    "        print(\"F1-Avg\", classify_report[\"macro avg\"][\"f1-score\"])\n",
    "\n",
    "    return mean_valid_loss, confuse_mat ,classify_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "957a5599",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S0022000014000026-7850\n",
      "233 15 248\n",
      "Fixed sentence splitting: {'figs.': 4, 'fig.': 92, 'et al.': 31, 'ref.': 3, 'eq.': 3, 'e.g.': 8, 'i.e.': 3, 'nos.': 2, 'no.': 3, 'spp.': 2}\n",
      "Flattened 248 docs into 1265 data points \n",
      "Some examples: [['S0006322312001096-1136.tsv', 'Data were drawn from the Whitehall II study with baseline examination in 0000; follow-up screenings in 0000, 0000, and 0000; and additional disease ascertainment from hospital data and registry linkage on 0000 participants (mean age 00.0 years, 00% women) without depressive symptoms at baseline.', [0, 296], {'QUANT': [(73, 77), (103, 123), (205, 222), (233, 243), (245, 248)], 'ME': [(49, 69), (79, 99), (25, 43), (205, 222)], 'MP': [(224, 232), (249, 254)], 'QUAL': []}], ['S0006322312001096-1136.tsv', ' Vascular risk was assessed with the Framingham Cardiovascular, Coronary Heart Disease, and Stroke Risk Scores.', [296, 407], {'QUANT': [], 'ME': [], 'MP': [], 'QUAL': []}]]\n",
      "num_batches= 20  | num_data= 1265\n"
     ]
    }
   ],
   "source": [
    "text_path = \"./data/train/text/\"\n",
    "label_path = \"./data/train/tsv/\"\n",
    "\n",
    "textset = get_doc_ids(text_path)\n",
    "\n",
    "# Load all annotations\n",
    "files_with_label = [file_name[:-4] for file_name in os.listdir(label_path)]\n",
    "all_files_with_or_without_label = [file_name[:-4] for file_name in os.listdir(text_path)]\n",
    "files_without_label = list(set(all_files_with_or_without_label).difference(set(files_with_label)))\n",
    "print(files_without_label[0])\n",
    "print(len(files_with_label), len(files_without_label), len(all_files_with_or_without_label))\n",
    "\n",
    "all_data = load_dataset(files_with_label, files_without_label)\n",
    "\n",
    "tokenized_data = tokenize_split_data(all_data)\n",
    "\n",
    "all_annotated_split_data = annotation_map(tokenized_data, all_data)\n",
    "\n",
    "#### RENAME THIS IF RUNNING SEQUENTIALLY!\n",
    "train_dataset = batch_dataset(all_annotated_split_data, shuffle=True if \"train\" in text_path else False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e631e0d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65 0 65\n",
      "Fixed sentence splitting: {'figs.': 6, 'fig.': 61, 'et al.': 22, 'ref.': 0, 'eq.': 2, 'e.g.': 5, 'i.e.': 0, 'nos.': 0, 'no.': 0, 'spp.': 4}\n",
      "Flattened 65 docs into 420 data points \n",
      "Some examples: [['S0012821X12004384-1302.tsv', 'Correspondence analysis (CA) and statistical diversity analysis were carried out on the palynological dataset (total counts per gram) to confirm assemblage designations (Figs. 0 and 0), to identify any disturbance to the core prior to interpretation, and to estimate diversity (Fig. 0).', [0, 286], {'QUANT': [], 'ME': [], 'MP': [], 'QUAL': []}], ['S0012821X12004384-1302.tsv', ' Dinoflagellate cyst assemblages (DA0–DA0) and pollen assemblages (PA0–PA0) were defined by visually comparing changes in the species dominance (Figs. 0 and 0), and confirmed by CA (Fig. 0) using the first three axes (describing the highest percentages of variance).', [286, 552], {'QUANT': [], 'ME': [], 'MP': [], 'QUAL': []}]]\n",
      "num_batches= 7  | num_data= 420\n"
     ]
    }
   ],
   "source": [
    "text_path = \"./data/trial/text/\"\n",
    "label_path = \"./data/trial/tsv/\"\n",
    "\n",
    "textset = get_doc_ids(text_path)\n",
    "\n",
    "# Load all annotations\n",
    "files_with_label = [file_name[:-4] for file_name in os.listdir(label_path)]\n",
    "all_files_with_or_without_label = [file_name[:-4] for file_name in os.listdir(text_path)]\n",
    "files_without_label = list(set(all_files_with_or_without_label).difference(set(files_with_label)))\n",
    "print(len(files_with_label), len(files_without_label), len(all_files_with_or_without_label))\n",
    "\n",
    "all_data = load_dataset(files_with_label, files_without_label)\n",
    "\n",
    "tokenized_data = tokenize_split_data(all_data)\n",
    "\n",
    "all_annotated_split_data = annotation_map(tokenized_data, all_data)\n",
    "\n",
    "#### RENAME THIS IF RUNNING SEQUENTIALLY!\n",
    "valid_dataset = batch_dataset(all_annotated_split_data, shuffle=True if \"train\" in text_path else False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a13218f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset created\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Dataset created\")\n",
    "os.system(\"nvidia-smi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "ab063226",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_type = \"roberta-base\"\n",
    "device = 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "6984c00f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model created\n",
      "124647170 parameters!\n",
      "Detected 1 GPUs!\n"
     ]
    }
   ],
   "source": [
    "class OurBERTModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(OurBERTModel, self).__init__()\n",
    "        self.bert = AutoModel.from_pretrained(bert_type)\n",
    "        self.drop = nn.Dropout(self.bert.config.hidden_dropout_prob)\n",
    "        self.classifier = nn.Linear(self.bert.config.hidden_size, 2)\n",
    "\n",
    "    def forward(self, text, att_mask):\n",
    "        b, num_tokens = text.shape\n",
    "        token_type = torch.zeros((b, num_tokens), dtype=torch.long).to(device)\n",
    "        outputs = self.bert(text, attention_mask=att_mask, token_type_ids=token_type)\n",
    "        return self.classifier(self.drop(outputs['last_hidden_state']))\n",
    "\n",
    "model = OurBERTModel()\n",
    "print(\"Model created\")\n",
    "os.system(\"nvidia-smi\")\n",
    "\n",
    "print(sum(p.numel() for p in model.parameters()),\"parameters!\")\n",
    "model = model.to(params.device)\n",
    "print(\"Detected\", torch.cuda.device_count(), \"GPUs!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "006af66a",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 6\n",
    "lr = 3e-05\n",
    "dummy_run = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "b46da35b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "========= Beginning epoch  1 ==========\n",
      "Train loss at 0: 0.006757479626685381\n",
      "Train loss at 10: 0.007817627862095833\n",
      "\n",
      "====EVALUATING On Training set :====\n",
      "\n",
      "Valid_loss 0.002977533807279542\n",
      "[[43973    41]\n",
      " [   42  3538]]\n",
      "NOT_QUANT F1-score: 0.9990571289007033\n",
      "QUANT F1-score: 0.9884062019835173\n",
      "Accu: 0.9982560826995\n",
      "F1-Weighted 0.9982559708059113\n",
      "F1-Avg 0.9937316654421102\n",
      "\n",
      "====EVALUATING On Validation set :====\n",
      "\n",
      "Valid_loss 0.07035595870443753\n",
      "[[15646   136]\n",
      " [   69  1135]]\n",
      "NOT_QUANT F1-score: 0.993491443629552\n",
      "QUANT F1-score: 0.9171717171717172\n",
      "Accu: 0.9879312374896974\n",
      "F1-Weighted 0.9880817562013621\n",
      "F1-Avg 0.9553315804006346\n",
      "[1/6]     train_loss: 0.00555 valid_loss: 0.07036\n",
      "\n",
      "\n",
      "========= Beginning epoch  2 ==========\n",
      "Train loss at 0: 0.0020206396002322435\n",
      "Train loss at 10: 0.00983401108533144\n",
      "\n",
      "====EVALUATING On Training set :====\n",
      "\n",
      "Valid_loss 0.004273403610568494\n",
      "[[43914   100]\n",
      " [    2  3578]]\n",
      "NOT_QUANT F1-score: 0.9988399863527806\n",
      "QUANT F1-score: 0.9859465417470378\n",
      "Accu: 0.9978568727150481\n",
      "F1-Weighted 0.9978701470518486\n",
      "F1-Avg 0.9923932640499091\n",
      "\n",
      "====EVALUATING On Validation set :====\n",
      "\n",
      "Valid_loss 0.08025670051574707\n",
      "[[15540   242]\n",
      " [   36  1168]]\n",
      "NOT_QUANT F1-score: 0.9911346386886918\n",
      "QUANT F1-score: 0.8936495791889825\n",
      "Accu: 0.9836335805957848\n",
      "F1-Weighted 0.9842247121822955\n",
      "F1-Avg 0.9423921089388372\n",
      "[2/6]     train_loss: 0.00475 valid_loss: 0.08026\n",
      "\n",
      "\n",
      "========= Beginning epoch  3 ==========\n",
      "Train loss at 0: 0.002791615203022957\n",
      "Train loss at 10: 0.0048952121287584305\n",
      "\n",
      "====EVALUATING On Training set :====\n",
      "\n",
      "Valid_loss 0.003937241609673947\n",
      "[[43921    93]\n",
      " [    3  3577]]\n",
      "NOT_QUANT F1-score: 0.9989083217721576\n",
      "QUANT F1-score: 0.9867586206896551\n",
      "Accu: 0.9979829390259276\n",
      "F1-Weighted 0.9979944265358807\n",
      "F1-Avg 0.9928334712309064\n",
      "\n",
      "====EVALUATING On Validation set :====\n",
      "\n",
      "Valid_loss 0.08261053902762276\n",
      "[[15592   190]\n",
      " [   46  1158]]\n",
      "NOT_QUANT F1-score: 0.9924888605983452\n",
      "QUANT F1-score: 0.9075235109717869\n",
      "Accu: 0.9861062051100906\n",
      "F1-Weighted 0.9864663549495535\n",
      "F1-Avg 0.950006185785066\n",
      "[3/6]     train_loss: 0.00491 valid_loss: 0.08261\n",
      "\n",
      "\n",
      "========= Beginning epoch  4 ==========\n",
      "Train loss at 0: 0.0022282979916781187\n",
      "Train loss at 10: 0.0062606423161923885\n",
      "\n",
      "====EVALUATING On Training set :====\n",
      "\n",
      "Valid_loss 0.0032298189937137066\n",
      "[[43948    66]\n",
      " [   21  3559]]\n",
      "NOT_QUANT F1-score: 0.9990111726128911\n",
      "QUANT F1-score: 0.9879250520471895\n",
      "Accu: 0.9981720384922469\n",
      "F1-Weighted 0.9981772794409532\n",
      "F1-Avg 0.9934681123300403\n",
      "\n",
      "====EVALUATING On Validation set :====\n",
      "\n",
      "Valid_loss 0.07013547952686038\n",
      "[[15652   130]\n",
      " [   83  1121]]\n",
      "NOT_QUANT F1-score: 0.9932417425516389\n",
      "QUANT F1-score: 0.9132382892057026\n",
      "Accu: 0.9874602613917344\n",
      "F1-Weighted 0.9875709455524333\n",
      "F1-Avg 0.9532400158786707\n",
      "[4/6]     train_loss: 0.00505 valid_loss: 0.07014\n",
      "\n",
      "\n",
      "========= Beginning epoch  5 ==========\n",
      "Train loss at 0: 0.001864080666564405\n",
      "Train loss at 10: 0.0069219875149428844\n",
      "\n",
      "====EVALUATING On Training set :====\n",
      "\n",
      "Valid_loss 0.002124230080517009\n",
      "[[43987    27]\n",
      " [   23  3557]]\n",
      "NOT_QUANT F1-score: 0.9994319730982459\n",
      "QUANT F1-score: 0.9930206588498045\n",
      "Accu: 0.9989494474093373\n",
      "F1-Weighted 0.998949716826249\n",
      "F1-Avg 0.9962263159740252\n",
      "\n",
      "====EVALUATING On Validation set :====\n",
      "\n",
      "Valid_loss 0.08629751365099635\n",
      "[[15646   136]\n",
      " [   80  1124]]\n",
      "NOT_QUANT F1-score: 0.9931445981972833\n",
      "QUANT F1-score: 0.9123376623376624\n",
      "Accu: 0.9872836453549982\n",
      "F1-Weighted 0.9874168488286865\n",
      "F1-Avg 0.9527411302674729\n",
      "[5/6]     train_loss: 0.00359 valid_loss: 0.08630\n",
      "\n",
      "\n",
      "========= Beginning epoch  6 ==========\n",
      "Train loss at 0: 0.002763062948361039\n",
      "Train loss at 10: 0.008047402836382389\n",
      "\n",
      "====EVALUATING On Training set :====\n",
      "\n",
      "Valid_loss 0.0013661923196195858\n",
      "[[44014     0]\n",
      " [   22  3558]]\n",
      "NOT_QUANT F1-score: 0.9997501419647926\n",
      "QUANT F1-score: 0.9969179041748389\n",
      "Accu: 0.9995377568601084\n",
      "F1-Weighted 0.9995371022688638\n",
      "F1-Avg 0.9983340230698158\n",
      "\n",
      "====EVALUATING On Validation set :====\n",
      "\n",
      "Valid_loss 0.06036297444786344\n",
      "[[15683    99]\n",
      " [   87  1117]]\n",
      "NOT_QUANT F1-score: 0.9941049695740366\n",
      "QUANT F1-score: 0.9231404958677687\n",
      "Accu: 0.9890498057223596\n",
      "F1-Weighted 0.9890748726505498\n",
      "F1-Avg 0.9586227327209027\n",
      "[6/6]     train_loss: 0.00685 valid_loss: 0.06036\n"
     ]
    }
   ],
   "source": [
    "########## Optimizer & Loss ###########\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss(reduction='none')\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "########## Training loop ###########\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    print(\"\\n\\n========= Beginning epoch\", epoch+1,\"==========\")\n",
    "\n",
    "    train_loss = train(model, train_dataset, criterion)\n",
    "    print(\"\\n==== EVALUATING On Training set ====\\n\")\n",
    "    _, _, _ = evaluate(model, train_dataset, criterion)\n",
    "\n",
    "    print(\"\\n==== EVALUATING On Validation set ====\\n\")\n",
    "    valid_loss, confuse_mat, classify_report = evaluate(model, valid_dataset, criterion)\n",
    "\n",
    "    epoch_len = len(str(n_epochs))\n",
    "    print_msg = (f'[{epoch+1:>{epoch_len}}/{n_epochs:>{epoch_len}}]     ' +\n",
    "                    f'train_loss: {train_loss:.5f} ' +\n",
    "                    f'valid_loss: {valid_loss:.5f}')\n",
    "    print(print_msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "da7a2446",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mispredict: S0012821X12004384-1302.tsv for sentence at offset [552, 745]\n",
      "Mispredict: S0012821X12004384-1302.tsv for sentence at offset [745, 864]\n",
      "Mispredict: S0012821X12004384-1302.tsv for sentence at offset [1476, 1951]\n",
      "Mispredict: S0012821X12004384-1302.tsv for sentence at offset [1951, 2142]\n",
      "Mispredict: S0012821X12004384-1415.tsv for sentence at offset [0, 473]\n",
      "Mispredict: S0012821X13002185-1217.tsv for sentence at offset [199, 367]\n",
      "Mispredict: S0012821X13002185-1217.tsv for sentence at offset [367, 562]\n",
      "Mispredict: S0012821X13002185-1217.tsv for sentence at offset [683, 883]\n",
      "Mispredict: S0012821X13002185-1217.tsv for sentence at offset [944, 1057]\n",
      "Mispredict: S0012821X13002185-1231.tsv for sentence at offset [0, 379]\n",
      "Mispredict: S0012821X13002185-1231.tsv for sentence at offset [565, 663]\n",
      "Mispredict: S0012821X13002185-1231.tsv for sentence at offset [1039, 1375]\n",
      "Mispredict: S0012821X13002185-835.tsv for sentence at offset [129, 370]\n",
      "Mispredict: S0012821X13007309-1605.tsv for sentence at offset [1273, 1594]\n",
      "Mispredict: S0012821X13007309-1989.tsv for sentence at offset [918, 1133]\n",
      "Mispredict: S0016236113008041-3031.tsv for sentence at offset [803, 912]\n",
      "Mispredict: S0016236113008041-3031.tsv for sentence at offset [912, 1107]\n",
      "Mispredict: S0016236113008041-3031.tsv for sentence at offset [1107, 1420]\n",
      "Mispredict: S0016236113008041-3031.tsv for sentence at offset [1420, 1584]\n",
      "Mispredict: S0016236113008041-3171.tsv for sentence at offset [535, 683]\n",
      "Mispredict: S0016236113008041-3186.tsv for sentence at offset [130, 267]\n",
      "Mispredict: S0016236113008041-3207.tsv for sentence at offset [0, 304]\n",
      "Mispredict: S0016236113008041-3269.tsv for sentence at offset [212, 372]\n",
      "Mispredict: S0016236113008041-3269.tsv for sentence at offset [891, 1073]\n",
      "Mispredict: S0019103511004994-1382.tsv for sentence at offset [350, 511]\n",
      "Mispredict: S0019103511004994-1382.tsv for sentence at offset [1827, 2052]\n",
      "Mispredict: S0019103511004994-1511.tsv for sentence at offset [148, 358]\n",
      "Mispredict: S0019103511004994-1511.tsv for sentence at offset [864, 994]\n",
      "Mispredict: S0019103511004994-1511.tsv for sentence at offset [1270, 1400]\n",
      "Mispredict: S0019103511004994-1565.tsv for sentence at offset [860, 987]\n",
      "Mispredict: S0019103512002801-1342.tsv for sentence at offset [0, 58]\n",
      "Mispredict: S0019103512002801-1342.tsv for sentence at offset [58, 180]\n",
      "Mispredict: S0019103512002801-1608.tsv for sentence at offset [217, 538]\n",
      "Mispredict: S0019103512002801-1849.tsv for sentence at offset [83, 151]\n",
      "Mispredict: S0019103512002801-1927.tsv for sentence at offset [0, 345]\n",
      "Mispredict: S0019103512002801-1927.tsv for sentence at offset [345, 495]\n",
      "Mispredict: S0019103512003533-3348.tsv for sentence at offset [247, 399]\n",
      "Mispredict: S0019103512003533-3348.tsv for sentence at offset [399, 501]\n",
      "Mispredict: S0019103512003533-5031.tsv for sentence at offset [189, 413]\n",
      "Mispredict: S0019103512003533-5031.tsv for sentence at offset [413, 575]\n",
      "Mispredict: S0019103512003533-5072.tsv for sentence at offset [0, 175]\n",
      "Mispredict: S0019103512003533-5251.tsv for sentence at offset [0, 299]\n",
      "Mispredict: S0019103512003533-5251.tsv for sentence at offset [299, 700]\n",
      "Mispredict: S0019103512003533-5251.tsv for sentence at offset [700, 941]\n",
      "Mispredict: S0019103512003533-5598.tsv for sentence at offset [762, 1127]\n",
      "Mispredict: S0019103512003995-1807.tsv for sentence at offset [204, 392]\n",
      "Mispredict: S0019103512003995-1807.tsv for sentence at offset [559, 764]\n",
      "Mispredict: S0019103512003995-1910.tsv for sentence at offset [0, 163]\n",
      "Mispredict: S0019103512003995-1910.tsv for sentence at offset [163, 346]\n",
      "Mispredict: S0019103512003995-1910.tsv for sentence at offset [525, 623]\n",
      "Mispredict: S0019103512003995-3548.tsv for sentence at offset [612, 894]\n",
      "Mispredict: S0019103512004009-3488.tsv for sentence at offset [431, 589]\n",
      "Mispredict: S0019103512004009-3825.tsv for sentence at offset [334, 667]\n",
      "Mispredict: S0019103512004009-3825.tsv for sentence at offset [929, 1002]\n",
      "Mispredict: S0019103512004009-3962.tsv for sentence at offset [237, 368]\n",
      "Mispredict: S0019103512004009-3962.tsv for sentence at offset [415, 584]\n",
      "Mispredict: S0019103512004009-4007.tsv for sentence at offset [577, 711]\n",
      "Mispredict: S0019103512004009-4492.tsv for sentence at offset [297, 471]\n",
      "Mispredict: S0019103512004009-5507.tsv for sentence at offset [0, 312]\n",
      "Mispredict: S0019103513005058-3094.tsv for sentence at offset [588, 787]\n",
      "Mispredict: S0019103513005058-3094.tsv for sentence at offset [1322, 1527]\n",
      "Mispredict: S0019103513005058-4210.tsv for sentence at offset [88, 244]\n",
      "Mispredict: S0019103513005058-4210.tsv for sentence at offset [244, 266]\n",
      "Mispredict: S0019103513005058-4210.tsv for sentence at offset [283, 413]\n",
      "Mispredict: S0021979713004438-1969.tsv for sentence at offset [188, 307]\n",
      "Mispredict: S0021979713004438-1969.tsv for sentence at offset [437, 498]\n",
      "Mispredict: S0021979713004438-2148.tsv for sentence at offset [849, 986]\n",
      "Mispredict: S0021979713004438-2148.tsv for sentence at offset [1419, 1639]\n",
      "Mispredict: S0022000014000026-18167.tsv for sentence at offset [1053, 1397]\n",
      "Mispredict: S0022000014000026-18167.tsv for sentence at offset [5228, 5552]\n",
      "Mispredict: S0022000014000026-18167.tsv for sentence at offset [5663, 5830]\n",
      "Mispredict: S0022000014000026-18167.tsv for sentence at offset [5830, 6080]\n",
      "Mispredict: S0022000014000026-18167.tsv for sentence at offset [6635, 6714]\n",
      "Mispredict: S0022459611006116-1195.tsv for sentence at offset [138, 237]\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "valid_losses = []\n",
    "correct_sentences = 0\n",
    "total_sentences = 0 \n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in valid_dataset:\n",
    "        # Unpack the batch and feed into the model\n",
    "        (texts, labels, att_masks, doc_ids, offsets, token_offsets) = batch\n",
    "        preds = model(texts, att_masks)\n",
    "\n",
    "        # Check for mispredicts\n",
    "        for sent_preds, sent_labels, sent_att_masks, sent_doc_id, sent_offset in zip(preds, labels, att_masks, doc_ids, offsets):\n",
    "            this_correct = (sent_preds.argmax(1) == sent_labels).sum()\n",
    "            this_total = len(sent_labels)\n",
    "\n",
    "            if this_correct != this_total:\n",
    "                print(\"Mispredict:\", sent_doc_id, \"for sentence at offset\", sent_offset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "325ce8b8",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'METestDataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [81]\u001b[0m, in \u001b[0;36m<cell line: 55>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     52\u001b[0m     os\u001b[38;5;241m.\u001b[39msystem(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrm -rf \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m folder_name)\n\u001b[0;32m     53\u001b[0m os\u001b[38;5;241m.\u001b[39mmkdir(folder_name)\n\u001b[1;32m---> 55\u001b[0m test_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mMETestDataset\u001b[49m(eval_doc_path)\n\u001b[0;32m     57\u001b[0m predict_spans(train_dataset, folder_name\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/train_spans.json\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     58\u001b[0m predict_spans(valid_dataset, folder_name\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/trial_spans.json\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'METestDataset' is not defined"
     ]
    }
   ],
   "source": [
    "# Predict spans on Test, Train and Val set\n",
    "\n",
    "def predict_spans(batched_dataset, save_path):\n",
    "    # Put the model to eval mode and track the predictions\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        span_dict = {}\n",
    "        for batch in batched_dataset:\n",
    "            (texts, raw_texts, att_masks, doc_ids, offsets, token_offsets) = batch\n",
    "            preds = model(texts, att_masks)\n",
    "\n",
    "            for sent_preds, sent_raw_text, sent_att_masks, sent_doc_id, sent_offset, sent_token_offsets in zip(preds, raw_texts, att_masks, doc_ids, offsets, token_offsets):\n",
    "\n",
    "                this_sentence_positives = []\n",
    "                curr_positive_idx = -1\n",
    "                for i, (token_preds, token_labels, token_masks) in enumerate(zip(sent_preds, sent_labels, sent_att_masks)):\n",
    "                    if token_masks.item() != 0:\n",
    "                        if token_preds.argmax().item() == 1:\n",
    "                            if curr_positive_idx == -1:\n",
    "                                curr_positive_idx = i\n",
    "                        else:\n",
    "                            if curr_positive_idx != -1:\n",
    "                                this_sentence_positives.append([curr_positive_idx, i-1])\n",
    "                                curr_positive_idx = -1\n",
    "                    else:\n",
    "                        if curr_positive_idx != -1:\n",
    "                            this_sentence_positives.append([curr_positive_idx, i-1])\n",
    "                            curr_positive_idx = -1\n",
    "                            break\n",
    "\n",
    "                # Here convert indices to offsets\n",
    "                if sent_doc_id not in span_dict.keys():\n",
    "                    span_dict[sent_doc_id] = {}\n",
    "\n",
    "                this_sent_spans = []\n",
    "                for span_offsets in this_sentence_positives:\n",
    "                    this_sent_spans.append([sent_token_offsets[span_offsets[0]][0],\n",
    "                                            sent_token_offsets[span_offsets[1]][1]\n",
    "                                        ])\n",
    "                \n",
    "                assert sent_offset[0] not in span_dict[sent_doc_id].keys()\n",
    "                span_dict[sent_doc_id][sent_offset[0]] = this_sent_spans\n",
    "    \n",
    "    json.dump(span_dict, open(save_path, 'w+'))\n",
    "    \n",
    "\n",
    "# Save model and predicition\n",
    "from datetime import datetime\n",
    "folder_name = \"./measeval/task1/output/\" + bert_type.replace('/', '_') #datetime.today().strftime('%Y-%m-%d-%H-%M-%S')\n",
    "if os.path.isdir(folder_name):\n",
    "    os.system(\"rm -rf \" + folder_name)\n",
    "os.mkdir(folder_name)\n",
    "\n",
    "test_dataset = METestDataset(eval_doc_path)\n",
    "\n",
    "predict_spans(train_dataset, folder_name+\"/train_spans.json\")\n",
    "predict_spans(valid_dataset, folder_name+\"/trial_spans.json\")\n",
    "predict_spans(test_dataset, folder_name+\"/test_spans.json\")\n",
    "\n",
    "torch.save(model.state_dict(), folder_name+\"/model.pt\")\n",
    "json.dump(vars(params), open(folder_name+\"/params.json\", 'w+'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "measeval-kernel",
   "language": "python",
   "name": "measeval-kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
